user_input,retrieved_contexts,response,reference,context_precision,faithfulness,answer_relevancy
How many requirements does the ASRD contain and where did they come from?,"[""CONTEXT: **Dataset Filtering and Composition for ASRD**: This chunk details the rigorous filtering, randomization, and composition process used to create the Agency Security Requirements Dataset (ASRD), a Turkish-language corpus of 2,652 functional requirements across six business domains, annotated for security classification using the OWASP ASVS taxonomy. It also provides sample requirements to illustrate the dataset's diversity and focus on implicit security dependencies.\n\nCONTENT: 10 Yusuf G¨ ur et al.\n3. Filtering: The initial extraction yielded 3,264 items. To ensure the qual-\nity of the dataset, domain experts conducted a rigorous manual filtering\npass to remove 612 items. This exclusion process went beyond simple for-\nmatting artifacts (such as headers or tables of contents). We specifically\nremoved non-functional administrative clauses that addressed the vendor\nrather than the system (e.g., “The contractor must provide training man-\nuals”), project management constraints (e.g., “The project must be deliv-\nered by Q3”), and requirements with ambiguous scope that lacked clear\nverification criteria.\n4. Randomization: The remaining requirements were randomized and masked\nto mitigate potential annotation bias related to the document source or the\nsequence of requirements.\nThe resulting ASRD comprises 2,652 well-formed functional requirements\nacross six distinct business areas: Construction (1,202), Legal (177), Education\n(312), Software (199), Meeting (126), and Accommodation (636). A sample of\nthese requirements is provided in Table 2. The requirements included in this\nsample were selected based on three specific criteria to ensure a representative\noverview:\n–Samples were chosen to reflect the varied terminology and phrasing styles\nused across the six different business domains.\n–We prioritized requirements that demonstrate how standard functional de-\nscriptions conceal underlying security dependencies.\n–We ensured that the sample includes representative examples for all 11\nOWASP ASVS security classes used in the study to demonstrate the full\nscope of the classification task.\nIt is important to note that the ASRD is a Turkish-language corpus; all experi-\nments detailed in this study, including BERT fine-tuning and LLM prompting,\nwere conducted exclusively on the original text. English translations are pro-\nvided only for readability.\nTable 2: Sample Requirements from ASRD\nNo Original Requirements Translated Requirements\n1 Sistem, yetkili kullanıcının hukuk\ndosyası t¨ ur¨ u kayıt etmesini\nsa˘ glayacaktır.\nThe system shall allow authorized\nusers to record the legal file type.\n2 Sistem, kullanıcının kullanılmamı¸ s\nhukuk dosyası t¨ ur¨ un¨ u g¨ uncellemesini\nsa˘ glayacaktır.\nThe system shall allow the user to up-\ndate an unused legal file type.\n3 Sistem, kullanıcının kullanılmamı¸ s\nolan hukuk dosyası t¨ ur¨ un¨ u silmesini\nsa˘ glayacaktır.\nThe system shall allow the user to\ndelete an unused legal file type.\n4 Sistem, kullanıcının hukuk dosyası\nt¨ urlerini sorgulamasını ve listelemesini\nsa˘ glayacaktır.\nThe system shall allow the user to\nquery and list legal file types."", 'CONTEXT: This chunk provides an overview of the Agency Security Requirements Dataset (ASRD), detailing its real-world data sources, expert annotation process, and multi-label structure based on OWASP ASVS categories, while also introducing the research questions and comparing the performance of fine-tuned transformer models and prompt-based approaches for classifying implicit security requirements.\n\nCONTENT: 4 Yusuf G¨ ur et al.\ncient and cost-effective compared to addressing security issues at later stages\nof development ((Khan et al., 2024).\nThis paper proposes a machine learning-based system to automatically an-\nalyze SRS documents and classify security requirements according to OWASP\nASVS. Its main contribution is the Agency Security Requirements Dataset\n(ASRD), a publicly available corpus created to support research in automated\nSRA. The ASRD comprises:\n–Real-World Data: 2,652 Software Requirements Specification (SRS) sen-\ntences extracted from six diverse, real-world industry projects,\n–Expert Annotation: Meticulously annotated by three cybersecurity experts,\neach with over 15 years of industry experience,\n–Rigorous Methodology: Developed using the iterative MATTER cycle an-\nnotation framework (Pustejovsky and Stubbs, 2012),\n–High Granularity and Multi-label Structure: Mapped to 11 distinct, action-\nable security categories from the industry-standard OWASP Application\nSecurity Verification Standard (ASVS,V2–V13), allowing each require-\nment to be associated with multiple categories.\nThe ASRD is intended as a shared research resource and benchmark. Using\nthis dataset, the study conducts an empirical analysis of automated classifica-\ntion approaches and addresses the following research questions:\n1. To what extent can fine-tuned transformer models accurately perform\nmulti-label classification of implicit security requirements into OWASP\nASVS categories using the ASRD?\n2. How does the performance of prompt-based approaches (zero-shot and few-\nshot) using modern LLMs compare to fine-tuned BERT-based models and\nnaive baselines for this task?\nThe results show that few-shot prompting with a modern language model\n(Gemini 2.0) achieves a macro-average F1 score of 0.941, closely matching the\nbest fine-tuned BERT-based model (0.942). This demonstrates that example-\nguided inference can reach performance comparable to supervised fine-tuning\nwhile requiring substantially less annotated data and model retraining. The\nevaluation compares fine-tuned transformer models (e.g., SecureBERT) with\nprompt-based methods in zero-shot and few-shot settings across multiple con-\ntemporary models, demonstrating that prompt-based approaches provide a\npractical and resource-efficient alternative for identifying implicit security re-\nquirements in industrial SRS documents.\nThe remainder of this paper is organized as follows. Section 2 reviews re-\nlated work on NLP-based requirements classification, security frameworks, and\nexisting security datasets. Section 3 describes the curation and expert anno-\ntation of the ASRD and summarizes its key characteristics. Section 4 outlines\nthe experimental setup and classification methods, including fine-tuned BERT\nmodels and prompt-based approaches. Section 5 reports the experimental re-\nsults. Section 6 discusses the findings and their implications, with a com-\nparative analysis of fine-tuning and prompting strategies. Section 7 examines', 'CONTEXT: This chunk discusses the curation and preprocessing of the Agency Security Requirements Dataset (ASRD), detailing the multi-stage pipeline used to extract, anonymize, and prepare real-world Turkish-language Software Requirements Specification (SRS) documents for multi-label security annotation, while addressing gaps in prior datasets.\n\nCONTENT: Implicit Security Requirements Classification 9\n2.3.3 Electronic Health Domain Dataset\nThis corpus focuses on the electronic healthcare industry, a sector defined\nby stringent regulatory and high-assurance demands (Riaz et al., 2014). The\ndataset’s strength lies in its rigorous multi-stage annotation methodology,\nwhich utilized independent researchers and a mediator to ensure high inter-\nannotator agreement. While it provides a large-scale, authentic resource for\nhealthcare-specific tasks, its narrow focus may limit the generalizability of\nmodels to other industries, and its high-level security objectives may be too\ncoarse for detailed technical analysis.\n2.3.4 Summary of Datasets\nAs summarized in Table 1, these datasets vary significantly in their suitability\nfor tasks ranging from general non-functional requirement (NFR) classifica-\ntion to detailed security analysis. Our proposed Agency Security Require-\nments Dataset (ASRD) addresses gaps in prior work by adopting the OWASP\nASVS taxonomy for multi-label annotation. Furthermore, by utilizing real-\nworld projects originally written in Turkish, ASRD introduces both industrial\ncomplexity and linguistic diversity to the field.\n3 Dataset Curation, Annotation, and Evaluation\n3.1 Data Collection and Preprocessing\nThe dataset compilation began with the collection of six Software Require-\nments Specification (SRS) documents from an Agency’s Security Department.\nThese documents were selected from an initial pool of 19 projects executed\nbetween 2019 and 2023, specifically chosen for their domain diversity and\namenability to effective anonymization. The original documents were provided\nin standard document formats (e.g., PDF, DOCX) where requirements were\norganized in itemized lists. To transform these documents into a machine-\nreadable dataset, we employed a multi-stage preprocessing pipeline:\n1. Sentence Segmentation and Extraction: As the source SRS documents uti-\nlized standard itemization for requirements, we employed a rule-based ex-\ntraction method. We utilized pattern matching to identify requirement\nblocks based on hierarchical numbering schemes and line breaks. Each iden-\ntified requirement was extracted and treated as a single processing unit,\npreserving the structural integrity of the original specification.\n2. Anonymization: To protect sensitive data, a semi-automated anonymiza-\ntion process was implemented. Custom scripts replaced entities such as\nproject names, specific URLs, and IP addresses with generic placeholders.\nThis was followed by a manual review by domain experts to ensure that\nthe removal of personally identifiable information (PII) did not compromise\nthe semantic meaning or contextual integrity of the requirements.', 'CONTEXT: **Comparison of Effort vs. Performance and Validation of ASRD Dataset**: This chunk discusses the trade-offs between data preparation effort and model performance across fine-tuned BERT models and LLM approaches, highlights the advantages of the ASRD dataset over existing benchmarks, and emphasizes the challenges of implicit security requirement elicitation in real-world contexts.\n\nCONTENT: 34 Yusuf G¨ ur et al.\nTable 9: Comparison of Effort vs. Performance across Approaches\nApproach Data Requirement Est. Setup Effort Best Macro-F1\nBERT Fine-Tuning High (∼2,600 ex) High (267 hours)0.942\nLLM Few-Shot Low (∼30 ex) Low (<10 hours)0.941\nLLM Zero-Shot None Minimal (<2 hours) 0.890\nLLM Few-Shot approach (using Gemini 2.0) achieved a similar performance\n(Macro-F1 0.941) while requiring only the selection of a small number of in-\ncontext examples (approx. 30 examples per prompt).\nThis comparison points to diminishing returns from large-scale annotation\nwhen capable LLMs are available. Although extensive annotation was required\nhere to establish a reliable ground truth and validate the test set, the results\nindicate that, in industrial settings, a few-shot LLM approach can offer a more\nfavorable balance between effort and accuracy. Comparable performance can\nbe reached with substantially less data preparation, reducing the need for\nprolonged expert annotation.\nThe main contribution of this work is the introduction and validation of\nthe ASRD. The consistent performance of both fine-tuned BERT models and\nLLM-based approaches on ASRD supports its reliability and practical value.\nThe dataset addresses a persistent limitation in SRE research which is the\nlack of large, well-curated, professionally sourced benchmarks for security re-\nquirement classification. Compared with existing datasets, ASRD offers sev-\neral advantages. Unlike DOSSPRE (Kadebu et al., 2023), which is derived\nfrom student projects, ASRD includes 2,652 requirements from six commer-\ncial software systems, reflecting real industrial language and complexity. Its\nOWASP ASVS–based taxonomy spans 11 security classes, providing finer de-\ntail than PROMISE exp (Lima et al., 2019), where security is treated as a\nsingle category. Although the healthcare dataset by Riaz et al. (Riaz et al.,\n2014) is larger, it focuses on six broad objectives within a single domain, lim-\niting its scope. By contrast, ASRD supports cross-domain analysis and aligns\nwith a framework widely used in practice.\nThe dataset also benefits from a rigorous annotation process. Three cy-\nbersecurity specialists, each with over 15 years of experience, labeled the data\nusing an iterative MATTER-based procedure. This resulted in strong agree-\nment (Fleiss’ Kappa = 0.82) and helps mitigate the subjectivity that often\naffects security-related requirement datasets.\nBeyond the dataset construction, this study highlights the complexity of\nimplicit security requirement elicitation. A significant portion of security vul-\nnerabilities stems not from missing explicit security constraints (e.g., “The\nsystem shall encrypt passwords”), but from functional requirements that im-\nplicitly demand robust security controls. Our analysis of the ASRD reveals\nthat functional descriptions of business logic often conceal a dense network of\nsecurity dependencies. A representative example from our results is Require-\nment No. 2 (“The system will carry out the asset handover process”). While', 'CONTEXT: This chunk discusses the challenges of class imbalance in security requirements classification, the impact of few-shot prompting on model performance, and the potential of hybrid approaches like retrieval-augmented generation (RAG) to improve results, particularly for underrepresented security classes. It also highlights future research directions, including the integration of LLMs with agent-based components to enhance automated security analysis.\n\nCONTENT: 36 Yusuf G¨ ur et al.\nfocus their efforts on high-risk adjudication rather than manual discovery. Our\nresults suggest that commercially available LLMs can serve as always-available\nassistants, democratizing high-level security analysis for development teams.\nFurthermore, future studies could investigate the impact of this tool on prac-\ntitioners with varying levels of experience. Specifically, research is needed to\ndetermine if such automation allows junior developers to identify security con-\ncerns more accurately or if it primarily eases the cognitive load for senior\nexperts by filtering out routine classifications.\nThis study highlights class imbalance as a persistent challenge in SRE.\nThe issue is not specific to ASRD but reflects the nature of the domain itself.\nIn typical specifications, references to architectural security concerns such as\ncryptographic storage or low-level file handling, occur far less often than user-\nfacing controls like authentication (V2) or access control (V4). This creates\na “long-tail” distribution where the most important security failures often\nreside in the least represented classes. Standard supervised learning and even\nfew-shot prompting struggle to generalize from such limited examples. This is\na well-documented issue in requirements datasets, including benchmarks like\nPROMISE exp (Lima et al., 2019), which also suffers from imbalanced classes.\nThe improvement from zero-shot to few-shot prompting shows the positive\nimpact of adding in-context examples. For example, Gemma 4b’s F1 score on\nclass V5 rose from 0.709 to 0.814 with only a small number of examples. This\ngain shows how limited, well-chosen examples can steer model behavior toward\nthe intended classification task. Similar observations have been reported in\nprior work across domains, where prompt design and example selection were\nshown to play a decisive role in task performance (Mann et al., 2020; Liu et al.,\n2023a).\n8 Conclusion and Future Work\nThis work examined the feasibility of automated security requirements classi-\nfication under realistic conditions, where requirements are implicit, unevenly\ndistributed across categories, and drawn from industrial settings rather than\ncurated benchmarks. By introducing ASRD and evaluating supervised and\nprompt-based methods on a shared benchmark, this work enables direct com-\nparison across approaches. The results show that few-shot prompting is suit-\nable when labeled data is limited, while fine-tuned models are better at han-\ndling rare security requirements.\nAs a future work, we plan to explore hybrid SRE approaches that combine\nLLMs with retrieval and agent-based components. In particular, improving\nperformance on underrepresented classes remains an open problem and may\nbenefit from targeted data augmentation, retrieval-augmented methods (RAG)\nsuch as those proposed by Liu et al. (Liu et al., 2025), or agent-based strategies.\nFor instance, instead of relying on learned patterns from non-existent training\nexamples, a RAG-enabled system can dynamically retrieve the specific veri-\nfication requirements and definitions relevant to the input text at inference', 'CONTEXT: **Comparison of Fine-Tuned Transformers and LLMs for Security Requirements Classification**  \nThis chunk discusses the performance comparison between fine-tuned transformer models and few-shot prompting with LLMs (e.g., Gemini 2.0), highlighting their near-equal effectiveness in classifying implicit security requirements within the ASRD dataset, while emphasizing the challenges of integrating security early in the SDLC through the ""Shift-Left"" approach.\n\nCONTENT: 2 Yusuf G¨ ur et al.\ntional fine-tuned transformer models and contemporary Large Language Mod-\nels (LLMs) employing few-shot and zero-shot prompt engineering strategies.\nThe results show that a few-shot prompting with Gemini 2.0 achieves a macro-\naverage F1 score of 0.941, directly comparable to the fine-tuned BERT model’s\n0.942. This study culminates in two primary findings: first, the validation and\npublication of the ASRD, a high-granularity, multi-label dataset for implicit\nsecurity requirements based on OWASP ASVS V2-V13; and second, the di-\nrect comparison demonstrating that few-shot Large Language Models (LLMs)\nachieve competitive multi-label classification performance (Macro-F1 0.941)\nnearly equal to resource-intensive fine-tuned transformer models (Macro-F1\n0.942). This confirms that LLMs represent a highly practical and resource-\nsaving strategy for automating the identification of embedded (implicit) secu-\nrity requirements for software security in industrial SRS documents\nKeywordsCybersecurity requirement elicitation·OWASP based security\nrequirement classification·OWASP ASVS-based Requirement Labeling·\nLLM based classification\n1 Introduction\nRequirements Engineering (RE) concerns the identification, documentation,\nand management of software requirements (Sommerville and Sawyer, 1997).\nThe increasing interconnectivity of information systems forces organizations\nto provide secure services in cyberspace. Integrating security during initial\nrequirements analysis is essential to proactively mitigate risks, reduce vulner-\nabilities, and reduce the high costs associated with downstream remediation\n(Villamizar et al., 2018). This proactive approach is central to the ”Shift-\nLeft” security paradigm, which advocates for the identification and mitigation\nof vulnerabilities as early as possible in the Software Development Life Cycle\n(SDLC).\nAlthough Security Requirements Engineering (SRE) has matured as a dis-\ncipline, its effective application in practice remains challenging. The issue is\nnot the absence of security considerations, but the difficulty of ensuring that\nsecurity requirements are properly validated and verified. An empirical investi-\ngation that combines a systematic literature review with a survey of 58 indus-\ntry professionals highlights this gap (Alam et al., 2025). While practitioners\nreported a moderate understanding of SRE practices, Security Requirements\nAssurance (SRA) was found to be poorly understood and rarely applied in\nindustrial contexts. The study concludes that validating and ensuring security\nrequirements remains a major and unresolved challenge in practice.\nThis assurance gap persists due to the documented lack of specialized se-\ncurity training for developers in early-phase RE (Andrade et al., 2023). The\ntask relies heavily on scarce specialized expertise and time-intensive manual\nanalysis. The main challenge is that critical security requirements are often not\nexplicitly stated. Instead, they are implicitly embedded within standard func-\ntional descriptions. For example, a seemingly simple functional requirement,', 'CONTEXT: This chunk discusses the challenges and methodologies involved in addressing class imbalance and reproducibility issues in the classification of implicit security requirements, highlighting the use of experimental LLMs for advanced reasoning and the trade-offs between dataset curation effort and model performance.\n\nCONTENT: Implicit Security Requirements Classification 33\nlacked prior experience in Natural Language Processing (NLP) annotation,\nthis was addressed through focused training sessions to align their domain\nexpertise with the technical requirements of the MATTER cycle.\nVariations in model versions and dependencies (e.g., library updates) pose\nchallenges to reproducibility.\nA significant class imbalance, with categories like V6 and V12 severely\nunderrepresented, restricts model learning and generalization. To address this,\nextensive training, refined annotation protocols, iterative consensus-building,\nand detailed error analysis were employed.\nA significant challenge in evaluating Large Language Models (LLMs) is the\nrapid pace of model iteration. In this study, we deliberately selected several\n“experimental” and “preview” models (specifically gemini-2.0-flash-thinking-\nexp-01-21, gemini-2.0-pro-exp-02-05, and gemini-2.5-flash-preview-04-17) along-\nside stable releases. The rationale for including these unversioned, experi-\nmental models was to benchmark the absolute state-of-the-art capabilities in\n“reasoning” (Chain-of-Thought) and architectural efficiency that are not yet\npresent in General Availability (GA) versions at the time of the study. We\nhypothesized that the improved reasoning capabilities of models like Flash-\nThinking would be particularly advantageous for identifying implicit security\nrequirements, which often require multi-step deduction rather than simple pat-\ntern matching.\nHowever, we acknowledge that the use of experimental endpoints poses a\nthreat to internal validity regarding reproducibility. These models are subject\nto deprecation or unannounced updates by the provider, meaning other re-\nsearchers may not be able to query the exact same model weights in the future.\nTo mitigate this, we have documented the exact model identifier strings used\nduring our data collection window. While this does not guarantee future ac-\ncess to these specific snapshots, it ensures transparency regarding the specific\ntechnological generation evaluated. We argue that this trade-off is necessary\nto provide a forward-looking analysis of how emerging LLM capabilities (such\nas intrinsic reasoning) impact the field of Security Requirements Engineering.\n7 Discussion\nThe combined experimental results obtained from fine-tuning BERT models\nand employing various LLM prompting strategies, further refined by the com-\nparison between zero-shot and few-shot methods, yield several key insights.\nAn important dimension of our comparison is the trade-off between the ef-\nfort required to curate the dataset and the resulting classification performance.\nAs detailed in Section 3, the construction of the ASRD involved 267 person-\nhours of expert labor to annotate 2,652 requirements. This extensive effort\nwas a prerequisite for the fine-tuned BERT models, which rely on large-scale\nsupervised data.\nAs summarized in Table 9, the BERT Fine-Tuning approach achieved the\nhighest performance (Macro-F1 0.942) but at the highest cost. In contrast, the', 'CONTEXT: This chunk focuses on the comparative performance analysis of models across OWASP ASVS classes, highlighting challenges with rare categories (e.g., Stored Cryptography, File Handling) through heatmaps and gap analysis, while addressing dataset limitations and threats to validity related to linguistic diversity and generalizability.\n\nCONTENT: 30 Yusuf G¨ ur et al.\nFig. 2: Critical Difference (CD) diagram visualizing the statistical comparison\nof classifiers. The horizontal axis represents the average rank of each model\nacross all 11 OWASP ASVS classes (lower ranks indicate better performance).\nThe Critical Difference (CD = 10.46) represents the minimum difference in\naverage rank required for two models to be considered statistically significantly\ndifferent atp <0.05.\n5.6 Comparative Visualization Analysis\nTo complement the statistical rankings, we visualized the performance land-\nscape across all 20 model configurations.\nHeatmap Analysis: Figure 3 shows F1-scores by class. Requirements related\nto V2 (Authentication), V3 (Session Management), and V4 (Access Control)\nare handled well by nearly all models, with scores typically above 0.95. These\ncategories are common and appear consistently across training sources. In\ncontrast, V6 (Stored Cryptography) and V12 (Files and Resources) show low\nscores across the board, including for GPT-4o and Gemini 2.0 Pro. This pat-\ntern points to a systematic difficulty with sparse categories rather than a\nweakness of a particular model.\nGap Analysis: Figure 4 contrasts average performance on frequent and in-\nfrequent classes. Fine-tuned models such as SecureBERT and BERT-Multilingual\nperform better on rare classes, with F1-scores around 0.70–0.80. Zero-shot\nLLMs perform poorly in these cases, often below 0.30. Few-shot prompting\nimproves results for some models, such as Gemini 2.0, but the gap remains\nsubstantial.\n6 Threats to Validity\nOur dataset consists of 2,652 requirements derived from six Software Require-\nments Specification (SRS) documents. While the number of source documents\n\nImplicit Security Requirements Classification 31\nFig. 3:Comprehensive Performance Heatmap.F1-scores for all 20 eval-\nuated models across 11 OWASP ASVS classes. The visualization highlights\nthe “zone of failure” (red) for classes V6 and V12 across models, contrasting\nwith the high performance (green) on ubiquitous classes like V2 and V4.\nis limited, potential bias was mitigated by strategically selecting projects from\nsix distinct business domains: Construction, Legal, Education, Software, Meet-\ning Management, and Accommodation. This semantic diversity ensures that\nthe models are evaluated on a broad vocabulary and varied functional contexts\nrather than a single industry vertical. A specific consideration regarding the\nexternal validity of this study is that the ASRD is a Turkish-language corpus\nwhich allows for the evaluation of automated tools in linguistically diverse\nindustrial contexts.\nFurthermore, because security requirements (e.g., Authentication, Log-\nging) represent cross-cutting concerns that apply universally across software\nsystems, we believe the fundamental patterns captured in the ASRD are trans-\nferable to other industries. Our current evaluation used a stratified split across\nthe entire dataset, meaning requirements from all six source projects are\npresent in the training, validation, and test sets. However, we acknowledge\nthat this approach does not fully measure the models’ ability to generalize to\nentirely new projects with different authors or linguistic styles, as the test set\nis not completely “unseen” in terms of project context.']","The Agency Security Requirements Dataset (ASRD) contains **2,652 functional requirements**. These requirements were derived from **six real-world Software Requirements Specification (SRS) documents** across diverse business domains: Construction, Legal, Education, Software, Meeting Management, and Accommodation. The dataset was meticulously curated through a multi-stage process, including rigorous filtering, anonymization, and expert annotation, and is based on the OWASP ASVS taxonomy for security classification.

(Source: temp/b31390a6-3d40-4aa2-b634-17feb052e723_template.pdf)","The ASRD comprises 2,652 real-world requirement statements derived from six diverse Software Requirements Specification (SRS) documents from a government agency[cite: 12, 71].",0.9249999999815,1.0,0.722152709386786
Who performed the annotations for the ASRD?,"['CONTEXT: **Comparison of Effort vs. Performance and Validation of ASRD Dataset**: This chunk discusses the trade-offs between data preparation effort and model performance across fine-tuned BERT models and LLM approaches, highlights the advantages of the ASRD dataset over existing benchmarks, and emphasizes the challenges of implicit security requirement elicitation in real-world contexts.\n\nCONTENT: 34 Yusuf G¨ ur et al.\nTable 9: Comparison of Effort vs. Performance across Approaches\nApproach Data Requirement Est. Setup Effort Best Macro-F1\nBERT Fine-Tuning High (∼2,600 ex) High (267 hours)0.942\nLLM Few-Shot Low (∼30 ex) Low (<10 hours)0.941\nLLM Zero-Shot None Minimal (<2 hours) 0.890\nLLM Few-Shot approach (using Gemini 2.0) achieved a similar performance\n(Macro-F1 0.941) while requiring only the selection of a small number of in-\ncontext examples (approx. 30 examples per prompt).\nThis comparison points to diminishing returns from large-scale annotation\nwhen capable LLMs are available. Although extensive annotation was required\nhere to establish a reliable ground truth and validate the test set, the results\nindicate that, in industrial settings, a few-shot LLM approach can offer a more\nfavorable balance between effort and accuracy. Comparable performance can\nbe reached with substantially less data preparation, reducing the need for\nprolonged expert annotation.\nThe main contribution of this work is the introduction and validation of\nthe ASRD. The consistent performance of both fine-tuned BERT models and\nLLM-based approaches on ASRD supports its reliability and practical value.\nThe dataset addresses a persistent limitation in SRE research which is the\nlack of large, well-curated, professionally sourced benchmarks for security re-\nquirement classification. Compared with existing datasets, ASRD offers sev-\neral advantages. Unlike DOSSPRE (Kadebu et al., 2023), which is derived\nfrom student projects, ASRD includes 2,652 requirements from six commer-\ncial software systems, reflecting real industrial language and complexity. Its\nOWASP ASVS–based taxonomy spans 11 security classes, providing finer de-\ntail than PROMISE exp (Lima et al., 2019), where security is treated as a\nsingle category. Although the healthcare dataset by Riaz et al. (Riaz et al.,\n2014) is larger, it focuses on six broad objectives within a single domain, lim-\niting its scope. By contrast, ASRD supports cross-domain analysis and aligns\nwith a framework widely used in practice.\nThe dataset also benefits from a rigorous annotation process. Three cy-\nbersecurity specialists, each with over 15 years of experience, labeled the data\nusing an iterative MATTER-based procedure. This resulted in strong agree-\nment (Fleiss’ Kappa = 0.82) and helps mitigate the subjectivity that often\naffects security-related requirement datasets.\nBeyond the dataset construction, this study highlights the complexity of\nimplicit security requirement elicitation. A significant portion of security vul-\nnerabilities stems not from missing explicit security constraints (e.g., “The\nsystem shall encrypt passwords”), but from functional requirements that im-\nplicitly demand robust security controls. Our analysis of the ASRD reveals\nthat functional descriptions of business logic often conceal a dense network of\nsecurity dependencies. A representative example from our results is Require-\nment No. 2 (“The system will carry out the asset handover process”). While', 'CONTEXT: This chunk provides an overview of the Agency Security Requirements Dataset (ASRD), detailing its real-world data sources, expert annotation process, and multi-label structure based on OWASP ASVS categories, while also introducing the research questions and comparing the performance of fine-tuned transformer models and prompt-based approaches for classifying implicit security requirements.\n\nCONTENT: 4 Yusuf G¨ ur et al.\ncient and cost-effective compared to addressing security issues at later stages\nof development ((Khan et al., 2024).\nThis paper proposes a machine learning-based system to automatically an-\nalyze SRS documents and classify security requirements according to OWASP\nASVS. Its main contribution is the Agency Security Requirements Dataset\n(ASRD), a publicly available corpus created to support research in automated\nSRA. The ASRD comprises:\n–Real-World Data: 2,652 Software Requirements Specification (SRS) sen-\ntences extracted from six diverse, real-world industry projects,\n–Expert Annotation: Meticulously annotated by three cybersecurity experts,\neach with over 15 years of industry experience,\n–Rigorous Methodology: Developed using the iterative MATTER cycle an-\nnotation framework (Pustejovsky and Stubbs, 2012),\n–High Granularity and Multi-label Structure: Mapped to 11 distinct, action-\nable security categories from the industry-standard OWASP Application\nSecurity Verification Standard (ASVS,V2–V13), allowing each require-\nment to be associated with multiple categories.\nThe ASRD is intended as a shared research resource and benchmark. Using\nthis dataset, the study conducts an empirical analysis of automated classifica-\ntion approaches and addresses the following research questions:\n1. To what extent can fine-tuned transformer models accurately perform\nmulti-label classification of implicit security requirements into OWASP\nASVS categories using the ASRD?\n2. How does the performance of prompt-based approaches (zero-shot and few-\nshot) using modern LLMs compare to fine-tuned BERT-based models and\nnaive baselines for this task?\nThe results show that few-shot prompting with a modern language model\n(Gemini 2.0) achieves a macro-average F1 score of 0.941, closely matching the\nbest fine-tuned BERT-based model (0.942). This demonstrates that example-\nguided inference can reach performance comparable to supervised fine-tuning\nwhile requiring substantially less annotated data and model retraining. The\nevaluation compares fine-tuned transformer models (e.g., SecureBERT) with\nprompt-based methods in zero-shot and few-shot settings across multiple con-\ntemporary models, demonstrating that prompt-based approaches provide a\npractical and resource-efficient alternative for identifying implicit security re-\nquirements in industrial SRS documents.\nThe remainder of this paper is organized as follows. Section 2 reviews re-\nlated work on NLP-based requirements classification, security frameworks, and\nexisting security datasets. Section 3 describes the curation and expert anno-\ntation of the ASRD and summarizes its key characteristics. Section 4 outlines\nthe experimental setup and classification methods, including fine-tuned BERT\nmodels and prompt-based approaches. Section 5 reports the experimental re-\nsults. Section 6 discusses the findings and their implications, with a com-\nparative analysis of fine-tuning and prompting strategies. Section 7 examines', ""CONTEXT: **Dataset Filtering and Composition for ASRD**: This chunk details the rigorous filtering, randomization, and composition process used to create the Agency Security Requirements Dataset (ASRD), a Turkish-language corpus of 2,652 functional requirements across six business domains, annotated for security classification using the OWASP ASVS taxonomy. It also provides sample requirements to illustrate the dataset's diversity and focus on implicit security dependencies.\n\nCONTENT: 10 Yusuf G¨ ur et al.\n3. Filtering: The initial extraction yielded 3,264 items. To ensure the qual-\nity of the dataset, domain experts conducted a rigorous manual filtering\npass to remove 612 items. This exclusion process went beyond simple for-\nmatting artifacts (such as headers or tables of contents). We specifically\nremoved non-functional administrative clauses that addressed the vendor\nrather than the system (e.g., “The contractor must provide training man-\nuals”), project management constraints (e.g., “The project must be deliv-\nered by Q3”), and requirements with ambiguous scope that lacked clear\nverification criteria.\n4. Randomization: The remaining requirements were randomized and masked\nto mitigate potential annotation bias related to the document source or the\nsequence of requirements.\nThe resulting ASRD comprises 2,652 well-formed functional requirements\nacross six distinct business areas: Construction (1,202), Legal (177), Education\n(312), Software (199), Meeting (126), and Accommodation (636). A sample of\nthese requirements is provided in Table 2. The requirements included in this\nsample were selected based on three specific criteria to ensure a representative\noverview:\n–Samples were chosen to reflect the varied terminology and phrasing styles\nused across the six different business domains.\n–We prioritized requirements that demonstrate how standard functional de-\nscriptions conceal underlying security dependencies.\n–We ensured that the sample includes representative examples for all 11\nOWASP ASVS security classes used in the study to demonstrate the full\nscope of the classification task.\nIt is important to note that the ASRD is a Turkish-language corpus; all experi-\nments detailed in this study, including BERT fine-tuning and LLM prompting,\nwere conducted exclusively on the original text. English translations are pro-\nvided only for readability.\nTable 2: Sample Requirements from ASRD\nNo Original Requirements Translated Requirements\n1 Sistem, yetkili kullanıcının hukuk\ndosyası t¨ ur¨ u kayıt etmesini\nsa˘ glayacaktır.\nThe system shall allow authorized\nusers to record the legal file type.\n2 Sistem, kullanıcının kullanılmamı¸ s\nhukuk dosyası t¨ ur¨ un¨ u g¨ uncellemesini\nsa˘ glayacaktır.\nThe system shall allow the user to up-\ndate an unused legal file type.\n3 Sistem, kullanıcının kullanılmamı¸ s\nolan hukuk dosyası t¨ ur¨ un¨ u silmesini\nsa˘ glayacaktır.\nThe system shall allow the user to\ndelete an unused legal file type.\n4 Sistem, kullanıcının hukuk dosyası\nt¨ urlerini sorgulamasını ve listelemesini\nsa˘ glayacaktır.\nThe system shall allow the user to\nquery and list legal file types."", 'CONTEXT: This chunk discusses the curation and preprocessing of the Agency Security Requirements Dataset (ASRD), detailing the multi-stage pipeline used to extract, anonymize, and prepare real-world Turkish-language Software Requirements Specification (SRS) documents for multi-label security annotation, while addressing gaps in prior datasets.\n\nCONTENT: Implicit Security Requirements Classification 9\n2.3.3 Electronic Health Domain Dataset\nThis corpus focuses on the electronic healthcare industry, a sector defined\nby stringent regulatory and high-assurance demands (Riaz et al., 2014). The\ndataset’s strength lies in its rigorous multi-stage annotation methodology,\nwhich utilized independent researchers and a mediator to ensure high inter-\nannotator agreement. While it provides a large-scale, authentic resource for\nhealthcare-specific tasks, its narrow focus may limit the generalizability of\nmodels to other industries, and its high-level security objectives may be too\ncoarse for detailed technical analysis.\n2.3.4 Summary of Datasets\nAs summarized in Table 1, these datasets vary significantly in their suitability\nfor tasks ranging from general non-functional requirement (NFR) classifica-\ntion to detailed security analysis. Our proposed Agency Security Require-\nments Dataset (ASRD) addresses gaps in prior work by adopting the OWASP\nASVS taxonomy for multi-label annotation. Furthermore, by utilizing real-\nworld projects originally written in Turkish, ASRD introduces both industrial\ncomplexity and linguistic diversity to the field.\n3 Dataset Curation, Annotation, and Evaluation\n3.1 Data Collection and Preprocessing\nThe dataset compilation began with the collection of six Software Require-\nments Specification (SRS) documents from an Agency’s Security Department.\nThese documents were selected from an initial pool of 19 projects executed\nbetween 2019 and 2023, specifically chosen for their domain diversity and\namenability to effective anonymization. The original documents were provided\nin standard document formats (e.g., PDF, DOCX) where requirements were\norganized in itemized lists. To transform these documents into a machine-\nreadable dataset, we employed a multi-stage preprocessing pipeline:\n1. Sentence Segmentation and Extraction: As the source SRS documents uti-\nlized standard itemization for requirements, we employed a rule-based ex-\ntraction method. We utilized pattern matching to identify requirement\nblocks based on hierarchical numbering schemes and line breaks. Each iden-\ntified requirement was extracted and treated as a single processing unit,\npreserving the structural integrity of the original specification.\n2. Anonymization: To protect sensitive data, a semi-automated anonymiza-\ntion process was implemented. Custom scripts replaced entities such as\nproject names, specific URLs, and IP addresses with generic placeholders.\nThis was followed by a manual review by domain experts to ensure that\nthe removal of personally identifiable information (PII) did not compromise\nthe semantic meaning or contextual integrity of the requirements.', 'CONTEXT: This chunk discusses the challenges and methodologies involved in addressing class imbalance and reproducibility issues in the classification of implicit security requirements, highlighting the use of experimental LLMs for advanced reasoning and the trade-offs between dataset curation effort and model performance.\n\nCONTENT: Implicit Security Requirements Classification 33\nlacked prior experience in Natural Language Processing (NLP) annotation,\nthis was addressed through focused training sessions to align their domain\nexpertise with the technical requirements of the MATTER cycle.\nVariations in model versions and dependencies (e.g., library updates) pose\nchallenges to reproducibility.\nA significant class imbalance, with categories like V6 and V12 severely\nunderrepresented, restricts model learning and generalization. To address this,\nextensive training, refined annotation protocols, iterative consensus-building,\nand detailed error analysis were employed.\nA significant challenge in evaluating Large Language Models (LLMs) is the\nrapid pace of model iteration. In this study, we deliberately selected several\n“experimental” and “preview” models (specifically gemini-2.0-flash-thinking-\nexp-01-21, gemini-2.0-pro-exp-02-05, and gemini-2.5-flash-preview-04-17) along-\nside stable releases. The rationale for including these unversioned, experi-\nmental models was to benchmark the absolute state-of-the-art capabilities in\n“reasoning” (Chain-of-Thought) and architectural efficiency that are not yet\npresent in General Availability (GA) versions at the time of the study. We\nhypothesized that the improved reasoning capabilities of models like Flash-\nThinking would be particularly advantageous for identifying implicit security\nrequirements, which often require multi-step deduction rather than simple pat-\ntern matching.\nHowever, we acknowledge that the use of experimental endpoints poses a\nthreat to internal validity regarding reproducibility. These models are subject\nto deprecation or unannounced updates by the provider, meaning other re-\nsearchers may not be able to query the exact same model weights in the future.\nTo mitigate this, we have documented the exact model identifier strings used\nduring our data collection window. While this does not guarantee future ac-\ncess to these specific snapshots, it ensures transparency regarding the specific\ntechnological generation evaluated. We argue that this trade-off is necessary\nto provide a forward-looking analysis of how emerging LLM capabilities (such\nas intrinsic reasoning) impact the field of Security Requirements Engineering.\n7 Discussion\nThe combined experimental results obtained from fine-tuning BERT models\nand employing various LLM prompting strategies, further refined by the com-\nparison between zero-shot and few-shot methods, yield several key insights.\nAn important dimension of our comparison is the trade-off between the ef-\nfort required to curate the dataset and the resulting classification performance.\nAs detailed in Section 3, the construction of the ASRD involved 267 person-\nhours of expert labor to annotate 2,652 requirements. This extensive effort\nwas a prerequisite for the fine-tuned BERT models, which rely on large-scale\nsupervised data.\nAs summarized in Table 9, the BERT Fine-Tuning approach achieved the\nhighest performance (Macro-F1 0.942) but at the highest cost. In contrast, the', 'CONTEXT: **Comparison of Fine-Tuned Transformers and LLMs for Security Requirements Classification**  \nThis chunk discusses the performance comparison between fine-tuned transformer models and few-shot prompting with LLMs (e.g., Gemini 2.0), highlighting their near-equal effectiveness in classifying implicit security requirements within the ASRD dataset, while emphasizing the challenges of integrating security early in the SDLC through the ""Shift-Left"" approach.\n\nCONTENT: 2 Yusuf G¨ ur et al.\ntional fine-tuned transformer models and contemporary Large Language Mod-\nels (LLMs) employing few-shot and zero-shot prompt engineering strategies.\nThe results show that a few-shot prompting with Gemini 2.0 achieves a macro-\naverage F1 score of 0.941, directly comparable to the fine-tuned BERT model’s\n0.942. This study culminates in two primary findings: first, the validation and\npublication of the ASRD, a high-granularity, multi-label dataset for implicit\nsecurity requirements based on OWASP ASVS V2-V13; and second, the di-\nrect comparison demonstrating that few-shot Large Language Models (LLMs)\nachieve competitive multi-label classification performance (Macro-F1 0.941)\nnearly equal to resource-intensive fine-tuned transformer models (Macro-F1\n0.942). This confirms that LLMs represent a highly practical and resource-\nsaving strategy for automating the identification of embedded (implicit) secu-\nrity requirements for software security in industrial SRS documents\nKeywordsCybersecurity requirement elicitation·OWASP based security\nrequirement classification·OWASP ASVS-based Requirement Labeling·\nLLM based classification\n1 Introduction\nRequirements Engineering (RE) concerns the identification, documentation,\nand management of software requirements (Sommerville and Sawyer, 1997).\nThe increasing interconnectivity of information systems forces organizations\nto provide secure services in cyberspace. Integrating security during initial\nrequirements analysis is essential to proactively mitigate risks, reduce vulner-\nabilities, and reduce the high costs associated with downstream remediation\n(Villamizar et al., 2018). This proactive approach is central to the ”Shift-\nLeft” security paradigm, which advocates for the identification and mitigation\nof vulnerabilities as early as possible in the Software Development Life Cycle\n(SDLC).\nAlthough Security Requirements Engineering (SRE) has matured as a dis-\ncipline, its effective application in practice remains challenging. The issue is\nnot the absence of security considerations, but the difficulty of ensuring that\nsecurity requirements are properly validated and verified. An empirical investi-\ngation that combines a systematic literature review with a survey of 58 indus-\ntry professionals highlights this gap (Alam et al., 2025). While practitioners\nreported a moderate understanding of SRE practices, Security Requirements\nAssurance (SRA) was found to be poorly understood and rarely applied in\nindustrial contexts. The study concludes that validating and ensuring security\nrequirements remains a major and unresolved challenge in practice.\nThis assurance gap persists due to the documented lack of specialized se-\ncurity training for developers in early-phase RE (Andrade et al., 2023). The\ntask relies heavily on scarce specialized expertise and time-intensive manual\nanalysis. The main challenge is that critical security requirements are often not\nexplicitly stated. Instead, they are implicitly embedded within standard func-\ntional descriptions. For example, a seemingly simple functional requirement,', 'CONTEXT: This chunk discusses the challenges of class imbalance in security requirements classification, the impact of few-shot prompting on model performance, and the potential of hybrid approaches like retrieval-augmented generation (RAG) to improve results, particularly for underrepresented security classes. It also highlights future research directions, including the integration of LLMs with agent-based components to enhance automated security analysis.\n\nCONTENT: 36 Yusuf G¨ ur et al.\nfocus their efforts on high-risk adjudication rather than manual discovery. Our\nresults suggest that commercially available LLMs can serve as always-available\nassistants, democratizing high-level security analysis for development teams.\nFurthermore, future studies could investigate the impact of this tool on prac-\ntitioners with varying levels of experience. Specifically, research is needed to\ndetermine if such automation allows junior developers to identify security con-\ncerns more accurately or if it primarily eases the cognitive load for senior\nexperts by filtering out routine classifications.\nThis study highlights class imbalance as a persistent challenge in SRE.\nThe issue is not specific to ASRD but reflects the nature of the domain itself.\nIn typical specifications, references to architectural security concerns such as\ncryptographic storage or low-level file handling, occur far less often than user-\nfacing controls like authentication (V2) or access control (V4). This creates\na “long-tail” distribution where the most important security failures often\nreside in the least represented classes. Standard supervised learning and even\nfew-shot prompting struggle to generalize from such limited examples. This is\na well-documented issue in requirements datasets, including benchmarks like\nPROMISE exp (Lima et al., 2019), which also suffers from imbalanced classes.\nThe improvement from zero-shot to few-shot prompting shows the positive\nimpact of adding in-context examples. For example, Gemma 4b’s F1 score on\nclass V5 rose from 0.709 to 0.814 with only a small number of examples. This\ngain shows how limited, well-chosen examples can steer model behavior toward\nthe intended classification task. Similar observations have been reported in\nprior work across domains, where prompt design and example selection were\nshown to play a decisive role in task performance (Mann et al., 2020; Liu et al.,\n2023a).\n8 Conclusion and Future Work\nThis work examined the feasibility of automated security requirements classi-\nfication under realistic conditions, where requirements are implicit, unevenly\ndistributed across categories, and drawn from industrial settings rather than\ncurated benchmarks. By introducing ASRD and evaluating supervised and\nprompt-based methods on a shared benchmark, this work enables direct com-\nparison across approaches. The results show that few-shot prompting is suit-\nable when labeled data is limited, while fine-tuned models are better at han-\ndling rare security requirements.\nAs a future work, we plan to explore hybrid SRE approaches that combine\nLLMs with retrieval and agent-based components. In particular, improving\nperformance on underrepresented classes remains an open problem and may\nbenefit from targeted data augmentation, retrieval-augmented methods (RAG)\nsuch as those proposed by Liu et al. (Liu et al., 2025), or agent-based strategies.\nFor instance, instead of relying on learned patterns from non-existent training\nexamples, a RAG-enabled system can dynamically retrieve the specific veri-\nfication requirements and definitions relevant to the input text at inference', 'CONTEXT: This chunk focuses on the comparative performance analysis of models across OWASP ASVS classes, highlighting challenges with rare categories (e.g., Stored Cryptography, File Handling) through heatmaps and gap analysis, while addressing dataset limitations and threats to validity related to linguistic diversity and generalizability.\n\nCONTENT: 30 Yusuf G¨ ur et al.\nFig. 2: Critical Difference (CD) diagram visualizing the statistical comparison\nof classifiers. The horizontal axis represents the average rank of each model\nacross all 11 OWASP ASVS classes (lower ranks indicate better performance).\nThe Critical Difference (CD = 10.46) represents the minimum difference in\naverage rank required for two models to be considered statistically significantly\ndifferent atp <0.05.\n5.6 Comparative Visualization Analysis\nTo complement the statistical rankings, we visualized the performance land-\nscape across all 20 model configurations.\nHeatmap Analysis: Figure 3 shows F1-scores by class. Requirements related\nto V2 (Authentication), V3 (Session Management), and V4 (Access Control)\nare handled well by nearly all models, with scores typically above 0.95. These\ncategories are common and appear consistently across training sources. In\ncontrast, V6 (Stored Cryptography) and V12 (Files and Resources) show low\nscores across the board, including for GPT-4o and Gemini 2.0 Pro. This pat-\ntern points to a systematic difficulty with sparse categories rather than a\nweakness of a particular model.\nGap Analysis: Figure 4 contrasts average performance on frequent and in-\nfrequent classes. Fine-tuned models such as SecureBERT and BERT-Multilingual\nperform better on rare classes, with F1-scores around 0.70–0.80. Zero-shot\nLLMs perform poorly in these cases, often below 0.30. Few-shot prompting\nimproves results for some models, such as Gemini 2.0, but the gap remains\nsubstantial.\n6 Threats to Validity\nOur dataset consists of 2,652 requirements derived from six Software Require-\nments Specification (SRS) documents. While the number of source documents\n\nImplicit Security Requirements Classification 31\nFig. 3:Comprehensive Performance Heatmap.F1-scores for all 20 eval-\nuated models across 11 OWASP ASVS classes. The visualization highlights\nthe “zone of failure” (red) for classes V6 and V12 across models, contrasting\nwith the high performance (green) on ubiquitous classes like V2 and V4.\nis limited, potential bias was mitigated by strategically selecting projects from\nsix distinct business domains: Construction, Legal, Education, Software, Meet-\ning Management, and Accommodation. This semantic diversity ensures that\nthe models are evaluated on a broad vocabulary and varied functional contexts\nrather than a single industry vertical. A specific consideration regarding the\nexternal validity of this study is that the ASRD is a Turkish-language corpus\nwhich allows for the evaluation of automated tools in linguistically diverse\nindustrial contexts.\nFurthermore, because security requirements (e.g., Authentication, Log-\nging) represent cross-cutting concerns that apply universally across software\nsystems, we believe the fundamental patterns captured in the ASRD are trans-\nferable to other industries. Our current evaluation used a stratified split across\nthe entire dataset, meaning requirements from all six source projects are\npresent in the training, validation, and test sets. However, we acknowledge\nthat this approach does not fully measure the models’ ability to generalize to\nentirely new projects with different authors or linguistic styles, as the test set\nis not completely “unseen” in terms of project context.']","The annotations for the ASRD were performed by three cybersecurity specialists, each with over 15 years of industry experience. They followed an iterative MATTER-based procedure to label the data, ensuring strong agreement (Fleiss’ Kappa = 0.82) and mitigating subjectivity in the annotation process. 

Source: temp/b31390a6-3d40-4aa2-b634-17feb052e723_template.pdf","Three cybersecurity subject matter experts (SME) from the agency, each with over 15 years of industry experience, performed the annotations[cite: 71, 241].",0.99999999995,1.0,0.9186133023860611
Which specific OWASP ASVS classes were excluded from the study and why?,"['CONTEXT: This chunk focuses on the comparative performance analysis of models across OWASP ASVS classes, highlighting challenges with rare categories (e.g., Stored Cryptography, File Handling) through heatmaps and gap analysis, while addressing dataset limitations and threats to validity related to linguistic diversity and generalizability.\n\nCONTENT: 30 Yusuf G¨ ur et al.\nFig. 2: Critical Difference (CD) diagram visualizing the statistical comparison\nof classifiers. The horizontal axis represents the average rank of each model\nacross all 11 OWASP ASVS classes (lower ranks indicate better performance).\nThe Critical Difference (CD = 10.46) represents the minimum difference in\naverage rank required for two models to be considered statistically significantly\ndifferent atp <0.05.\n5.6 Comparative Visualization Analysis\nTo complement the statistical rankings, we visualized the performance land-\nscape across all 20 model configurations.\nHeatmap Analysis: Figure 3 shows F1-scores by class. Requirements related\nto V2 (Authentication), V3 (Session Management), and V4 (Access Control)\nare handled well by nearly all models, with scores typically above 0.95. These\ncategories are common and appear consistently across training sources. In\ncontrast, V6 (Stored Cryptography) and V12 (Files and Resources) show low\nscores across the board, including for GPT-4o and Gemini 2.0 Pro. This pat-\ntern points to a systematic difficulty with sparse categories rather than a\nweakness of a particular model.\nGap Analysis: Figure 4 contrasts average performance on frequent and in-\nfrequent classes. Fine-tuned models such as SecureBERT and BERT-Multilingual\nperform better on rare classes, with F1-scores around 0.70–0.80. Zero-shot\nLLMs perform poorly in these cases, often below 0.30. Few-shot prompting\nimproves results for some models, such as Gemini 2.0, but the gap remains\nsubstantial.\n6 Threats to Validity\nOur dataset consists of 2,652 requirements derived from six Software Require-\nments Specification (SRS) documents. While the number of source documents\n\nImplicit Security Requirements Classification 31\nFig. 3:Comprehensive Performance Heatmap.F1-scores for all 20 eval-\nuated models across 11 OWASP ASVS classes. The visualization highlights\nthe “zone of failure” (red) for classes V6 and V12 across models, contrasting\nwith the high performance (green) on ubiquitous classes like V2 and V4.\nis limited, potential bias was mitigated by strategically selecting projects from\nsix distinct business domains: Construction, Legal, Education, Software, Meet-\ning Management, and Accommodation. This semantic diversity ensures that\nthe models are evaluated on a broad vocabulary and varied functional contexts\nrather than a single industry vertical. A specific consideration regarding the\nexternal validity of this study is that the ASRD is a Turkish-language corpus\nwhich allows for the evaluation of automated tools in linguistically diverse\nindustrial contexts.\nFurthermore, because security requirements (e.g., Authentication, Log-\nging) represent cross-cutting concerns that apply universally across software\nsystems, we believe the fundamental patterns captured in the ASRD are trans-\nferable to other industries. Our current evaluation used a stratified split across\nthe entire dataset, meaning requirements from all six source projects are\npresent in the training, validation, and test sets. However, we acknowledge\nthat this approach does not fully measure the models’ ability to generalize to\nentirely new projects with different authors or linguistic styles, as the test set\nis not completely “unseen” in terms of project context.', ""CONTEXT: **Dataset Filtering and Composition for ASRD**: This chunk details the rigorous filtering, randomization, and composition process used to create the Agency Security Requirements Dataset (ASRD), a Turkish-language corpus of 2,652 functional requirements across six business domains, annotated for security classification using the OWASP ASVS taxonomy. It also provides sample requirements to illustrate the dataset's diversity and focus on implicit security dependencies.\n\nCONTENT: 10 Yusuf G¨ ur et al.\n3. Filtering: The initial extraction yielded 3,264 items. To ensure the qual-\nity of the dataset, domain experts conducted a rigorous manual filtering\npass to remove 612 items. This exclusion process went beyond simple for-\nmatting artifacts (such as headers or tables of contents). We specifically\nremoved non-functional administrative clauses that addressed the vendor\nrather than the system (e.g., “The contractor must provide training man-\nuals”), project management constraints (e.g., “The project must be deliv-\nered by Q3”), and requirements with ambiguous scope that lacked clear\nverification criteria.\n4. Randomization: The remaining requirements were randomized and masked\nto mitigate potential annotation bias related to the document source or the\nsequence of requirements.\nThe resulting ASRD comprises 2,652 well-formed functional requirements\nacross six distinct business areas: Construction (1,202), Legal (177), Education\n(312), Software (199), Meeting (126), and Accommodation (636). A sample of\nthese requirements is provided in Table 2. The requirements included in this\nsample were selected based on three specific criteria to ensure a representative\noverview:\n–Samples were chosen to reflect the varied terminology and phrasing styles\nused across the six different business domains.\n–We prioritized requirements that demonstrate how standard functional de-\nscriptions conceal underlying security dependencies.\n–We ensured that the sample includes representative examples for all 11\nOWASP ASVS security classes used in the study to demonstrate the full\nscope of the classification task.\nIt is important to note that the ASRD is a Turkish-language corpus; all experi-\nments detailed in this study, including BERT fine-tuning and LLM prompting,\nwere conducted exclusively on the original text. English translations are pro-\nvided only for readability.\nTable 2: Sample Requirements from ASRD\nNo Original Requirements Translated Requirements\n1 Sistem, yetkili kullanıcının hukuk\ndosyası t¨ ur¨ u kayıt etmesini\nsa˘ glayacaktır.\nThe system shall allow authorized\nusers to record the legal file type.\n2 Sistem, kullanıcının kullanılmamı¸ s\nhukuk dosyası t¨ ur¨ un¨ u g¨ uncellemesini\nsa˘ glayacaktır.\nThe system shall allow the user to up-\ndate an unused legal file type.\n3 Sistem, kullanıcının kullanılmamı¸ s\nolan hukuk dosyası t¨ ur¨ un¨ u silmesini\nsa˘ glayacaktır.\nThe system shall allow the user to\ndelete an unused legal file type.\n4 Sistem, kullanıcının hukuk dosyası\nt¨ urlerini sorgulamasını ve listelemesini\nsa˘ glayacaktır.\nThe system shall allow the user to\nquery and list legal file types."", 'CONTEXT: This chunk delves into the challenges and findings related to classifying implicit security requirements in Software Requirements Specifications (SRS), highlighting the limitations of static few-shot prompting for minority classes, the comparative performance of fine-tuned BERT models and LLMs, and the practical implications for automating security compliance in industrial contexts.\n\nCONTENT: Implicit Security Requirements Classification 35\nexplicitly stated as a functional transaction, our domain experts classified this\nrequirement across nearly all OWASP ASVS categories. In an industrial con-\ntext, an “asset handover” implies a legally binding transfer of custody, ne-\ncessitating Authentication (V2) and Access Control (V4) to verify identity\nand authority, Data Protection (V8) for non-repudiation, and Secure Logging\n(V7) for auditability. A purely keyword-based or explicit extraction approach\nwould likely miss these dependencies, leaving the module vulnerable. Our find-\nings demonstrate that LLMs, when prompted with few-shot examples, can\neffectively mimic this expert reasoning, bridging the gap between functional\nspecifications and security compliance.\nWhile the ASRD dataset accurately reflects the natural distribution of se-\ncurity requirements in industrial projects, the performance drop on minority\nclasses reveals distinct limitations across the two model architectures eval-\nuated. For the LLMs, the limitation lies in the rigidity of Static Few-Shot\nPrompting. Our results indicate that providing a fixed set of 1–3 examples\n(Few-Shot) yielded inconsistent gains and, in cases like Llama-3 on V12, ac-\ntually degraded performance compared to Zero-Shot. This suggests that “im-\nplicit” requirements for minority classes are highly context-dependent. A fixed\nexample of V12 related to “file upload” does not help the LLM identify a V12\nrequirement related to “directory traversal.” Consequently, the models fail to\ngeneralize the concept of the minority class from static prompts, leading to\nthe low F1-scores (0.077–0.172) observed. This confirms that for rare, high-\nvariance security classes, static prompting is insufficient.\nOur study found that while fine-tuned BERT models provide strong results\n(up to 0.942 Macro-F1), few-shot prompting with a top-tier LLM like Gemini\n2.0 can achieve nearly identical performance (0.941 Macro-F1). This supports\nthe growing body of literature suggesting that in-context learning with large\nmodels can close the performance gap with task-specific fine-tuning, often\nwith significantly less labeled data. This shift from a traditional “pre-train,\nfine-tune” to a “pre-train, prompt, and predict” paradigm, as surveyed by\nLiu et al. (Liu et al., 2023a), offers a more agile and resource-efficient path\nfor deploying NLP solutions in specialized industrial contexts. Our results\nprovide a concrete example of this trend within the SRE domain and align\nwith findings given by Karlsson et.al (Karlsson et al., 2025).\nThese findings have immediate implications for the software industry, par-\nticularly in implementing “Security by Design” and “Shift-Left” strategies. In\ndiverse development environments—ranging from Agile teams to regulated sec-\ntors like healthcare or defense—developers often lack deep security expertise.\nThey focus on writing functional requirements (SRS) to meet business needs,\noften assuming security will be “added on” later. By automating the classi-\nfication of implicit security requirements using the method proposed in this\nstudy, organizations can instantly map functional specs to actionable OWASP\nASVS controls before a single line of code is written. Rather than replacing\nhuman oversight, this approach functions as a human-in-the-loop productiv-\nity multiplier. It serves as a sophisticated pre-screening tool that drastically\nspeeds up the initial analysis phase, allowing scarce cybersecurity experts to', 'CONTEXT: This chunk details the annotation methodology and dataset curation process for the ASRD, following the MATTER cycle framework. It emphasizes the selection of expert annotators, corpus preparation through anonymization, and the iterative development of annotation guidelines based on OWASP ASVS categories to ensure consistency and accuracy in classifying implicit security requirements.\n\nCONTENT: Implicit Security Requirements Classification 15\n3.3 Annotation Methodology (MATTER Cycle) & Dataset Curation\nThe annotation process followed the MATTER development cycle, an estab-\nlished iterative framework for corpus annotation (Pustejovsky and Stubbs,\n2012). Crucially, the annotation task was defined as a multi-label text classifi-\ncation problem. It consists of the phases Model, Annotate, Train, Test, Eval-\nuate, and Revise. The annotation guidelines were developed and iteratively\nrefined to ensure consistency and clarity.\nThe Model–Annotate cycle involved writing guidelines, selecting annota-\ntors, conducting trial annotations, and refining the process. Once stabilized,\nthis cycle was expanded to the full dataset to generate a Gold Standard corpus.\nThe annotation workflow included the following processes:\n–Annotator Selection: Following Bayerl and Paul’s recommendation to use\ndomain-aligned annotators (Bayerl and Paul, 2011), three cybersecurity\nsubject matter experts (SME) from the agency, each with over 15 years\nof experience in designing security architectures, analyzing threat models,\nand identifying security requirements, performed the annotations. Their\ndeep practical experience ensured accurate and context-aware interpreta-\ntion which aligned with the requirement “experts must demonstrate signif-\nicant skills, knowledge, and experience” emphasized by Hopkins (Hopkins\nand Unger, 2017). The senior manager among them also provided over-\nsight and led the adjudication process. The senior SME acted as both an\nannotator and the final adjudicator for resolving disagreements. They also\nprovided feedback to improve the quality and clarity of the annotation\nguideline.\n–Corpus Selection and Preparation: Cohen (Cohen et al., 2005) recommends\nthat during corpus design, balance and representativeness of the corpus\nis important. In line with his recommendation, of the 19 initial project\ndocuments, six were chosen for their domain diversity and potential for\neffective anonymization by the SMEs. These documents underwent a semi-\nautomated anonymization process to remove all project-specific and per-\nsonally identifiable information (PII). The process targeted entities such\nas names of individuals and organizations, project codenames, and specific\ntechnical identifiers. This was done using custom scripts with placeholder\nreplacement, followed by manual review to ensure that anonymization did\nnot alter the meaning, structure, or intent of the original requirements. The\nextracted requirements were then randomized and masked to prevent po-\ntential bias related to document origin or sequence during the annotation\nphase. Total corpus has 2,652 requirements (30 pilot + 2,622 independent\nannotations).\n–Annotation Guideline Development: The Annotation Guidelines’ develop-\nment involved an initial iterative process, refining the guidelines three times\nto resolve ambiguities before the formal assessment phase. Annotation rules\nwere based on OWASP ASVS categories V2–V13 and refined iteratively\nusing example annotations. Following best practices suggested by Cohen\n(Cohen et al., 2005), the guidelines and annotation documents ensured:', 'CONTEXT: **Evaluation of Model Challenges and Prompt Engineering Strategies**: This chunk discusses the challenges faced by models, particularly with underrepresented OWASP ASVS categories like V6 (Stored Cryptography) and V12 (File and Resources), and outlines the experimental setup for LLM prompt engineering, including zero-shot and few-shot configurations, persona-based prompting, and strategies to mitigate prompt sensitivity and contextual bias.\n\nCONTENT: 26 Yusuf G¨ ur et al.\nshowed both low Recall (0.658) and low Precision (0.781), indicating it was\nboth missing true cases and incorrectly labeling other requirements as V6.\nModels struggle significantly with underrepresented categories, particularly\nV12 (File and Resources) and V6 (Stored Cryptography). This is evident in\nthe high number of false negatives, where the models fail to identify true\ninstances of these classes. For example, the baseline BERT model incorrectly\nclassified over half of the V12 requirements as negative (14 false negatives vs.\n13 true positives). Conversely, the matrices show that models often misclassify\nrequirements by incorrectly assigning them to other categories. The BERT\nBase model, for example, incorrectly labeled 92 different requirements as V10\n(Malicious Code) and 78 requirements as V5 (Malicious Input Handling).\n5.2 Experimental Setup for LLM Prompt Engineering\nWe adapted prompt engineering strategies from Ronanki (Ronanki et al.,\n2024), originally used for multi-class requirement classification. Two configu-\nrations were evaluated: zero-shot prompting (Appendix A), which relied solely\non task instructions and the input requirement, and few-shot prompting (Ap-\npendix B), which additionally incorporated labeled examples. The few-shot\nsetup used 30 training instances that were chosen to (i) capture multi-label\ncases where a single requirement spans multiple security categories, (ii) cover\nall 11 OWASP ASVS categories, and (iii) reflect requirements from different\nbusiness domains to preserve linguistic variation.\nPersona-based prompting was applied in both configurations by instruct-\ning the model to act as an experienced cybersecurity specialist. This alignment\nhelped reduce output variance by grounding responses in professional domain\nexpertise rather than general conversational patterns. The final prompt was\nobtained after five iterations. These iterations focused on three primary ar-\neas: (1) instruction clarity to prevent the model from defaulting to binary\nsecurity labels; (2) contextual grounding, by iteratively refining the OWASP\nASVS category definitions to ensure the model adhered to provided standards;\n(3) constraint formatting, to stabilize the PSV output structure for machine-\nreadability and ensure the inclusion of mandatory explanation fields. All ex-\nperiments were conducted with fixed parameters (Temperature = 0, Top-p =\n0.95) and repeated three times. The reported results are averaged to reduce\nthe impact of any minor, non-deterministic fluctuations in the models’ token\nselection during individual inferences.\nPrompt-based methods can be sensitive to phrasing, structure, and exam-\nple selection. To manage prompt sensitivity and contextual bias, we adopted a\nunified Persona–Context–Constraint strategy. A single prompt structure was\nused across all models, without model-specific tuning or iterative optimization\naimed at maximizing scores. The few-shot examples were chosen for representa-\ntiveness with respect to the OWASP ASVS taxonomy rather than performance\ngains.', 'CONTEXT: This chunk provides a detailed example of a zero-shot prompt designed for classifying and analyzing project requirements based on the OWASP ASVS 4.0.3-tr categories, showcasing how large language models (LLMs) can be guided to perform security-focused requirement classification with explanations, aligning with the broader study\'s focus on automating security requirements analysis.\n\nCONTENT: Implicit Security Requirements Classification 43\nAdvanced Intelligent Systems 6(7):2300798\nWen SF, Katt B (2023) A quantitative security evaluation and analysis model\nfor web applications based on owasp application security verification stan-\ndard. Computers & Security 135:103532, DOI 10.1016/j.cose.2023.103532\nWhite J, Hays S, Fu Q, Spencer-Smith J, Schmidt DC (2024) Chatgpt prompt\npatterns for improving code quality, refactoring, requirements elicitation,\nand software design. In: Generative AI for Effective Software Development,\nSpringer, pp 71–108\nYe J, Yao Z, Huang Z, Pan L, Liu J, Bai Y, Xin A, Weichuan L, Che X,\nHou L, et al. (2025) How do transformers learn implicit reasoning? In: The\nThirty-ninth Annual Conference on Neural Information Processing Systems\nZadenoori MA, Dabrowski J, Alhoshan W, Zhao L, Ferrari A (2025) Large\nlanguage models (llms) for requirements engineering (re): A systematic lit-\nerature review. arXiv preprint arXiv:250911446\nZhou X, Li Y, Chen H (2025) The transformer architecture: Foundations and\nadvances. IEEE Transactions on Neural Networks and Learning Systems\nForthcoming\nZhu Y, Wang J, Liu K (2025) Emergent abilities of large language models:\nIn-context learning, instruction following, and reasoning. Nature Machine\nIntelligence In press\n Lukasiewicz K, Cyga´ nska S (2019) Security-oriented agile approach with\nagilesafe and owasp asvs. In: 2019 Federated Conference on Computer\nScience and Information Systems (FedCSIS), IEEE, pp 653–662, DOI\n10.23919/FedCSIS.2019.00012\n\nZERO SHOT PROMPT \n \nPersona: You are an experienced cybersecurity specialist with extensive knowledge of the OWASP ASVS \n4.0.3-tr and secure software development practices. You are meticulous, detail-oriented, and focused on \nidentifying potential security vulnerabilities and ensuring that applications meet the highest security \nstandards. \n \n********* \n \nTask: Classify and analyze project requirements based on their security and functional aspects from a \ncybersecurity perspective, aligning with the OWASP Application Security Verification Standard (ASVS) \n4.0.3-tr. Provide detailed explanations for all classifications and potential security concerns. \n \n********* \n \nInstructions: \n \n1.  You will receive a list of project requirements in PSV (Pipe Separated Values) format. \n2.  Each line in the input will represent a requirement in “Requirement Number|Requirement Description” \nformat and will contain: \n    * Requirement Number (integer) \n    * Requirement Description (string) \n3.  For each requirement, classify it into the categories defined in the table below, which are derived from \nthe OWASP ASVS 4.0.3-tr. \n4.  Output the classification in PSV format, including all columns from the table, AND an ""Explanation"" \ncolumn within a new line. \n5.  Use ""1"" if the requirement aligns with the ASVS category, and ""0"" if it does not. \n6.  Provide a concise explanation for each classification, justifying why you assigned a ""1"" or ""0"" to each \ncategory. \n7.  Ensure the output has the same number of rows as the input. \n8.  Use the Extra Information for Choosing Categories to aid classification. \n \n********* \n \nContext: \n \nThe categories below are based on the OWASP Application Security Verification Standard (ASVS) 4.0.3-tr. \nRefer to the ASVS documentation for detailed information on each category. The specific ASVS levels (L1, \nL2, L3) are not explicitly represented in the columns, but the categories themselves align with the ASVS \nrequirements. \n \n********* \n \nCategories (Aligned with OWASP ASVS 4.0.3-tr): \n \nV2, Kimlik Doğrulama (Identity Verification) - Covers requirements related to user authentication, \npassword management, and session handling (e.g., ASVS 2.0). \nV3, Oturum Yönetimi (Session Management) - Covers requirements related to secure session handling, \nincluding session timeouts, protection against session fixation, and secure cookie management (e.g., ASVS \n44 Yusuf G¨ ur et al.\nA Zero-shot Prompt', 'CONTEXT: This chunk provides detailed descriptions of OWASP ASVS categories (V2–V13) and their associated security requirements, along with guidelines for classifying requirements based on specific operations (e.g., CRUD, file handling, sensitive data) and an output format for structured classification.\n\nCONTENT: 3.0). \nV4, Erişim Kontrolleri (Access Controls) - Covers requirements related to authorization, role management, \nand preventing unauthorized access to resources (e.g., ASVS 4.0). \nV5, ""Doğrulama, Temizleme ve Şifreleme"" (Verification, Cleaning, and Encryption) - Covers requirements \nrelated to input validation, output encoding, data sanitization, and cryptographic protection of sensitive \ndata (e.g., ASVS 5.0). \nV6, Depolanmış Kriptografi (Stored Cryptography) - Covers requirements for protecting data at rest using \ncryptography, including key management and secure storage of encryption keys (e.g., ASVS 6.0). \nV7, Hata Ayıklama ve Kayıt (Debugging and Logging) - Covers requirements related to secure logging \npractices, error handling, and preventing information leakage through debug information (e.g., ASVS 7.0). \nV8, Veri Koruma (Data Protection) - Covers requirements related to data at rest and in transit protection, \nincluding encryption, backups, and data retention policies (e.g., ASVS 8.0). \nV9, İletişim (Communication) - Covers requirements related to secure communication protocols, protecting \nagainst network attacks, and ensuring data integrity during transmission (e.g., ASVS 9.0). \nV10, Zararlı Kod (Malicious Code) - Covers requirements related to preventing and mitigating the risk of \nmalicious code injection, such as cross-site scripting (XSS) and SQL injection (e.g., ASVS 10.0). \nV12, Dosya ve Kaynakları (Files and Resources) - Covers requirements related to secure file handling, \naccess control to files and resources, and preventing directory traversal attacks (e.g., ASVS 12.0). \nV13, API ve Web Servisleri (API and Web Services) - Covers requirements specific to securing web \nservices, APIs, and microservices, including authentication, authorization, input validation, and error \nhandling (e.g., ASVS 13.0). \n \n********* \n \nExtra Information for Choosing Categories (Aligned with OWASP ASVS 4.0.3-tr): \n \nAccording to given descriptions for kind of requirements, we can decide categories as in PSV: \n \nIf Requirement describes a CRUD operation and if it is Create operation|1|1|1|1|0|1|1|1|1|0|1| \nIf Requirement describes a CRUD operation and if it is Read operation|1|1|1|1|0|1|1|1|1|0|1| \nIf Requirement describes a CRUD operation and if it is Update operation|1|1|1|1|0|1|1|1|1|0|1| \nIf Requirement describes a CRUD operation and if it is Delete operation|1|1|1|0|0|1|1|1|0|0|0| \nIf Requirement describes User Types: usage of various user types as role management|1|1|1|0|0|0|0|0|0|0|0| \nIf Requirement describes Store: actions related to storage and backup of the assets at rest, e.g., backing up \nlog files|0|0|0|1|0|1|1|0|0|1|0| \nIf Requirement describes Transfer: actions related to transfer or sharing of the assets, e.g., sending data \nfrom one service to another|0|0|0|0|0|1|1|1|0|0|1| \nIf Requirement describes Upload and Download Files etc. operation|1|1|1|1|0|1|0|1|1|1|1| \nIf Requirement describes operation on sensitive data like personal, financial, health etc|1|0|1|1|1|0|1|0|0|0|0| \n \n \n********* \nOutput (PSV- The requirements that are classified): \nWe need each requirement response in given format below within new line separator: \n<Requirement Number>|<Requirement \nDescription>|<V2>|<V3>|<V4>|<V5>|<V6>|<V7>|<V8>|<V9>|<V10>|<V12>|<V13>|<Explanation> format. \nand will contain: \n   - Requirement Number (integer) \n   - Requirement Description (string)', 'CONTEXT: **Annotation Process and Quality Assurance for ASRD**: This chunk details the iterative annotation process for the ASRD dataset, including pilot and independent annotation phases, SME-led adjudication, guideline refinements, and the calculation of Inter-Annotator Agreement (IAA) metrics to ensure high-quality and consistent annotations.\n\nCONTENT: 16 Yusuf G¨ ur et al.\n(1) original text and annotations were recoverable, (2) clear documenta-\ntion was maintained, (3) annotation quality was consistently high. The\nannotation template included fields for the software requirement text, se-\nlected OWASP labels, and annotator comments or questions. All versions\nof the guideline and annotation documents were preserved to track revi-\nsions based on annotator feedback. For the annotation process, a sample\ncorpus from the requirement set has been selected by inter agreement of\nSMEs. The first version of the Annotation Guideline based on OWASP\nASVS security classes has been developed, where each annotator has in-\ndividually classified the sample corpus by annotating it with one or more\nlabels following the instructions provided in the first Annotation Guide-\nline. After the initial annotation, the SMEs met under the guidance of the\nsenior SME, who is also the branch manager, to review and discuss the\nannotation decisions. Each of the 30 software requirements was examined\nindividually, and in cases where there were differing opinions among the\nannotators, efforts were made to persuade and convince others by provid-\ning explanations and reasons for their choices. After three iterations of\ndiscussions and deliberations, a full agreement was reached for 30 software\nrequirements.\n–Pilot Annotation: An initial batch of 30 software requirements was anno-\ntated early in the study, prompting multiple discussion rounds and three\nupdates to the guidelines. Adjudication meetings enabled the resolution\nof non-consensus requirements through SME-led discussions, during which\nthe rationale for classification was articulated. Discrepancies arising from\nquality issues, such as guideline violations, were addressed by implement-\ning corrections to ensure conformity with established criteria. Each SME\nprovided justifications for their decisions, which facilitated informed dis-\ncussions and enhanced understanding during adjudication. This iterative\nprocess resolved discrepancies across 98 requirements throughout the entire\ncorpus and was important in establishing the finalized annotation frame-\nwork.\n–Independent Annotation: Each SME classified all remaining 2622 require-\nments independently, using the finalized guideline. Annotators were re-\nquired to justify each decision. This practice proved important for facili-\ntating informed discussions and understanding annotator reasoning during\nthe subsequent adjudication phase. The process took 284 days and 267\nperson-hours of expert time.\n–Agreement Metrics: Inter-Annotator Agreement (IAA) was calculated us-\ning Fleiss’s Kappa and pairwise Cohen’s Kappa to assess consistency. IAA\nscores were instrumental in identifying instances of disagreement among\nannotators, thereby highlighting areas where the guidelines might require\nfurther clarification or where ambiguous requirement phrasings contributed\nto annotation discrepancies. Fleiss’s Kappa coefficient of 0.82 was obtained\nacross the three annotators, indicating strong agreement that significantly\nexceeded random chance. Pairwise Cohen’s Kappa values, ranging from']",I cannot find the answer in the provided documents.,"Classes V1 (Architecture), V11 (Business Logic), and V14 (Configuration) were excluded because they are generally not inferable from functional or non-functional requirements statements[cite: 223, 224].",0.0,0.0,0.0
Why is the Electronic Health Domain Dataset considered limited for this study's purpose?,"['CONTEXT: This chunk discusses the curation and preprocessing of the Agency Security Requirements Dataset (ASRD), detailing the multi-stage pipeline used to extract, anonymize, and prepare real-world Turkish-language Software Requirements Specification (SRS) documents for multi-label security annotation, while addressing gaps in prior datasets.\n\nCONTENT: Implicit Security Requirements Classification 9\n2.3.3 Electronic Health Domain Dataset\nThis corpus focuses on the electronic healthcare industry, a sector defined\nby stringent regulatory and high-assurance demands (Riaz et al., 2014). The\ndataset’s strength lies in its rigorous multi-stage annotation methodology,\nwhich utilized independent researchers and a mediator to ensure high inter-\nannotator agreement. While it provides a large-scale, authentic resource for\nhealthcare-specific tasks, its narrow focus may limit the generalizability of\nmodels to other industries, and its high-level security objectives may be too\ncoarse for detailed technical analysis.\n2.3.4 Summary of Datasets\nAs summarized in Table 1, these datasets vary significantly in their suitability\nfor tasks ranging from general non-functional requirement (NFR) classifica-\ntion to detailed security analysis. Our proposed Agency Security Require-\nments Dataset (ASRD) addresses gaps in prior work by adopting the OWASP\nASVS taxonomy for multi-label annotation. Furthermore, by utilizing real-\nworld projects originally written in Turkish, ASRD introduces both industrial\ncomplexity and linguistic diversity to the field.\n3 Dataset Curation, Annotation, and Evaluation\n3.1 Data Collection and Preprocessing\nThe dataset compilation began with the collection of six Software Require-\nments Specification (SRS) documents from an Agency’s Security Department.\nThese documents were selected from an initial pool of 19 projects executed\nbetween 2019 and 2023, specifically chosen for their domain diversity and\namenability to effective anonymization. The original documents were provided\nin standard document formats (e.g., PDF, DOCX) where requirements were\norganized in itemized lists. To transform these documents into a machine-\nreadable dataset, we employed a multi-stage preprocessing pipeline:\n1. Sentence Segmentation and Extraction: As the source SRS documents uti-\nlized standard itemization for requirements, we employed a rule-based ex-\ntraction method. We utilized pattern matching to identify requirement\nblocks based on hierarchical numbering schemes and line breaks. Each iden-\ntified requirement was extracted and treated as a single processing unit,\npreserving the structural integrity of the original specification.\n2. Anonymization: To protect sensitive data, a semi-automated anonymiza-\ntion process was implemented. Custom scripts replaced entities such as\nproject names, specific URLs, and IP addresses with generic placeholders.\nThis was followed by a manual review by domain experts to ensure that\nthe removal of personally identifiable information (PII) did not compromise\nthe semantic meaning or contextual integrity of the requirements.']","The Electronic Health Domain Dataset is considered limited for this study's purpose because, while it provides a large-scale and authentic resource for healthcare-specific tasks, its narrow focus on the electronic healthcare industry restricts the generalizability of models to other industries. Additionally, its high-level security objectives are too coarse for detailed technical analysis, which may not align with the study's need for more granular and diverse security requirements. 

(Source: temp/b31390a6-3d40-4aa2-b634-17feb052e723_template.pdf)","It is limited to a narrow domain (healthcare) and uses coarse-grained labels (6 security objectives) rather than detailed technical categories[cite: 58, 164].",0.9999999999,0.5,0.9999999999999999
What are the six business domains covered in the ASRD?,"[""CONTEXT: **Dataset Filtering and Composition for ASRD**: This chunk details the rigorous filtering, randomization, and composition process used to create the Agency Security Requirements Dataset (ASRD), a Turkish-language corpus of 2,652 functional requirements across six business domains, annotated for security classification using the OWASP ASVS taxonomy. It also provides sample requirements to illustrate the dataset's diversity and focus on implicit security dependencies.\n\nCONTENT: 10 Yusuf G¨ ur et al.\n3. Filtering: The initial extraction yielded 3,264 items. To ensure the qual-\nity of the dataset, domain experts conducted a rigorous manual filtering\npass to remove 612 items. This exclusion process went beyond simple for-\nmatting artifacts (such as headers or tables of contents). We specifically\nremoved non-functional administrative clauses that addressed the vendor\nrather than the system (e.g., “The contractor must provide training man-\nuals”), project management constraints (e.g., “The project must be deliv-\nered by Q3”), and requirements with ambiguous scope that lacked clear\nverification criteria.\n4. Randomization: The remaining requirements were randomized and masked\nto mitigate potential annotation bias related to the document source or the\nsequence of requirements.\nThe resulting ASRD comprises 2,652 well-formed functional requirements\nacross six distinct business areas: Construction (1,202), Legal (177), Education\n(312), Software (199), Meeting (126), and Accommodation (636). A sample of\nthese requirements is provided in Table 2. The requirements included in this\nsample were selected based on three specific criteria to ensure a representative\noverview:\n–Samples were chosen to reflect the varied terminology and phrasing styles\nused across the six different business domains.\n–We prioritized requirements that demonstrate how standard functional de-\nscriptions conceal underlying security dependencies.\n–We ensured that the sample includes representative examples for all 11\nOWASP ASVS security classes used in the study to demonstrate the full\nscope of the classification task.\nIt is important to note that the ASRD is a Turkish-language corpus; all experi-\nments detailed in this study, including BERT fine-tuning and LLM prompting,\nwere conducted exclusively on the original text. English translations are pro-\nvided only for readability.\nTable 2: Sample Requirements from ASRD\nNo Original Requirements Translated Requirements\n1 Sistem, yetkili kullanıcının hukuk\ndosyası t¨ ur¨ u kayıt etmesini\nsa˘ glayacaktır.\nThe system shall allow authorized\nusers to record the legal file type.\n2 Sistem, kullanıcının kullanılmamı¸ s\nhukuk dosyası t¨ ur¨ un¨ u g¨ uncellemesini\nsa˘ glayacaktır.\nThe system shall allow the user to up-\ndate an unused legal file type.\n3 Sistem, kullanıcının kullanılmamı¸ s\nolan hukuk dosyası t¨ ur¨ un¨ u silmesini\nsa˘ glayacaktır.\nThe system shall allow the user to\ndelete an unused legal file type.\n4 Sistem, kullanıcının hukuk dosyası\nt¨ urlerini sorgulamasını ve listelemesini\nsa˘ glayacaktır.\nThe system shall allow the user to\nquery and list legal file types."", 'CONTEXT: This chunk discusses the curation and preprocessing of the Agency Security Requirements Dataset (ASRD), detailing the multi-stage pipeline used to extract, anonymize, and prepare real-world Turkish-language Software Requirements Specification (SRS) documents for multi-label security annotation, while addressing gaps in prior datasets.\n\nCONTENT: Implicit Security Requirements Classification 9\n2.3.3 Electronic Health Domain Dataset\nThis corpus focuses on the electronic healthcare industry, a sector defined\nby stringent regulatory and high-assurance demands (Riaz et al., 2014). The\ndataset’s strength lies in its rigorous multi-stage annotation methodology,\nwhich utilized independent researchers and a mediator to ensure high inter-\nannotator agreement. While it provides a large-scale, authentic resource for\nhealthcare-specific tasks, its narrow focus may limit the generalizability of\nmodels to other industries, and its high-level security objectives may be too\ncoarse for detailed technical analysis.\n2.3.4 Summary of Datasets\nAs summarized in Table 1, these datasets vary significantly in their suitability\nfor tasks ranging from general non-functional requirement (NFR) classifica-\ntion to detailed security analysis. Our proposed Agency Security Require-\nments Dataset (ASRD) addresses gaps in prior work by adopting the OWASP\nASVS taxonomy for multi-label annotation. Furthermore, by utilizing real-\nworld projects originally written in Turkish, ASRD introduces both industrial\ncomplexity and linguistic diversity to the field.\n3 Dataset Curation, Annotation, and Evaluation\n3.1 Data Collection and Preprocessing\nThe dataset compilation began with the collection of six Software Require-\nments Specification (SRS) documents from an Agency’s Security Department.\nThese documents were selected from an initial pool of 19 projects executed\nbetween 2019 and 2023, specifically chosen for their domain diversity and\namenability to effective anonymization. The original documents were provided\nin standard document formats (e.g., PDF, DOCX) where requirements were\norganized in itemized lists. To transform these documents into a machine-\nreadable dataset, we employed a multi-stage preprocessing pipeline:\n1. Sentence Segmentation and Extraction: As the source SRS documents uti-\nlized standard itemization for requirements, we employed a rule-based ex-\ntraction method. We utilized pattern matching to identify requirement\nblocks based on hierarchical numbering schemes and line breaks. Each iden-\ntified requirement was extracted and treated as a single processing unit,\npreserving the structural integrity of the original specification.\n2. Anonymization: To protect sensitive data, a semi-automated anonymiza-\ntion process was implemented. Custom scripts replaced entities such as\nproject names, specific URLs, and IP addresses with generic placeholders.\nThis was followed by a manual review by domain experts to ensure that\nthe removal of personally identifiable information (PII) did not compromise\nthe semantic meaning or contextual integrity of the requirements.', 'CONTEXT: This chunk discusses the challenges of class imbalance in security requirements classification, the impact of few-shot prompting on model performance, and the potential of hybrid approaches like retrieval-augmented generation (RAG) to improve results, particularly for underrepresented security classes. It also highlights future research directions, including the integration of LLMs with agent-based components to enhance automated security analysis.\n\nCONTENT: 36 Yusuf G¨ ur et al.\nfocus their efforts on high-risk adjudication rather than manual discovery. Our\nresults suggest that commercially available LLMs can serve as always-available\nassistants, democratizing high-level security analysis for development teams.\nFurthermore, future studies could investigate the impact of this tool on prac-\ntitioners with varying levels of experience. Specifically, research is needed to\ndetermine if such automation allows junior developers to identify security con-\ncerns more accurately or if it primarily eases the cognitive load for senior\nexperts by filtering out routine classifications.\nThis study highlights class imbalance as a persistent challenge in SRE.\nThe issue is not specific to ASRD but reflects the nature of the domain itself.\nIn typical specifications, references to architectural security concerns such as\ncryptographic storage or low-level file handling, occur far less often than user-\nfacing controls like authentication (V2) or access control (V4). This creates\na “long-tail” distribution where the most important security failures often\nreside in the least represented classes. Standard supervised learning and even\nfew-shot prompting struggle to generalize from such limited examples. This is\na well-documented issue in requirements datasets, including benchmarks like\nPROMISE exp (Lima et al., 2019), which also suffers from imbalanced classes.\nThe improvement from zero-shot to few-shot prompting shows the positive\nimpact of adding in-context examples. For example, Gemma 4b’s F1 score on\nclass V5 rose from 0.709 to 0.814 with only a small number of examples. This\ngain shows how limited, well-chosen examples can steer model behavior toward\nthe intended classification task. Similar observations have been reported in\nprior work across domains, where prompt design and example selection were\nshown to play a decisive role in task performance (Mann et al., 2020; Liu et al.,\n2023a).\n8 Conclusion and Future Work\nThis work examined the feasibility of automated security requirements classi-\nfication under realistic conditions, where requirements are implicit, unevenly\ndistributed across categories, and drawn from industrial settings rather than\ncurated benchmarks. By introducing ASRD and evaluating supervised and\nprompt-based methods on a shared benchmark, this work enables direct com-\nparison across approaches. The results show that few-shot prompting is suit-\nable when labeled data is limited, while fine-tuned models are better at han-\ndling rare security requirements.\nAs a future work, we plan to explore hybrid SRE approaches that combine\nLLMs with retrieval and agent-based components. In particular, improving\nperformance on underrepresented classes remains an open problem and may\nbenefit from targeted data augmentation, retrieval-augmented methods (RAG)\nsuch as those proposed by Liu et al. (Liu et al., 2025), or agent-based strategies.\nFor instance, instead of relying on learned patterns from non-existent training\nexamples, a RAG-enabled system can dynamically retrieve the specific veri-\nfication requirements and definitions relevant to the input text at inference', 'CONTEXT: This chunk provides an overview of the Agency Security Requirements Dataset (ASRD), detailing its real-world data sources, expert annotation process, and multi-label structure based on OWASP ASVS categories, while also introducing the research questions and comparing the performance of fine-tuned transformer models and prompt-based approaches for classifying implicit security requirements.\n\nCONTENT: 4 Yusuf G¨ ur et al.\ncient and cost-effective compared to addressing security issues at later stages\nof development ((Khan et al., 2024).\nThis paper proposes a machine learning-based system to automatically an-\nalyze SRS documents and classify security requirements according to OWASP\nASVS. Its main contribution is the Agency Security Requirements Dataset\n(ASRD), a publicly available corpus created to support research in automated\nSRA. The ASRD comprises:\n–Real-World Data: 2,652 Software Requirements Specification (SRS) sen-\ntences extracted from six diverse, real-world industry projects,\n–Expert Annotation: Meticulously annotated by three cybersecurity experts,\neach with over 15 years of industry experience,\n–Rigorous Methodology: Developed using the iterative MATTER cycle an-\nnotation framework (Pustejovsky and Stubbs, 2012),\n–High Granularity and Multi-label Structure: Mapped to 11 distinct, action-\nable security categories from the industry-standard OWASP Application\nSecurity Verification Standard (ASVS,V2–V13), allowing each require-\nment to be associated with multiple categories.\nThe ASRD is intended as a shared research resource and benchmark. Using\nthis dataset, the study conducts an empirical analysis of automated classifica-\ntion approaches and addresses the following research questions:\n1. To what extent can fine-tuned transformer models accurately perform\nmulti-label classification of implicit security requirements into OWASP\nASVS categories using the ASRD?\n2. How does the performance of prompt-based approaches (zero-shot and few-\nshot) using modern LLMs compare to fine-tuned BERT-based models and\nnaive baselines for this task?\nThe results show that few-shot prompting with a modern language model\n(Gemini 2.0) achieves a macro-average F1 score of 0.941, closely matching the\nbest fine-tuned BERT-based model (0.942). This demonstrates that example-\nguided inference can reach performance comparable to supervised fine-tuning\nwhile requiring substantially less annotated data and model retraining. The\nevaluation compares fine-tuned transformer models (e.g., SecureBERT) with\nprompt-based methods in zero-shot and few-shot settings across multiple con-\ntemporary models, demonstrating that prompt-based approaches provide a\npractical and resource-efficient alternative for identifying implicit security re-\nquirements in industrial SRS documents.\nThe remainder of this paper is organized as follows. Section 2 reviews re-\nlated work on NLP-based requirements classification, security frameworks, and\nexisting security datasets. Section 3 describes the curation and expert anno-\ntation of the ASRD and summarizes its key characteristics. Section 4 outlines\nthe experimental setup and classification methods, including fine-tuned BERT\nmodels and prompt-based approaches. Section 5 reports the experimental re-\nsults. Section 6 discusses the findings and their implications, with a com-\nparative analysis of fine-tuning and prompting strategies. Section 7 examines', 'CONTEXT: **Comparison of Fine-Tuned Transformers and LLMs for Security Requirements Classification**  \nThis chunk discusses the performance comparison between fine-tuned transformer models and few-shot prompting with LLMs (e.g., Gemini 2.0), highlighting their near-equal effectiveness in classifying implicit security requirements within the ASRD dataset, while emphasizing the challenges of integrating security early in the SDLC through the ""Shift-Left"" approach.\n\nCONTENT: 2 Yusuf G¨ ur et al.\ntional fine-tuned transformer models and contemporary Large Language Mod-\nels (LLMs) employing few-shot and zero-shot prompt engineering strategies.\nThe results show that a few-shot prompting with Gemini 2.0 achieves a macro-\naverage F1 score of 0.941, directly comparable to the fine-tuned BERT model’s\n0.942. This study culminates in two primary findings: first, the validation and\npublication of the ASRD, a high-granularity, multi-label dataset for implicit\nsecurity requirements based on OWASP ASVS V2-V13; and second, the di-\nrect comparison demonstrating that few-shot Large Language Models (LLMs)\nachieve competitive multi-label classification performance (Macro-F1 0.941)\nnearly equal to resource-intensive fine-tuned transformer models (Macro-F1\n0.942). This confirms that LLMs represent a highly practical and resource-\nsaving strategy for automating the identification of embedded (implicit) secu-\nrity requirements for software security in industrial SRS documents\nKeywordsCybersecurity requirement elicitation·OWASP based security\nrequirement classification·OWASP ASVS-based Requirement Labeling·\nLLM based classification\n1 Introduction\nRequirements Engineering (RE) concerns the identification, documentation,\nand management of software requirements (Sommerville and Sawyer, 1997).\nThe increasing interconnectivity of information systems forces organizations\nto provide secure services in cyberspace. Integrating security during initial\nrequirements analysis is essential to proactively mitigate risks, reduce vulner-\nabilities, and reduce the high costs associated with downstream remediation\n(Villamizar et al., 2018). This proactive approach is central to the ”Shift-\nLeft” security paradigm, which advocates for the identification and mitigation\nof vulnerabilities as early as possible in the Software Development Life Cycle\n(SDLC).\nAlthough Security Requirements Engineering (SRE) has matured as a dis-\ncipline, its effective application in practice remains challenging. The issue is\nnot the absence of security considerations, but the difficulty of ensuring that\nsecurity requirements are properly validated and verified. An empirical investi-\ngation that combines a systematic literature review with a survey of 58 indus-\ntry professionals highlights this gap (Alam et al., 2025). While practitioners\nreported a moderate understanding of SRE practices, Security Requirements\nAssurance (SRA) was found to be poorly understood and rarely applied in\nindustrial contexts. The study concludes that validating and ensuring security\nrequirements remains a major and unresolved challenge in practice.\nThis assurance gap persists due to the documented lack of specialized se-\ncurity training for developers in early-phase RE (Andrade et al., 2023). The\ntask relies heavily on scarce specialized expertise and time-intensive manual\nanalysis. The main challenge is that critical security requirements are often not\nexplicitly stated. Instead, they are implicitly embedded within standard func-\ntional descriptions. For example, a seemingly simple functional requirement,', 'CONTEXT: **Comparison of Effort vs. Performance and Validation of ASRD Dataset**: This chunk discusses the trade-offs between data preparation effort and model performance across fine-tuned BERT models and LLM approaches, highlights the advantages of the ASRD dataset over existing benchmarks, and emphasizes the challenges of implicit security requirement elicitation in real-world contexts.\n\nCONTENT: 34 Yusuf G¨ ur et al.\nTable 9: Comparison of Effort vs. Performance across Approaches\nApproach Data Requirement Est. Setup Effort Best Macro-F1\nBERT Fine-Tuning High (∼2,600 ex) High (267 hours)0.942\nLLM Few-Shot Low (∼30 ex) Low (<10 hours)0.941\nLLM Zero-Shot None Minimal (<2 hours) 0.890\nLLM Few-Shot approach (using Gemini 2.0) achieved a similar performance\n(Macro-F1 0.941) while requiring only the selection of a small number of in-\ncontext examples (approx. 30 examples per prompt).\nThis comparison points to diminishing returns from large-scale annotation\nwhen capable LLMs are available. Although extensive annotation was required\nhere to establish a reliable ground truth and validate the test set, the results\nindicate that, in industrial settings, a few-shot LLM approach can offer a more\nfavorable balance between effort and accuracy. Comparable performance can\nbe reached with substantially less data preparation, reducing the need for\nprolonged expert annotation.\nThe main contribution of this work is the introduction and validation of\nthe ASRD. The consistent performance of both fine-tuned BERT models and\nLLM-based approaches on ASRD supports its reliability and practical value.\nThe dataset addresses a persistent limitation in SRE research which is the\nlack of large, well-curated, professionally sourced benchmarks for security re-\nquirement classification. Compared with existing datasets, ASRD offers sev-\neral advantages. Unlike DOSSPRE (Kadebu et al., 2023), which is derived\nfrom student projects, ASRD includes 2,652 requirements from six commer-\ncial software systems, reflecting real industrial language and complexity. Its\nOWASP ASVS–based taxonomy spans 11 security classes, providing finer de-\ntail than PROMISE exp (Lima et al., 2019), where security is treated as a\nsingle category. Although the healthcare dataset by Riaz et al. (Riaz et al.,\n2014) is larger, it focuses on six broad objectives within a single domain, lim-\niting its scope. By contrast, ASRD supports cross-domain analysis and aligns\nwith a framework widely used in practice.\nThe dataset also benefits from a rigorous annotation process. Three cy-\nbersecurity specialists, each with over 15 years of experience, labeled the data\nusing an iterative MATTER-based procedure. This resulted in strong agree-\nment (Fleiss’ Kappa = 0.82) and helps mitigate the subjectivity that often\naffects security-related requirement datasets.\nBeyond the dataset construction, this study highlights the complexity of\nimplicit security requirement elicitation. A significant portion of security vul-\nnerabilities stems not from missing explicit security constraints (e.g., “The\nsystem shall encrypt passwords”), but from functional requirements that im-\nplicitly demand robust security controls. Our analysis of the ASRD reveals\nthat functional descriptions of business logic often conceal a dense network of\nsecurity dependencies. A representative example from our results is Require-\nment No. 2 (“The system will carry out the asset handover process”). While', 'CONTEXT: This chunk focuses on the comparative performance analysis of models across OWASP ASVS classes, highlighting challenges with rare categories (e.g., Stored Cryptography, File Handling) through heatmaps and gap analysis, while addressing dataset limitations and threats to validity related to linguistic diversity and generalizability.\n\nCONTENT: 30 Yusuf G¨ ur et al.\nFig. 2: Critical Difference (CD) diagram visualizing the statistical comparison\nof classifiers. The horizontal axis represents the average rank of each model\nacross all 11 OWASP ASVS classes (lower ranks indicate better performance).\nThe Critical Difference (CD = 10.46) represents the minimum difference in\naverage rank required for two models to be considered statistically significantly\ndifferent atp <0.05.\n5.6 Comparative Visualization Analysis\nTo complement the statistical rankings, we visualized the performance land-\nscape across all 20 model configurations.\nHeatmap Analysis: Figure 3 shows F1-scores by class. Requirements related\nto V2 (Authentication), V3 (Session Management), and V4 (Access Control)\nare handled well by nearly all models, with scores typically above 0.95. These\ncategories are common and appear consistently across training sources. In\ncontrast, V6 (Stored Cryptography) and V12 (Files and Resources) show low\nscores across the board, including for GPT-4o and Gemini 2.0 Pro. This pat-\ntern points to a systematic difficulty with sparse categories rather than a\nweakness of a particular model.\nGap Analysis: Figure 4 contrasts average performance on frequent and in-\nfrequent classes. Fine-tuned models such as SecureBERT and BERT-Multilingual\nperform better on rare classes, with F1-scores around 0.70–0.80. Zero-shot\nLLMs perform poorly in these cases, often below 0.30. Few-shot prompting\nimproves results for some models, such as Gemini 2.0, but the gap remains\nsubstantial.\n6 Threats to Validity\nOur dataset consists of 2,652 requirements derived from six Software Require-\nments Specification (SRS) documents. While the number of source documents\n\nImplicit Security Requirements Classification 31\nFig. 3:Comprehensive Performance Heatmap.F1-scores for all 20 eval-\nuated models across 11 OWASP ASVS classes. The visualization highlights\nthe “zone of failure” (red) for classes V6 and V12 across models, contrasting\nwith the high performance (green) on ubiquitous classes like V2 and V4.\nis limited, potential bias was mitigated by strategically selecting projects from\nsix distinct business domains: Construction, Legal, Education, Software, Meet-\ning Management, and Accommodation. This semantic diversity ensures that\nthe models are evaluated on a broad vocabulary and varied functional contexts\nrather than a single industry vertical. A specific consideration regarding the\nexternal validity of this study is that the ASRD is a Turkish-language corpus\nwhich allows for the evaluation of automated tools in linguistically diverse\nindustrial contexts.\nFurthermore, because security requirements (e.g., Authentication, Log-\nging) represent cross-cutting concerns that apply universally across software\nsystems, we believe the fundamental patterns captured in the ASRD are trans-\nferable to other industries. Our current evaluation used a stratified split across\nthe entire dataset, meaning requirements from all six source projects are\npresent in the training, validation, and test sets. However, we acknowledge\nthat this approach does not fully measure the models’ ability to generalize to\nentirely new projects with different authors or linguistic styles, as the test set\nis not completely “unseen” in terms of project context.', 'CONTEXT: This chunk discusses the challenges and methodologies involved in addressing class imbalance and reproducibility issues in the classification of implicit security requirements, highlighting the use of experimental LLMs for advanced reasoning and the trade-offs between dataset curation effort and model performance.\n\nCONTENT: Implicit Security Requirements Classification 33\nlacked prior experience in Natural Language Processing (NLP) annotation,\nthis was addressed through focused training sessions to align their domain\nexpertise with the technical requirements of the MATTER cycle.\nVariations in model versions and dependencies (e.g., library updates) pose\nchallenges to reproducibility.\nA significant class imbalance, with categories like V6 and V12 severely\nunderrepresented, restricts model learning and generalization. To address this,\nextensive training, refined annotation protocols, iterative consensus-building,\nand detailed error analysis were employed.\nA significant challenge in evaluating Large Language Models (LLMs) is the\nrapid pace of model iteration. In this study, we deliberately selected several\n“experimental” and “preview” models (specifically gemini-2.0-flash-thinking-\nexp-01-21, gemini-2.0-pro-exp-02-05, and gemini-2.5-flash-preview-04-17) along-\nside stable releases. The rationale for including these unversioned, experi-\nmental models was to benchmark the absolute state-of-the-art capabilities in\n“reasoning” (Chain-of-Thought) and architectural efficiency that are not yet\npresent in General Availability (GA) versions at the time of the study. We\nhypothesized that the improved reasoning capabilities of models like Flash-\nThinking would be particularly advantageous for identifying implicit security\nrequirements, which often require multi-step deduction rather than simple pat-\ntern matching.\nHowever, we acknowledge that the use of experimental endpoints poses a\nthreat to internal validity regarding reproducibility. These models are subject\nto deprecation or unannounced updates by the provider, meaning other re-\nsearchers may not be able to query the exact same model weights in the future.\nTo mitigate this, we have documented the exact model identifier strings used\nduring our data collection window. While this does not guarantee future ac-\ncess to these specific snapshots, it ensures transparency regarding the specific\ntechnological generation evaluated. We argue that this trade-off is necessary\nto provide a forward-looking analysis of how emerging LLM capabilities (such\nas intrinsic reasoning) impact the field of Security Requirements Engineering.\n7 Discussion\nThe combined experimental results obtained from fine-tuning BERT models\nand employing various LLM prompting strategies, further refined by the com-\nparison between zero-shot and few-shot methods, yield several key insights.\nAn important dimension of our comparison is the trade-off between the ef-\nfort required to curate the dataset and the resulting classification performance.\nAs detailed in Section 3, the construction of the ASRD involved 267 person-\nhours of expert labor to annotate 2,652 requirements. This extensive effort\nwas a prerequisite for the fine-tuned BERT models, which rely on large-scale\nsupervised data.\nAs summarized in Table 9, the BERT Fine-Tuning approach achieved the\nhighest performance (Macro-F1 0.942) but at the highest cost. In contrast, the']","The six business domains covered in the Agency Security Requirements Dataset (ASRD) are:

1. **Construction** (1,202 requirements)
2. **Legal** (177 requirements)
3. **Education** (312 requirements)
4. **Software** (199 requirements)
5. **Meeting Management** (126 requirements)
6. **Accommodation** (636 requirements)

These domains reflect the diversity of functional requirements included in the dataset, which is annotated for security classification using the OWASP ASVS taxonomy. 

(Source: temp/b31390a6-3d40-4aa2-b634-17feb052e723_template.pdf)","The domains are Construction, Legal, Education, Software, Meeting, and Accommodation[cite: 187].",0.642857142825,1.0,0.8540320712644136
How does the performance of Gemini 2.0 Few-Shot compare to the best fine-tuned BERT model?,"['CONTEXT: **Performance Comparison of Models for Implicit Security Requirements Classification**  \nThis chunk focuses on the statistical analysis and performance comparison of baseline, fine-tuned, and few-shot/zero-shot LLM models, highlighting significant differences in their ability to classify implicit security requirements, with SecureBERT and Gemini 2.0 Few-Shot emerging as top performers.\n\nCONTENT: Implicit Security Requirements Classification 29\nThe analysis (Table 8) highlights the significant pairwise contrasts and\nreveals important performance distinctions:\n–Baseline vs. Advanced Models: The Naive Baseline was consistently out-\nperformed by the advanced models. The difference between the Baseline\nand models such as SecureBERT (p <0.001) and Gemini 2.0 Few-Shot\n(p <0.001) was statistically significant, confirming that the high Weighted-\nF1 scores of these models reflect genuine learning beyond simple majority-\nclass prediction.\n–Fine-Tuning vs. Zero-Shot LLMs: Fine-tuned models like SecureBERT\ndemonstrated statistically significant performance distinctions (p <0.001)\nwhen compared to general-purpose LLMs operating in a zero-shot setting,\nsuch as GPT-4o (p <0.001) and Llama 3 70B (p <0.001). This indi-\ncates that without in-context examples, domain-specific fine-tuning retains\na measurable statistical advantage.\n–Fine-Tuning vs. Few-Shot State-of-the-Art: The pairwise comparison be-\ntween the best fine-tuned model (SecureBERT) and the best LLM ap-\nproach (Gemini 2.0 Few-Shot) yielded ap-value of 0.102. This value is not\nstatistically significant at theα= 0.05 level, supporting our finding that\nfew-shot prompting with state-of-the-art LLMs can effectively match the\nperformance of specialized, fine-tuned BERT models in this domain.\nTable 8: Significant Pairwise Differences (Wilcoxon Signed-Rank Test)\nModel A Model B Statistic p-value\nNaive Baseline SecureBERT 0.0 0.00098\nNaive Baseline BERT Turkish 0.0 0.00098\nNaive Baseline Gemini 2.0 (Few-Shot) 0.0 0.00098\nNaive Baseline Gemini 2.0 Pro (Few-Shot) 0.0 0.00098\nSecureBERT GPT-4o Mini (Few-Shot) 0.0 0.00098\nSecureBERT GPT-4o Mini (Zero-Shot) 0.0 0.00098\nSecureBERT GPT-4o (Zero-Shot) 0.0 0.00098\nSecureBERT GPT-4o (Few-Shot) 0.0 0.00098\nSecureBERT Gemini 2.0 (Zero-Shot) 25.0 0.501\nSecureBERT Gemini 2.0 (Few-Shot) 14.5 0.102\nTo visualize the relative performance and statistical significance groups\nof all evaluated models, we constructed a Critical Difference (CD) diagram\n(Figure 2) using the Nemenyi post-hoc test.\nIn the CD diagram, models are arranged along the horizontal axis accord-\ning to their average rank. The best performing models, such as Gemini 2.0\nPro (Few-Shot) and BERT-Multilingual, appear on the left (lowest ranks).\nA horizontal bar connects groups of models that are not statistically signifi-\ncantly different from one another. The wide span of the CD bar (10.46) reflects\nthe high number of models compared (23) relative to the number of datasets\n(11). Despite this conservative threshold, the diagram clearly separates the\ntop-tier models (Fine-tuned BERTs and Few-Shot Gemini/GPT-4o) from the\nlower-performing group (Naive Baseline, Zero-Shot Gemma/Llama).']","The performance comparison between Gemini 2.0 Few-Shot and the best fine-tuned BERT model (SecureBERT) reveals that their performance is very close. The pairwise comparison yielded a p-value of 0.102, which is not statistically significant at the α = 0.05 level. This indicates that few-shot prompting with state-of-the-art models like Gemini 2.0 can effectively match the performance of specialized, fine-tuned BERT models in the domain of implicit security requirements classification.

(Source: temp/b31390a6-3d40-4aa2-b634-17feb052e723_template.pdf)","It is directly comparable, with Gemini 2.0 Few-Shot scoring 0.941 Macro-F1 versus the fine-tuned BERT model's 0.942[cite: 24, 26].",0.9999999999,1.0,0.8498696105976624
How many person-hours were required to annotate the ASRD?,"['CONTEXT: **Comparison of Effort vs. Performance and Validation of ASRD Dataset**: This chunk discusses the trade-offs between data preparation effort and model performance across fine-tuned BERT models and LLM approaches, highlights the advantages of the ASRD dataset over existing benchmarks, and emphasizes the challenges of implicit security requirement elicitation in real-world contexts.\n\nCONTENT: 34 Yusuf G¨ ur et al.\nTable 9: Comparison of Effort vs. Performance across Approaches\nApproach Data Requirement Est. Setup Effort Best Macro-F1\nBERT Fine-Tuning High (∼2,600 ex) High (267 hours)0.942\nLLM Few-Shot Low (∼30 ex) Low (<10 hours)0.941\nLLM Zero-Shot None Minimal (<2 hours) 0.890\nLLM Few-Shot approach (using Gemini 2.0) achieved a similar performance\n(Macro-F1 0.941) while requiring only the selection of a small number of in-\ncontext examples (approx. 30 examples per prompt).\nThis comparison points to diminishing returns from large-scale annotation\nwhen capable LLMs are available. Although extensive annotation was required\nhere to establish a reliable ground truth and validate the test set, the results\nindicate that, in industrial settings, a few-shot LLM approach can offer a more\nfavorable balance between effort and accuracy. Comparable performance can\nbe reached with substantially less data preparation, reducing the need for\nprolonged expert annotation.\nThe main contribution of this work is the introduction and validation of\nthe ASRD. The consistent performance of both fine-tuned BERT models and\nLLM-based approaches on ASRD supports its reliability and practical value.\nThe dataset addresses a persistent limitation in SRE research which is the\nlack of large, well-curated, professionally sourced benchmarks for security re-\nquirement classification. Compared with existing datasets, ASRD offers sev-\neral advantages. Unlike DOSSPRE (Kadebu et al., 2023), which is derived\nfrom student projects, ASRD includes 2,652 requirements from six commer-\ncial software systems, reflecting real industrial language and complexity. Its\nOWASP ASVS–based taxonomy spans 11 security classes, providing finer de-\ntail than PROMISE exp (Lima et al., 2019), where security is treated as a\nsingle category. Although the healthcare dataset by Riaz et al. (Riaz et al.,\n2014) is larger, it focuses on six broad objectives within a single domain, lim-\niting its scope. By contrast, ASRD supports cross-domain analysis and aligns\nwith a framework widely used in practice.\nThe dataset also benefits from a rigorous annotation process. Three cy-\nbersecurity specialists, each with over 15 years of experience, labeled the data\nusing an iterative MATTER-based procedure. This resulted in strong agree-\nment (Fleiss’ Kappa = 0.82) and helps mitigate the subjectivity that often\naffects security-related requirement datasets.\nBeyond the dataset construction, this study highlights the complexity of\nimplicit security requirement elicitation. A significant portion of security vul-\nnerabilities stems not from missing explicit security constraints (e.g., “The\nsystem shall encrypt passwords”), but from functional requirements that im-\nplicitly demand robust security controls. Our analysis of the ASRD reveals\nthat functional descriptions of business logic often conceal a dense network of\nsecurity dependencies. A representative example from our results is Require-\nment No. 2 (“The system will carry out the asset handover process”). While', ""CONTEXT: **Dataset Filtering and Composition for ASRD**: This chunk details the rigorous filtering, randomization, and composition process used to create the Agency Security Requirements Dataset (ASRD), a Turkish-language corpus of 2,652 functional requirements across six business domains, annotated for security classification using the OWASP ASVS taxonomy. It also provides sample requirements to illustrate the dataset's diversity and focus on implicit security dependencies.\n\nCONTENT: 10 Yusuf G¨ ur et al.\n3. Filtering: The initial extraction yielded 3,264 items. To ensure the qual-\nity of the dataset, domain experts conducted a rigorous manual filtering\npass to remove 612 items. This exclusion process went beyond simple for-\nmatting artifacts (such as headers or tables of contents). We specifically\nremoved non-functional administrative clauses that addressed the vendor\nrather than the system (e.g., “The contractor must provide training man-\nuals”), project management constraints (e.g., “The project must be deliv-\nered by Q3”), and requirements with ambiguous scope that lacked clear\nverification criteria.\n4. Randomization: The remaining requirements were randomized and masked\nto mitigate potential annotation bias related to the document source or the\nsequence of requirements.\nThe resulting ASRD comprises 2,652 well-formed functional requirements\nacross six distinct business areas: Construction (1,202), Legal (177), Education\n(312), Software (199), Meeting (126), and Accommodation (636). A sample of\nthese requirements is provided in Table 2. The requirements included in this\nsample were selected based on three specific criteria to ensure a representative\noverview:\n–Samples were chosen to reflect the varied terminology and phrasing styles\nused across the six different business domains.\n–We prioritized requirements that demonstrate how standard functional de-\nscriptions conceal underlying security dependencies.\n–We ensured that the sample includes representative examples for all 11\nOWASP ASVS security classes used in the study to demonstrate the full\nscope of the classification task.\nIt is important to note that the ASRD is a Turkish-language corpus; all experi-\nments detailed in this study, including BERT fine-tuning and LLM prompting,\nwere conducted exclusively on the original text. English translations are pro-\nvided only for readability.\nTable 2: Sample Requirements from ASRD\nNo Original Requirements Translated Requirements\n1 Sistem, yetkili kullanıcının hukuk\ndosyası t¨ ur¨ u kayıt etmesini\nsa˘ glayacaktır.\nThe system shall allow authorized\nusers to record the legal file type.\n2 Sistem, kullanıcının kullanılmamı¸ s\nhukuk dosyası t¨ ur¨ un¨ u g¨ uncellemesini\nsa˘ glayacaktır.\nThe system shall allow the user to up-\ndate an unused legal file type.\n3 Sistem, kullanıcının kullanılmamı¸ s\nolan hukuk dosyası t¨ ur¨ un¨ u silmesini\nsa˘ glayacaktır.\nThe system shall allow the user to\ndelete an unused legal file type.\n4 Sistem, kullanıcının hukuk dosyası\nt¨ urlerini sorgulamasını ve listelemesini\nsa˘ glayacaktır.\nThe system shall allow the user to\nquery and list legal file types."", 'CONTEXT: This chunk discusses the curation and preprocessing of the Agency Security Requirements Dataset (ASRD), detailing the multi-stage pipeline used to extract, anonymize, and prepare real-world Turkish-language Software Requirements Specification (SRS) documents for multi-label security annotation, while addressing gaps in prior datasets.\n\nCONTENT: Implicit Security Requirements Classification 9\n2.3.3 Electronic Health Domain Dataset\nThis corpus focuses on the electronic healthcare industry, a sector defined\nby stringent regulatory and high-assurance demands (Riaz et al., 2014). The\ndataset’s strength lies in its rigorous multi-stage annotation methodology,\nwhich utilized independent researchers and a mediator to ensure high inter-\nannotator agreement. While it provides a large-scale, authentic resource for\nhealthcare-specific tasks, its narrow focus may limit the generalizability of\nmodels to other industries, and its high-level security objectives may be too\ncoarse for detailed technical analysis.\n2.3.4 Summary of Datasets\nAs summarized in Table 1, these datasets vary significantly in their suitability\nfor tasks ranging from general non-functional requirement (NFR) classifica-\ntion to detailed security analysis. Our proposed Agency Security Require-\nments Dataset (ASRD) addresses gaps in prior work by adopting the OWASP\nASVS taxonomy for multi-label annotation. Furthermore, by utilizing real-\nworld projects originally written in Turkish, ASRD introduces both industrial\ncomplexity and linguistic diversity to the field.\n3 Dataset Curation, Annotation, and Evaluation\n3.1 Data Collection and Preprocessing\nThe dataset compilation began with the collection of six Software Require-\nments Specification (SRS) documents from an Agency’s Security Department.\nThese documents were selected from an initial pool of 19 projects executed\nbetween 2019 and 2023, specifically chosen for their domain diversity and\namenability to effective anonymization. The original documents were provided\nin standard document formats (e.g., PDF, DOCX) where requirements were\norganized in itemized lists. To transform these documents into a machine-\nreadable dataset, we employed a multi-stage preprocessing pipeline:\n1. Sentence Segmentation and Extraction: As the source SRS documents uti-\nlized standard itemization for requirements, we employed a rule-based ex-\ntraction method. We utilized pattern matching to identify requirement\nblocks based on hierarchical numbering schemes and line breaks. Each iden-\ntified requirement was extracted and treated as a single processing unit,\npreserving the structural integrity of the original specification.\n2. Anonymization: To protect sensitive data, a semi-automated anonymiza-\ntion process was implemented. Custom scripts replaced entities such as\nproject names, specific URLs, and IP addresses with generic placeholders.\nThis was followed by a manual review by domain experts to ensure that\nthe removal of personally identifiable information (PII) did not compromise\nthe semantic meaning or contextual integrity of the requirements.', 'CONTEXT: This chunk provides an overview of the Agency Security Requirements Dataset (ASRD), detailing its real-world data sources, expert annotation process, and multi-label structure based on OWASP ASVS categories, while also introducing the research questions and comparing the performance of fine-tuned transformer models and prompt-based approaches for classifying implicit security requirements.\n\nCONTENT: 4 Yusuf G¨ ur et al.\ncient and cost-effective compared to addressing security issues at later stages\nof development ((Khan et al., 2024).\nThis paper proposes a machine learning-based system to automatically an-\nalyze SRS documents and classify security requirements according to OWASP\nASVS. Its main contribution is the Agency Security Requirements Dataset\n(ASRD), a publicly available corpus created to support research in automated\nSRA. The ASRD comprises:\n–Real-World Data: 2,652 Software Requirements Specification (SRS) sen-\ntences extracted from six diverse, real-world industry projects,\n–Expert Annotation: Meticulously annotated by three cybersecurity experts,\neach with over 15 years of industry experience,\n–Rigorous Methodology: Developed using the iterative MATTER cycle an-\nnotation framework (Pustejovsky and Stubbs, 2012),\n–High Granularity and Multi-label Structure: Mapped to 11 distinct, action-\nable security categories from the industry-standard OWASP Application\nSecurity Verification Standard (ASVS,V2–V13), allowing each require-\nment to be associated with multiple categories.\nThe ASRD is intended as a shared research resource and benchmark. Using\nthis dataset, the study conducts an empirical analysis of automated classifica-\ntion approaches and addresses the following research questions:\n1. To what extent can fine-tuned transformer models accurately perform\nmulti-label classification of implicit security requirements into OWASP\nASVS categories using the ASRD?\n2. How does the performance of prompt-based approaches (zero-shot and few-\nshot) using modern LLMs compare to fine-tuned BERT-based models and\nnaive baselines for this task?\nThe results show that few-shot prompting with a modern language model\n(Gemini 2.0) achieves a macro-average F1 score of 0.941, closely matching the\nbest fine-tuned BERT-based model (0.942). This demonstrates that example-\nguided inference can reach performance comparable to supervised fine-tuning\nwhile requiring substantially less annotated data and model retraining. The\nevaluation compares fine-tuned transformer models (e.g., SecureBERT) with\nprompt-based methods in zero-shot and few-shot settings across multiple con-\ntemporary models, demonstrating that prompt-based approaches provide a\npractical and resource-efficient alternative for identifying implicit security re-\nquirements in industrial SRS documents.\nThe remainder of this paper is organized as follows. Section 2 reviews re-\nlated work on NLP-based requirements classification, security frameworks, and\nexisting security datasets. Section 3 describes the curation and expert anno-\ntation of the ASRD and summarizes its key characteristics. Section 4 outlines\nthe experimental setup and classification methods, including fine-tuned BERT\nmodels and prompt-based approaches. Section 5 reports the experimental re-\nsults. Section 6 discusses the findings and their implications, with a com-\nparative analysis of fine-tuning and prompting strategies. Section 7 examines', 'CONTEXT: This chunk discusses the challenges and methodologies involved in addressing class imbalance and reproducibility issues in the classification of implicit security requirements, highlighting the use of experimental LLMs for advanced reasoning and the trade-offs between dataset curation effort and model performance.\n\nCONTENT: Implicit Security Requirements Classification 33\nlacked prior experience in Natural Language Processing (NLP) annotation,\nthis was addressed through focused training sessions to align their domain\nexpertise with the technical requirements of the MATTER cycle.\nVariations in model versions and dependencies (e.g., library updates) pose\nchallenges to reproducibility.\nA significant class imbalance, with categories like V6 and V12 severely\nunderrepresented, restricts model learning and generalization. To address this,\nextensive training, refined annotation protocols, iterative consensus-building,\nand detailed error analysis were employed.\nA significant challenge in evaluating Large Language Models (LLMs) is the\nrapid pace of model iteration. In this study, we deliberately selected several\n“experimental” and “preview” models (specifically gemini-2.0-flash-thinking-\nexp-01-21, gemini-2.0-pro-exp-02-05, and gemini-2.5-flash-preview-04-17) along-\nside stable releases. The rationale for including these unversioned, experi-\nmental models was to benchmark the absolute state-of-the-art capabilities in\n“reasoning” (Chain-of-Thought) and architectural efficiency that are not yet\npresent in General Availability (GA) versions at the time of the study. We\nhypothesized that the improved reasoning capabilities of models like Flash-\nThinking would be particularly advantageous for identifying implicit security\nrequirements, which often require multi-step deduction rather than simple pat-\ntern matching.\nHowever, we acknowledge that the use of experimental endpoints poses a\nthreat to internal validity regarding reproducibility. These models are subject\nto deprecation or unannounced updates by the provider, meaning other re-\nsearchers may not be able to query the exact same model weights in the future.\nTo mitigate this, we have documented the exact model identifier strings used\nduring our data collection window. While this does not guarantee future ac-\ncess to these specific snapshots, it ensures transparency regarding the specific\ntechnological generation evaluated. We argue that this trade-off is necessary\nto provide a forward-looking analysis of how emerging LLM capabilities (such\nas intrinsic reasoning) impact the field of Security Requirements Engineering.\n7 Discussion\nThe combined experimental results obtained from fine-tuning BERT models\nand employing various LLM prompting strategies, further refined by the com-\nparison between zero-shot and few-shot methods, yield several key insights.\nAn important dimension of our comparison is the trade-off between the ef-\nfort required to curate the dataset and the resulting classification performance.\nAs detailed in Section 3, the construction of the ASRD involved 267 person-\nhours of expert labor to annotate 2,652 requirements. This extensive effort\nwas a prerequisite for the fine-tuned BERT models, which rely on large-scale\nsupervised data.\nAs summarized in Table 9, the BERT Fine-Tuning approach achieved the\nhighest performance (Macro-F1 0.942) but at the highest cost. In contrast, the', 'CONTEXT: This chunk discusses the challenges of class imbalance in security requirements classification, the impact of few-shot prompting on model performance, and the potential of hybrid approaches like retrieval-augmented generation (RAG) to improve results, particularly for underrepresented security classes. It also highlights future research directions, including the integration of LLMs with agent-based components to enhance automated security analysis.\n\nCONTENT: 36 Yusuf G¨ ur et al.\nfocus their efforts on high-risk adjudication rather than manual discovery. Our\nresults suggest that commercially available LLMs can serve as always-available\nassistants, democratizing high-level security analysis for development teams.\nFurthermore, future studies could investigate the impact of this tool on prac-\ntitioners with varying levels of experience. Specifically, research is needed to\ndetermine if such automation allows junior developers to identify security con-\ncerns more accurately or if it primarily eases the cognitive load for senior\nexperts by filtering out routine classifications.\nThis study highlights class imbalance as a persistent challenge in SRE.\nThe issue is not specific to ASRD but reflects the nature of the domain itself.\nIn typical specifications, references to architectural security concerns such as\ncryptographic storage or low-level file handling, occur far less often than user-\nfacing controls like authentication (V2) or access control (V4). This creates\na “long-tail” distribution where the most important security failures often\nreside in the least represented classes. Standard supervised learning and even\nfew-shot prompting struggle to generalize from such limited examples. This is\na well-documented issue in requirements datasets, including benchmarks like\nPROMISE exp (Lima et al., 2019), which also suffers from imbalanced classes.\nThe improvement from zero-shot to few-shot prompting shows the positive\nimpact of adding in-context examples. For example, Gemma 4b’s F1 score on\nclass V5 rose from 0.709 to 0.814 with only a small number of examples. This\ngain shows how limited, well-chosen examples can steer model behavior toward\nthe intended classification task. Similar observations have been reported in\nprior work across domains, where prompt design and example selection were\nshown to play a decisive role in task performance (Mann et al., 2020; Liu et al.,\n2023a).\n8 Conclusion and Future Work\nThis work examined the feasibility of automated security requirements classi-\nfication under realistic conditions, where requirements are implicit, unevenly\ndistributed across categories, and drawn from industrial settings rather than\ncurated benchmarks. By introducing ASRD and evaluating supervised and\nprompt-based methods on a shared benchmark, this work enables direct com-\nparison across approaches. The results show that few-shot prompting is suit-\nable when labeled data is limited, while fine-tuned models are better at han-\ndling rare security requirements.\nAs a future work, we plan to explore hybrid SRE approaches that combine\nLLMs with retrieval and agent-based components. In particular, improving\nperformance on underrepresented classes remains an open problem and may\nbenefit from targeted data augmentation, retrieval-augmented methods (RAG)\nsuch as those proposed by Liu et al. (Liu et al., 2025), or agent-based strategies.\nFor instance, instead of relying on learned patterns from non-existent training\nexamples, a RAG-enabled system can dynamically retrieve the specific veri-\nfication requirements and definitions relevant to the input text at inference', 'CONTEXT: **Comparison of Fine-Tuned Transformers and LLMs for Security Requirements Classification**  \nThis chunk discusses the performance comparison between fine-tuned transformer models and few-shot prompting with LLMs (e.g., Gemini 2.0), highlighting their near-equal effectiveness in classifying implicit security requirements within the ASRD dataset, while emphasizing the challenges of integrating security early in the SDLC through the ""Shift-Left"" approach.\n\nCONTENT: 2 Yusuf G¨ ur et al.\ntional fine-tuned transformer models and contemporary Large Language Mod-\nels (LLMs) employing few-shot and zero-shot prompt engineering strategies.\nThe results show that a few-shot prompting with Gemini 2.0 achieves a macro-\naverage F1 score of 0.941, directly comparable to the fine-tuned BERT model’s\n0.942. This study culminates in two primary findings: first, the validation and\npublication of the ASRD, a high-granularity, multi-label dataset for implicit\nsecurity requirements based on OWASP ASVS V2-V13; and second, the di-\nrect comparison demonstrating that few-shot Large Language Models (LLMs)\nachieve competitive multi-label classification performance (Macro-F1 0.941)\nnearly equal to resource-intensive fine-tuned transformer models (Macro-F1\n0.942). This confirms that LLMs represent a highly practical and resource-\nsaving strategy for automating the identification of embedded (implicit) secu-\nrity requirements for software security in industrial SRS documents\nKeywordsCybersecurity requirement elicitation·OWASP based security\nrequirement classification·OWASP ASVS-based Requirement Labeling·\nLLM based classification\n1 Introduction\nRequirements Engineering (RE) concerns the identification, documentation,\nand management of software requirements (Sommerville and Sawyer, 1997).\nThe increasing interconnectivity of information systems forces organizations\nto provide secure services in cyberspace. Integrating security during initial\nrequirements analysis is essential to proactively mitigate risks, reduce vulner-\nabilities, and reduce the high costs associated with downstream remediation\n(Villamizar et al., 2018). This proactive approach is central to the ”Shift-\nLeft” security paradigm, which advocates for the identification and mitigation\nof vulnerabilities as early as possible in the Software Development Life Cycle\n(SDLC).\nAlthough Security Requirements Engineering (SRE) has matured as a dis-\ncipline, its effective application in practice remains challenging. The issue is\nnot the absence of security considerations, but the difficulty of ensuring that\nsecurity requirements are properly validated and verified. An empirical investi-\ngation that combines a systematic literature review with a survey of 58 indus-\ntry professionals highlights this gap (Alam et al., 2025). While practitioners\nreported a moderate understanding of SRE practices, Security Requirements\nAssurance (SRA) was found to be poorly understood and rarely applied in\nindustrial contexts. The study concludes that validating and ensuring security\nrequirements remains a major and unresolved challenge in practice.\nThis assurance gap persists due to the documented lack of specialized se-\ncurity training for developers in early-phase RE (Andrade et al., 2023). The\ntask relies heavily on scarce specialized expertise and time-intensive manual\nanalysis. The main challenge is that critical security requirements are often not\nexplicitly stated. Instead, they are implicitly embedded within standard func-\ntional descriptions. For example, a seemingly simple functional requirement,', 'CONTEXT: This chunk focuses on the comparative performance analysis of models across OWASP ASVS classes, highlighting challenges with rare categories (e.g., Stored Cryptography, File Handling) through heatmaps and gap analysis, while addressing dataset limitations and threats to validity related to linguistic diversity and generalizability.\n\nCONTENT: 30 Yusuf G¨ ur et al.\nFig. 2: Critical Difference (CD) diagram visualizing the statistical comparison\nof classifiers. The horizontal axis represents the average rank of each model\nacross all 11 OWASP ASVS classes (lower ranks indicate better performance).\nThe Critical Difference (CD = 10.46) represents the minimum difference in\naverage rank required for two models to be considered statistically significantly\ndifferent atp <0.05.\n5.6 Comparative Visualization Analysis\nTo complement the statistical rankings, we visualized the performance land-\nscape across all 20 model configurations.\nHeatmap Analysis: Figure 3 shows F1-scores by class. Requirements related\nto V2 (Authentication), V3 (Session Management), and V4 (Access Control)\nare handled well by nearly all models, with scores typically above 0.95. These\ncategories are common and appear consistently across training sources. In\ncontrast, V6 (Stored Cryptography) and V12 (Files and Resources) show low\nscores across the board, including for GPT-4o and Gemini 2.0 Pro. This pat-\ntern points to a systematic difficulty with sparse categories rather than a\nweakness of a particular model.\nGap Analysis: Figure 4 contrasts average performance on frequent and in-\nfrequent classes. Fine-tuned models such as SecureBERT and BERT-Multilingual\nperform better on rare classes, with F1-scores around 0.70–0.80. Zero-shot\nLLMs perform poorly in these cases, often below 0.30. Few-shot prompting\nimproves results for some models, such as Gemini 2.0, but the gap remains\nsubstantial.\n6 Threats to Validity\nOur dataset consists of 2,652 requirements derived from six Software Require-\nments Specification (SRS) documents. While the number of source documents\n\nImplicit Security Requirements Classification 31\nFig. 3:Comprehensive Performance Heatmap.F1-scores for all 20 eval-\nuated models across 11 OWASP ASVS classes. The visualization highlights\nthe “zone of failure” (red) for classes V6 and V12 across models, contrasting\nwith the high performance (green) on ubiquitous classes like V2 and V4.\nis limited, potential bias was mitigated by strategically selecting projects from\nsix distinct business domains: Construction, Legal, Education, Software, Meet-\ning Management, and Accommodation. This semantic diversity ensures that\nthe models are evaluated on a broad vocabulary and varied functional contexts\nrather than a single industry vertical. A specific consideration regarding the\nexternal validity of this study is that the ASRD is a Turkish-language corpus\nwhich allows for the evaluation of automated tools in linguistically diverse\nindustrial contexts.\nFurthermore, because security requirements (e.g., Authentication, Log-\nging) represent cross-cutting concerns that apply universally across software\nsystems, we believe the fundamental patterns captured in the ASRD are trans-\nferable to other industries. Our current evaluation used a stratified split across\nthe entire dataset, meaning requirements from all six source projects are\npresent in the training, validation, and test sets. However, we acknowledge\nthat this approach does not fully measure the models’ ability to generalize to\nentirely new projects with different authors or linguistic styles, as the test set\nis not completely “unseen” in terms of project context.']","The annotation of the ASRD required **267 person-hours** of expert labor to annotate 2,652 requirements. This extensive effort was necessary to establish a reliable ground truth for fine-tuned BERT models and validate the dataset. 

(Source: temp/b31390a6-3d40-4aa2-b634-17feb052e723_template.pdf)","The annotation process took 267 person-hours of expert time[cite: 273, 650].",0.699999999965,1.0,0.9043566789322643
What is SecureBERT and why was it included in the study?,"['CONTEXT: This chunk provides a list of references and related works that underpin the research on automating the classification of implicit security requirements, highlighting key studies, methodologies, and advancements in cybersecurity-focused NLP, including domain-specific models like SecureBERT and broader applications of AI in requirements engineering.\n\nCONTENT: Implicit Security Requirements Classification 39\nAghaei E, Niu X, Shadid W, Al-Shaer E (2022) Securebert: A domain-specific\nlanguage model for cybersecurity. In: International Conference on Security\nand Privacy in Communication Systems, Springer, pp 39–56\nAghaei S, Al-Shaer E, Duan Z (2020) SecureBERT: A domain-specific language\nmodel for cybersecurity.2105.04293\nAI at Meta (2024) The llama 3 herd of models. Tech. rep.,\nMeta, URLhttps://ai.meta.com/research/publications/\nthe-llama-3-herd-of-models/\nAlam JA, Ayman M, Rehman GA, Sarlan AB, et al. (2025) Security require-\nments engineering: A review and analysis. Computers 14(10):429\nAndrade R, Torres J, Ortiz-Garc´ es I, Mi˜ no J, Almeida L (2023) An exploratory\nstudy gathering security requirements for the software development process.\nElectronics 12(17):3594\nAnwar Mohammad MN, Nazir M, Mustafa K (2019) A systematic review\nand analytical evaluation of security requirements engineering approaches.\nArabian Journal for Science and Engineering 44(11):8963–8987\nBatool R, Naseer A, Maqbool A, Kayani M (2025) Automated categorization of\nsoftware security requirements: an nlp and ml based approach. Requirements\nEngineering pp 1–13\nBayerl PS, Paul KI (2011) What determines inter-coder agreement in man-\nual annotations? a meta-analytic investigation. Computational Linguistics\n37(4):699–725\nBolanos F, Salatino A, Osborne F, Motta E (2024) Artificial intelligence for lit-\nerature reviews: Opportunities and challenges. Artificial Intelligence Review\n57(10):259\nChristodoulopoulos C, Chakraborty T, Rose C, Peng V (2025) Proceedings of\nthe 2025 conference on empirical methods in natural language processing.\nIn: Proceedings of the 2025 Conference on Empirical Methods in Natural\nLanguage Processing\nCohen KB, Fox L, Ogren P, Hunter L (2005) Corpus design for biomedical\nnatural language processing. In: Proceedings of the ACL-ISMB workshop\non linking biological literature, ontologies and databases: mining biological\nsemantics, pp 38–45\nDalpiaz F, Dell’Anna D, Aydemir FB, C ¸ evikol S (2019) Requirements clas-\nsification with interpretable machine learning and dependency parsing. In:\n2019 IEEE 27th International Requirements Engineering Conference (RE),\nIEEE, pp 142–152\nDeepSeek AI (2024) DeepSeek-V2: A strong, economical, and efficient mixture-\nof-experts language model. arXiv preprint arXiv:240504434 URLhttps:\n//arxiv.org/abs/2405.04434\nDekhtyar A, Fong V (2017) Re data challenge: Requirements identification\nwith word2vec and tensorflow. In: 2017 IEEE 25th International Require-\nments Engineering Conference (RE), IEEE, pp 484–489\nDemˇ sar J (2006) Statistical comparisons of classifiers over multiple data sets.\nJournal of Machine learning research 7(Jan):1–30', 'CONTEXT: **Contextual Header:** This chunk provides a background on prior work in NLP-based techniques for software requirements classification, highlighting the evolution from traditional supervised learning methods to advanced transformer-based models like BERT and SecureBERT, and discussing the challenges of classifying implicit security requirements due to their abstract nature and reliance on domain expertise.\n\nCONTENT: Implicit Security Requirements Classification 5\nthreats to validity, and Section 8 concludes with a summary of contributions\nand directions for future work.\n2 Background\nThis section reviews prior work on NLP-based techniques for requirements\nclassification, followed by an overview of relevant security frameworks. It con-\ncludes with a discussion of existing datasets used in security requirements\nresearch.\n2.1 Related Work on Requirements Classification using NLP Techniques\nSoftware requirements (SR) classification supports effective project manage-\nment by enabling prioritization and risk assessment (Batool et al., 2025). The\ntask typically involves distinguishing functional requirements (FRs) from non-\nfunctional requirements (NFRs) and further categorizing NFRs into classes\nsuch as security, usability, and performance. Security requirements may ap-\npear as explicit system behaviors or as quality attributes addressing confiden-\ntiality, integrity, and availability—the security triad—which are often abstract\nand difficult to specify precisely in practice (Anwar Mohammad et al., 2019).\nThis inherent ambiguity complicates the development of clear guidelines for\nseparating security-related requirements from non-security ones.\nEarly studies applied supervised learning techniques, including Bayesian\nclassifiers(Knauss et al., 2011), decision trees (Jindal et al., 2016), and Sup-\nport Vector Machines (SVMs) (Dalpiaz et al., 2019), but these approaches\ndepended heavily on manual feature engineering and rigid linguistic patterns.\nTo address these limitations, later work adopted neural models such as convo-\nlutional neural networks (CNN) combined with Word2Vec embeddings (Dekht-\nyar and Fong, 2017). The adoption of transformer-based models such as BERT\nenabled improved classification accuracy through transfer learning and fine-\ntuning, eliminating the need for handcrafted features (Devlin et al., 2019; Sub-\nahi, 2023). More recent work explores domain-adapted transformer models,\nsuch as NoRBERT (Hey et al., 2020) and SecureBERT (Aghaei et al., 2022),\nto better capture domain-specific semantics and mitigate overfitting. These\nmodels have shown improved capability in identifying implicit non-functional\nrequirements, a particularly challenging task in security-critical settings (Nec-\nula et al., 2024). However, further progress in SRE is limited by the availability\nof large, high-quality, and domain-specific annotated datasets.\nThe subjective nature of defining security requirements further complicates\nreliable classification (Riaz and Williams, 2012). Security Requirement Elic-\nitation highly depends on domain-relevant expertise which includes various\ncostly manual steps such as creating security requirement templates, deter-\nmining security classification levels according to natural language artifacts,\nand mapping the class labels to related requirement templates by identifying\nentities.', 'CONTEXT: **Performance Comparison of Models for Implicit Security Requirements Classification**  \nThis chunk focuses on the statistical analysis and performance comparison of baseline, fine-tuned, and few-shot/zero-shot LLM models, highlighting significant differences in their ability to classify implicit security requirements, with SecureBERT and Gemini 2.0 Few-Shot emerging as top performers.\n\nCONTENT: Implicit Security Requirements Classification 29\nThe analysis (Table 8) highlights the significant pairwise contrasts and\nreveals important performance distinctions:\n–Baseline vs. Advanced Models: The Naive Baseline was consistently out-\nperformed by the advanced models. The difference between the Baseline\nand models such as SecureBERT (p <0.001) and Gemini 2.0 Few-Shot\n(p <0.001) was statistically significant, confirming that the high Weighted-\nF1 scores of these models reflect genuine learning beyond simple majority-\nclass prediction.\n–Fine-Tuning vs. Zero-Shot LLMs: Fine-tuned models like SecureBERT\ndemonstrated statistically significant performance distinctions (p <0.001)\nwhen compared to general-purpose LLMs operating in a zero-shot setting,\nsuch as GPT-4o (p <0.001) and Llama 3 70B (p <0.001). This indi-\ncates that without in-context examples, domain-specific fine-tuning retains\na measurable statistical advantage.\n–Fine-Tuning vs. Few-Shot State-of-the-Art: The pairwise comparison be-\ntween the best fine-tuned model (SecureBERT) and the best LLM ap-\nproach (Gemini 2.0 Few-Shot) yielded ap-value of 0.102. This value is not\nstatistically significant at theα= 0.05 level, supporting our finding that\nfew-shot prompting with state-of-the-art LLMs can effectively match the\nperformance of specialized, fine-tuned BERT models in this domain.\nTable 8: Significant Pairwise Differences (Wilcoxon Signed-Rank Test)\nModel A Model B Statistic p-value\nNaive Baseline SecureBERT 0.0 0.00098\nNaive Baseline BERT Turkish 0.0 0.00098\nNaive Baseline Gemini 2.0 (Few-Shot) 0.0 0.00098\nNaive Baseline Gemini 2.0 Pro (Few-Shot) 0.0 0.00098\nSecureBERT GPT-4o Mini (Few-Shot) 0.0 0.00098\nSecureBERT GPT-4o Mini (Zero-Shot) 0.0 0.00098\nSecureBERT GPT-4o (Zero-Shot) 0.0 0.00098\nSecureBERT GPT-4o (Few-Shot) 0.0 0.00098\nSecureBERT Gemini 2.0 (Zero-Shot) 25.0 0.501\nSecureBERT Gemini 2.0 (Few-Shot) 14.5 0.102\nTo visualize the relative performance and statistical significance groups\nof all evaluated models, we constructed a Critical Difference (CD) diagram\n(Figure 2) using the Nemenyi post-hoc test.\nIn the CD diagram, models are arranged along the horizontal axis accord-\ning to their average rank. The best performing models, such as Gemini 2.0\nPro (Few-Shot) and BERT-Multilingual, appear on the left (lowest ranks).\nA horizontal bar connects groups of models that are not statistically signifi-\ncantly different from one another. The wide span of the CD bar (10.46) reflects\nthe high number of models compared (23) relative to the number of datasets\n(11). Despite this conservative threshold, the diagram clearly separates the\ntop-tier models (Fine-tuned BERTs and Few-Shot Gemini/GPT-4o) from the\nlower-performing group (Naive Baseline, Zero-Shot Gemma/Llama).', 'CONTEXT: This chunk introduces the motivation and objectives of the study, emphasizing the challenges of identifying implicit security requirements in Software Requirements Specification (SRS) documents and proposing a data-driven methodology using the newly developed ASRD dataset and advanced NLP models to automate this process.\n\nCONTENT: Noname manuscript No.\n(will be inserted by the editor)\nImplicit Security Requirements Classification\nwith Large Language Models Using the OWASP\nApplication Security Verification Standard: A\nShift-Left Approach.\nYusuf G¨ ur·Tu˘ gba Ta¸ skaya Temizel·\nBanu G¨ unel Kılı¸ c\nReceived: date / Accepted: date\nAbstractCybersecurity threats require early integration of security, starting\nfrom the requirements analysis phase of the Software Development Life Cycle\n(SDLC). However, security requirements in Software Requirements Specifica-\ntion (SRS) documents are often implicitly embedded, making their manual\nidentification time-consuming, error-prone, and reliant on specialized exper-\ntise. The accurate classification of security requirements (SR) is important for\neffective resource allocation and risk management in software development.\nAutomated tools to extract implicit security requirements are lacking, largely\ndue to the scarcity of large, annotated datasets in Security Requirements En-\ngineering (SRE). This paper proposes a data-driven methodology to automate\nthe classification of implicit security requirements in SRS documents, sup-\nporting the early and systematic integration of security into software systems.\nWe introduce a novel multi-label corpus, the Agency Security Requirements\nDataset (ASRD), derived from 2,652 real-world requirement statements from\nsix diverse documents and annotated using a high-granularity taxonomy based\non the OWASP Application Security Verification Standard (ASVS) V2-V13\nand the MATTER cycle annotation framework by three cybersecurity experts.\nUsing this dataset, we evaluate both supervised fine-tuned BERT variants\n(such as SecureBERT) and general-purpose large-language models (LLMs) in-\ncluding Gemma, GPT, DeepSeek, Meta Llama, and Gemini under zero-shot\nand few-shot settings. We conduct an empirical comparison between tradi-\nYusuf G¨ ur\nGraduate School of Informatics, Middle East Technical University, Ankara 06800, T¨ urkiye\nE-mail: yusuf.gur@metu.edu.tr\nTu˘ gba Ta¸ skaya Temizel\nGraduate School of Informatics, Middle East Technical University, Ankara 06800, T¨ urkiye\nE-mail: ttemizel@metu.edu.tr\nBanu G¨ unel Kılı¸ c\nGraduate School of Informatics, Middle East Technical University, Ankara 06800, T¨ urkiye\nE-mail: bgunel@metu.edu.tr', 'CONTEXT: This chunk provides detailed performance metrics (Precision, Recall, F1-scores, and Support) for various BERT-based transformer models, including SecureBERT, in classifying security requirements across OWASP ASVS categories, highlighting their effectiveness and challenges in handling imbalanced classes.\n\nCONTENT: bert-base-multilingual-uncased\nV2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements\nV10, Malicious Code Verification Requirements\nV12, File and Resources Verification Requirements\nV13, API and Web Service Verification Requirements\nPRECISION 0.984 0.982 0.982 0.975 0.744 0.985 0.980 0.984 0.983 0.864 0.991RECALL 0.997 0.995 0.995 0.984 0.842 0.995 1.000 1.000 0.983 0.704 0.994F1 0.990 0.988 0.988 0.979 0.790 0.990 0.990 0.992 0.983 0.776 0.992SUPPORT 382 381 385 314 38 389 384 380 299 27 317TOTAL 398 398 398 398 398 398 398 398 398 398 398\nbert-base-turkish-cased\nV2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements\nV10, Malicious Code Verification Requirements\nV12, File and Resources Verification Requirements\nV13, API and Web Service Verification Requirements\nPRECISION 0.974 0.974 0.980 0.975 0.756 0.985 0.982 0.974 0.980 0.800 0.972RECALL 0.997 0.997 1.000 0.994 0.816 0.997 1.000 1.000 0.977 0.593 0.994F1 0.985 0.985 0.990 0.984 0.785 0.991 0.991 0.987 0.978 0.681 0.983SUPPORT 382 381 385 314 38 389 384 380 299 27 317TOTAL 398 398 398 398 398 398 398 398 398 398 398\nbert-base-uncased\nV2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements\nV10, Malicious Code Verification Requirements\nV12, File and Resources Verification Requirements\nV13, API and Web Service Verification Requirements\nPRECISION 0.982 0.979 0.982 0.981 0.781 0.985 0.980 0.974 0.977 0.813 0.978RECALL 0.995 0.995 0.995 0.975 0.658 0.992 0.997 0.997 0.987 0.481 0.987F1 0.988 0.987 0.988 0.978 0.714 0.988 0.988 0.985 0.982 0.604 0.982SUPPORT 382 381 385 314 38 389 384 380 299 27 317TOTAL 398 398 398 398 398 398 398 398 398 398 398\nSecureBERT\nV2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements\nV10, Malicious Code Verification Requirements\nV12, File and Resources Verification Requirements\nV13, API and Web Service Verification Requirements\nPRECISION 0.974 0.977 0.980 0.972 0.707 0.985 0.977 0.974 0.964 0.700 0.969RECALL 0.995 0.995 0.997 0.984 0.763 0.997 1.000 0.997 0.990 0.519 0.997F1 0.984 0.986 0.988 0.978 0.734 0.991 0.988 0.985 0.977 0.596 0.983SUPPORT 382 381 385 314 38 389 384 380 299 27 317TOTAL 398 398 398 398 398 398 398 398 398 398 398\nNaive Baseline\nV2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements', 'CONTEXT: This chunk provides details on the BERT-based models used in the study, including their architectures, training data, and rationale for selection, as well as the fine-tuning methodology employed to adapt these models for multi-label classification of security requirements in the ASRD dataset.\n\nCONTENT: Implicit Security Requirements Classification 21\nTable 5: Details and Rationale of the BERT-based Models Used in the Study\nModel Name\n(Architecture /\nParameters)\nDescription, Training Data,\nand Source\nRationale in Study\nBERT Base (En-\nglish)\n(12 layers, 768\nhidden, 12 heads,\n110M params.)\nStandard BERT-base archi-\ntecture. Pre-trained on the\nBookCorpus and English\nWikipedia.8(Devlin et al., 2019)\nTo serve as a strong performance\nbaseline.\nSecureBERT\n(12 layers, 768\nhidden, 12 heads,\n125M params.)\nBased on the RoBERTa architec-\nture. Pre-trained on a 12GB cor-\npus of cybersecurity texts (NVD,\nCVE, CAPEC). 9(Aghaei et al.,\n2020)\nTo test the hypothesis that\ndomain-specific vocabulary and\ncontext improve classification ac-\ncuracy.\nBERT Base\nMultilingual\n(12 layers, 768\nhidden, 12 heads,\n179M params.)\nStandard BERT-base architec-\nture. Pre-trained on the top\n104 languages with the largest\nWikipedia datasets. 10(Devlin\net al., 2019)\nTo investigate whether training\non a broader set of languages\nprovides any generalization ben-\nefit.\nBERT Base\nTurkish\n(12 layers, 768\nhidden, 12 heads,\n110M params.)\nStandard BERT-base architec-\nture. Pre-trained on a 35GB,\n4.4B token Turkish corpus from\nmultiple sources. 11(Schweter,\n2020)\nTo explore the performance of\na model pre-trained on a lan-\nguage with a different morpho-\nlogical structure than English.\nthe final hidden state of the [CLS] token to a vector corresponding to our\n11 security classes. A sigmoid activation function was applied to this layer to\nproduce independent probabilities, facilitating multi-label classification. The\nfine-tuning process was performed from end-to-end; all weights were unfrozen\nand updated to adapt the models to the specific nuances of the security re-\nquirements text.\nThe models were trained using the PyTorch Lightning framework to ensure\nreproducibility. We employed the AdamW optimizer (Loshchilov and Hutter,\n2019), an extension of the Adam optimizer with improved weight decay, uti-\nlizing a linear learning rate schedule with a warmup phase covering the first\n20% of training steps. This strategy stabilizes training in the initial epochs and\nimproves convergence. Given the multi-label nature of the task, Binary Cross-\nEntropy (BCE) Loss was used as the primary objective function. Because the\nsecurity classes are unevenly distributed, the label assignment threshold (Deci-\nsion Threshold) was tuned rather than fixed at 0.5 to better balance precision\nand recall across categories.', 'CONTEXT: This chunk focuses on the comparative performance analysis of models across OWASP ASVS classes, highlighting challenges with rare categories (e.g., Stored Cryptography, File Handling) through heatmaps and gap analysis, while addressing dataset limitations and threats to validity related to linguistic diversity and generalizability.\n\nCONTENT: 30 Yusuf G¨ ur et al.\nFig. 2: Critical Difference (CD) diagram visualizing the statistical comparison\nof classifiers. The horizontal axis represents the average rank of each model\nacross all 11 OWASP ASVS classes (lower ranks indicate better performance).\nThe Critical Difference (CD = 10.46) represents the minimum difference in\naverage rank required for two models to be considered statistically significantly\ndifferent atp <0.05.\n5.6 Comparative Visualization Analysis\nTo complement the statistical rankings, we visualized the performance land-\nscape across all 20 model configurations.\nHeatmap Analysis: Figure 3 shows F1-scores by class. Requirements related\nto V2 (Authentication), V3 (Session Management), and V4 (Access Control)\nare handled well by nearly all models, with scores typically above 0.95. These\ncategories are common and appear consistently across training sources. In\ncontrast, V6 (Stored Cryptography) and V12 (Files and Resources) show low\nscores across the board, including for GPT-4o and Gemini 2.0 Pro. This pat-\ntern points to a systematic difficulty with sparse categories rather than a\nweakness of a particular model.\nGap Analysis: Figure 4 contrasts average performance on frequent and in-\nfrequent classes. Fine-tuned models such as SecureBERT and BERT-Multilingual\nperform better on rare classes, with F1-scores around 0.70–0.80. Zero-shot\nLLMs perform poorly in these cases, often below 0.30. Few-shot prompting\nimproves results for some models, such as Gemini 2.0, but the gap remains\nsubstantial.\n6 Threats to Validity\nOur dataset consists of 2,652 requirements derived from six Software Require-\nments Specification (SRS) documents. While the number of source documents\n\nImplicit Security Requirements Classification 31\nFig. 3:Comprehensive Performance Heatmap.F1-scores for all 20 eval-\nuated models across 11 OWASP ASVS classes. The visualization highlights\nthe “zone of failure” (red) for classes V6 and V12 across models, contrasting\nwith the high performance (green) on ubiquitous classes like V2 and V4.\nis limited, potential bias was mitigated by strategically selecting projects from\nsix distinct business domains: Construction, Legal, Education, Software, Meet-\ning Management, and Accommodation. This semantic diversity ensures that\nthe models are evaluated on a broad vocabulary and varied functional contexts\nrather than a single industry vertical. A specific consideration regarding the\nexternal validity of this study is that the ASRD is a Turkish-language corpus\nwhich allows for the evaluation of automated tools in linguistically diverse\nindustrial contexts.\nFurthermore, because security requirements (e.g., Authentication, Log-\nging) represent cross-cutting concerns that apply universally across software\nsystems, we believe the fundamental patterns captured in the ASRD are trans-\nferable to other industries. Our current evaluation used a stratified split across\nthe entire dataset, meaning requirements from all six source projects are\npresent in the training, validation, and test sets. However, we acknowledge\nthat this approach does not fully measure the models’ ability to generalize to\nentirely new projects with different authors or linguistic styles, as the test set\nis not completely “unseen” in terms of project context.', 'CONTEXT: **Evaluation Protocol and BERT-Based Model Classification**: This chunk details the experimental methodology for evaluating fine-tuned BERT models and LLMs in classifying implicit security requirements, including data partitioning, performance metrics, and the transfer learning approach used to adapt pre-trained BERT models for the task.\n\nCONTENT: 20 Yusuf G¨ ur et al.\n4 Implicit Security Requirements Classification Methodology\nThis section presents the experiment protocol used to assess the two research\nquestions regarding the performance of fine-tuned BERT variant models in\nclassification of requirements statements and LLM models with zero and few-\nshot prompting.\n4.1 Evaluation Protocol and Data Usage\nThe dataset was split into training (70%, n=1,856), validation (15%, n=398),\nand test (15%, n=398) sets, with the validation set used for hyperparameter\ntuning and the test set reserved for final evaluation. Stratified sampling based\non security class labels was applied to preserve label distributions across all\nsplits. The test set was sized to include sufficient samples of minority classes\n(e.g., V6 and V12), while the training set remained large enough to support\neffective fine-tuning.\nAlthough zero-shot LLMs do not require data partitioning and few-shot\nsettings rely on only a small number of examples, the shared test set was\nretained to ensure a fair comparison between fine-tuned model variants and\nprompt-based LLM approaches.\nWe used a comprehensive suite of metrics for classification performance\nevaluation. We report Precision, which measures the accuracy of positive pre-\ndictions, and Recall, which measures the model’s ability to identify all rele-\nvant instances. The F1-score, the harmonic mean of precision and recall, is\nused to provide a single measure balancing this trade-off. To assess overall\nperformance across all 11 security classes, we calculated the micro-averaged\nF1, which reflects aggregate accuracy, the macro-averaged F1, which assesses\nperformance by treating all classes equally regardless of their frequency, and\nthe weighted-averaged F1, which accounts for class support.\n4.2 Classification using BERT based Models\nWe conducted a comparative analysis using four distinct BERT-based mod-\nels to systematically evaluate the impact of different pre-training strategies.\nThis approach allowed us to test hypotheses related to domain-specificity, lan-\nguage, and multilingualism, with the specific technical details of each model\nsummarized in Table 5.\nFor the classification task, we adopted a standard transfer learning ap-\nproach by adding a task-specific classification “head” on top of each pre-trained\nmodel. This head consists of a single fully-connected linear layer that maps\n8 https://huggingface.co/bert-base-uncased\n9 https://huggingface.co/ehsanaghaei/SecureBERT\n10 https://huggingface.co/bert-base-multilingual-cased\n11 https://huggingface.co/dbmdz/bert-base-turkish-cased']","SecureBERT is a domain-specific language model based on the RoBERTa architecture, pre-trained on a 12GB corpus of cybersecurity texts, including sources like NVD (National Vulnerability Database), CVE (Common Vulnerabilities and Exposures), and CAPEC (Common Attack Pattern Enumeration and Classification). It was designed to capture domain-specific vocabulary and context, making it particularly effective for tasks in the cybersecurity domain, such as classifying implicit security requirements.

SecureBERT was included in the study to test the hypothesis that domain-specific pre-training improves classification accuracy for implicit security requirements. The study aimed to evaluate its performance against other models, including general-purpose BERT variants and large language models (LLMs), in classifying security requirements across OWASP ASVS categories. SecureBERT demonstrated strong performance, particularly in handling rare and domain-specific security classes, highlighting its effectiveness in addressing the challenges of implicit security requirements classification.

(Source: temp/b31390a6-3d40-4aa2-b634-17feb052e723_template.pdf)","SecureBERT is a domain-specific language model pre-trained on cybersecurity texts (NVD, CVE, CAPEC). It was included to test if domain-specific vocabulary improves classification accuracy[cite: 375].",0.8541666666453125,1.0,0.8210623036288229
Why was the bert-base-turkish-cased model included?,"['CONTEXT: **Performance Comparison of Models for Implicit Security Requirements Classification**: This chunk presents a detailed comparison of F1-scores across various models, including fine-tuned BERT models, LLMs, and baseline approaches, highlighting their performance on imbalanced security requirement categories and the challenges faced with rare classes.\n\nCONTENT: Implicit Security Requirements Classification 25\nTable 7: Aggregated F1-Scores for LLM, BERT Models, and Baseline\nModel Prompting Strategy Macro-F1 Micro-F1 Weighted-F1\nLarge Language Model Results\nLlama 3 70b Zero Shot 0.813 0.941 0.941\nLlama 3 70b Few Shot 0.802 0.925 0.925\nDeepSeek R1 Zero Shot 0.812 0.921 0.916\nDeepSeek R1 Few Shot 0.820 0.927 0.921\nGemini 2.5 Zero Shot 0.8290.957 0.953\nGemini 2.5 Few Shot 0.8170.963 0.959\nGemini 2.0 Zero Shot0.8900.925 0.921\nGemini 2.0 Few Shot0.9410.938 0.938\nGemini 2.0 Pro Zero Shot 0.830 0.908 0.904\nGemini 2.0 Pro Few Shot 0.889 0.959 0.953\nGemma 3 27b Zero Shot 0.806 0.912 0.907\nGemma 3 27b Few Shot 0.802 0.917 0.912\nGemma 3 4b Zero Shot 0.605 0.738 0.742\nGemma 3 4b Few Shot 0.723 0.871 0.867\nGpt 4o Zero Shot 0.790 0.912 0.907\nGpt 4o Few Shot 0.814 0.943 0.939\nGpt 4o mini Zero Shot 0.703 0.803 0.809\nGpt 4o mini Few Shot 0.754 0.852 0.854\nBERT Fine-Tuning Results\nbert-base-multilingual-uncased Fine-Tuning0.942 0.985 0.983\nbert-base-turkish-cased Fine-Tuning 0.933 0.983 0.981\nbert-base-uncased Fine-Tuning 0.914 0.978 0.976\nSecureBERT Fine-Tuning 0.918 0.979 0.976\nBaseline\nNaive Baseline Majority Class 0.774 0.940 0.933\nrepresented categories. The bert-base-multilingual-uncased model achieved the\nhighest Weighted-F1 score at 0.983, while the lowest among the group was\n0.976 for both bert-base-uncased and SecureBERT. However, these results ob-\nscure the impact of class imbalance. Macro-F1 scores, which give equal weight\nto all categories, were lower, ranging from 0.942 to 0.914, revealing reduced\nperformance on less frequent classes.\nThis pattern is clear in the category-level results (Appendix C). Perfor-\nmance was high for common categories such as V8 (Data Protection), where\nthe bert-base-multilingual-uncased model achieved an F1-score of 0.992. In\ncontrast, results declined significantly for sparsely represented categories. For\nV6 (38 samples), F1-scores fell to 0.714 for bert-base-uncased and 0.734 for\nSecureBERT. The weakest performance was observed for V10 (27 samples),\nwith F1-scores around 0.60, indicating limited generalization due to limited\ndata.\nAn analysis of precision and recall for these difficult classes reveals specific\nfailure patterns. For class V10, the bert-base-uncased model has a low Recall of\n0.481. This indicates that while its predictions were often correct (Precision:\n0.813), it failed to identify more than half of the actual V10 requirements,\nresulting in a high number of false negatives. For class V6, the same model', 'CONTEXT: This chunk provides detailed performance metrics (Precision, Recall, F1-scores, and Support) for various BERT-based transformer models, including SecureBERT, in classifying security requirements across OWASP ASVS categories, highlighting their effectiveness and challenges in handling imbalanced classes.\n\nCONTENT: bert-base-multilingual-uncased\nV2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements\nV10, Malicious Code Verification Requirements\nV12, File and Resources Verification Requirements\nV13, API and Web Service Verification Requirements\nPRECISION 0.984 0.982 0.982 0.975 0.744 0.985 0.980 0.984 0.983 0.864 0.991RECALL 0.997 0.995 0.995 0.984 0.842 0.995 1.000 1.000 0.983 0.704 0.994F1 0.990 0.988 0.988 0.979 0.790 0.990 0.990 0.992 0.983 0.776 0.992SUPPORT 382 381 385 314 38 389 384 380 299 27 317TOTAL 398 398 398 398 398 398 398 398 398 398 398\nbert-base-turkish-cased\nV2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements\nV10, Malicious Code Verification Requirements\nV12, File and Resources Verification Requirements\nV13, API and Web Service Verification Requirements\nPRECISION 0.974 0.974 0.980 0.975 0.756 0.985 0.982 0.974 0.980 0.800 0.972RECALL 0.997 0.997 1.000 0.994 0.816 0.997 1.000 1.000 0.977 0.593 0.994F1 0.985 0.985 0.990 0.984 0.785 0.991 0.991 0.987 0.978 0.681 0.983SUPPORT 382 381 385 314 38 389 384 380 299 27 317TOTAL 398 398 398 398 398 398 398 398 398 398 398\nbert-base-uncased\nV2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements\nV10, Malicious Code Verification Requirements\nV12, File and Resources Verification Requirements\nV13, API and Web Service Verification Requirements\nPRECISION 0.982 0.979 0.982 0.981 0.781 0.985 0.980 0.974 0.977 0.813 0.978RECALL 0.995 0.995 0.995 0.975 0.658 0.992 0.997 0.997 0.987 0.481 0.987F1 0.988 0.987 0.988 0.978 0.714 0.988 0.988 0.985 0.982 0.604 0.982SUPPORT 382 381 385 314 38 389 384 380 299 27 317TOTAL 398 398 398 398 398 398 398 398 398 398 398\nSecureBERT\nV2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements\nV10, Malicious Code Verification Requirements\nV12, File and Resources Verification Requirements\nV13, API and Web Service Verification Requirements\nPRECISION 0.974 0.977 0.980 0.972 0.707 0.985 0.977 0.974 0.964 0.700 0.969RECALL 0.995 0.995 0.997 0.984 0.763 0.997 1.000 0.997 0.990 0.519 0.997F1 0.984 0.986 0.988 0.978 0.734 0.991 0.988 0.985 0.977 0.596 0.983SUPPORT 382 381 385 314 38 389 384 380 299 27 317TOTAL 398 398 398 398 398 398 398 398 398 398 398\nNaive Baseline\nV2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements', 'CONTEXT: **Evaluation Protocol and BERT-Based Model Classification**: This chunk details the experimental methodology for evaluating fine-tuned BERT models and LLMs in classifying implicit security requirements, including data partitioning, performance metrics, and the transfer learning approach used to adapt pre-trained BERT models for the task.\n\nCONTENT: 20 Yusuf G¨ ur et al.\n4 Implicit Security Requirements Classification Methodology\nThis section presents the experiment protocol used to assess the two research\nquestions regarding the performance of fine-tuned BERT variant models in\nclassification of requirements statements and LLM models with zero and few-\nshot prompting.\n4.1 Evaluation Protocol and Data Usage\nThe dataset was split into training (70%, n=1,856), validation (15%, n=398),\nand test (15%, n=398) sets, with the validation set used for hyperparameter\ntuning and the test set reserved for final evaluation. Stratified sampling based\non security class labels was applied to preserve label distributions across all\nsplits. The test set was sized to include sufficient samples of minority classes\n(e.g., V6 and V12), while the training set remained large enough to support\neffective fine-tuning.\nAlthough zero-shot LLMs do not require data partitioning and few-shot\nsettings rely on only a small number of examples, the shared test set was\nretained to ensure a fair comparison between fine-tuned model variants and\nprompt-based LLM approaches.\nWe used a comprehensive suite of metrics for classification performance\nevaluation. We report Precision, which measures the accuracy of positive pre-\ndictions, and Recall, which measures the model’s ability to identify all rele-\nvant instances. The F1-score, the harmonic mean of precision and recall, is\nused to provide a single measure balancing this trade-off. To assess overall\nperformance across all 11 security classes, we calculated the micro-averaged\nF1, which reflects aggregate accuracy, the macro-averaged F1, which assesses\nperformance by treating all classes equally regardless of their frequency, and\nthe weighted-averaged F1, which accounts for class support.\n4.2 Classification using BERT based Models\nWe conducted a comparative analysis using four distinct BERT-based mod-\nels to systematically evaluate the impact of different pre-training strategies.\nThis approach allowed us to test hypotheses related to domain-specificity, lan-\nguage, and multilingualism, with the specific technical details of each model\nsummarized in Table 5.\nFor the classification task, we adopted a standard transfer learning ap-\nproach by adding a task-specific classification “head” on top of each pre-trained\nmodel. This head consists of a single fully-connected linear layer that maps\n8 https://huggingface.co/bert-base-uncased\n9 https://huggingface.co/ehsanaghaei/SecureBERT\n10 https://huggingface.co/bert-base-multilingual-cased\n11 https://huggingface.co/dbmdz/bert-base-turkish-cased']","The **bert-base-turkish-cased** model was included as part of a comparative analysis to evaluate the performance of different BERT-based models in classifying implicit security requirements. This analysis aimed to test hypotheses related to domain-specificity, language, and multilingualism. The model was fine-tuned for the task and achieved competitive results, with a Weighted-F1 score of 0.981, demonstrating its effectiveness in handling the classification task. Its inclusion highlights the exploration of language-specific pre-trained models in addressing security requirement classification challenges.

(Source: temp/b31390a6-3d40-4aa2-b634-17feb052e723_template.pdf)",It was included to explore the performance of a model pre-trained on the native language (Turkish) of the dataset[cite: 375].,0.8333333332916666,1.0,0.7911277841622255
Why is the OWASP ASVS framework preferred over STRIDE for this specific task?,"['CONTEXT: This chunk details the annotation methodology and dataset curation process for the ASRD, following the MATTER cycle framework. It emphasizes the selection of expert annotators, corpus preparation through anonymization, and the iterative development of annotation guidelines based on OWASP ASVS categories to ensure consistency and accuracy in classifying implicit security requirements.\n\nCONTENT: Implicit Security Requirements Classification 15\n3.3 Annotation Methodology (MATTER Cycle) & Dataset Curation\nThe annotation process followed the MATTER development cycle, an estab-\nlished iterative framework for corpus annotation (Pustejovsky and Stubbs,\n2012). Crucially, the annotation task was defined as a multi-label text classifi-\ncation problem. It consists of the phases Model, Annotate, Train, Test, Eval-\nuate, and Revise. The annotation guidelines were developed and iteratively\nrefined to ensure consistency and clarity.\nThe Model–Annotate cycle involved writing guidelines, selecting annota-\ntors, conducting trial annotations, and refining the process. Once stabilized,\nthis cycle was expanded to the full dataset to generate a Gold Standard corpus.\nThe annotation workflow included the following processes:\n–Annotator Selection: Following Bayerl and Paul’s recommendation to use\ndomain-aligned annotators (Bayerl and Paul, 2011), three cybersecurity\nsubject matter experts (SME) from the agency, each with over 15 years\nof experience in designing security architectures, analyzing threat models,\nand identifying security requirements, performed the annotations. Their\ndeep practical experience ensured accurate and context-aware interpreta-\ntion which aligned with the requirement “experts must demonstrate signif-\nicant skills, knowledge, and experience” emphasized by Hopkins (Hopkins\nand Unger, 2017). The senior manager among them also provided over-\nsight and led the adjudication process. The senior SME acted as both an\nannotator and the final adjudicator for resolving disagreements. They also\nprovided feedback to improve the quality and clarity of the annotation\nguideline.\n–Corpus Selection and Preparation: Cohen (Cohen et al., 2005) recommends\nthat during corpus design, balance and representativeness of the corpus\nis important. In line with his recommendation, of the 19 initial project\ndocuments, six were chosen for their domain diversity and potential for\neffective anonymization by the SMEs. These documents underwent a semi-\nautomated anonymization process to remove all project-specific and per-\nsonally identifiable information (PII). The process targeted entities such\nas names of individuals and organizations, project codenames, and specific\ntechnical identifiers. This was done using custom scripts with placeholder\nreplacement, followed by manual review to ensure that anonymization did\nnot alter the meaning, structure, or intent of the original requirements. The\nextracted requirements were then randomized and masked to prevent po-\ntential bias related to document origin or sequence during the annotation\nphase. Total corpus has 2,652 requirements (30 pilot + 2,622 independent\nannotations).\n–Annotation Guideline Development: The Annotation Guidelines’ develop-\nment involved an initial iterative process, refining the guidelines three times\nto resolve ambiguities before the formal assessment phase. Annotation rules\nwere based on OWASP ASVS categories V2–V13 and refined iteratively\nusing example annotations. Following best practices suggested by Cohen\n(Cohen et al., 2005), the guidelines and annotation documents ensured:', 'CONTEXT: **Proposed Future Directions for Addressing Dataset Sparsity and Improving Classification Accuracy:** This chunk outlines three key algorithmic advancements—Dynamic Few-Shot Selection with RAG, a Human-AI Collaborative Framework, and Synthetic Data Injection—to tackle challenges like class imbalance and rare security requirements, while also suggesting improved evaluation methods for model generalizability.\n\nCONTENT: Implicit Security Requirements Classification 37\ntime. This approach effectively substitutes the need for dense in-context ex-\namples with explicit external standards, ensuring accurate classification for\nrare security events where traditional fine-tuning or few-shot prompting fails.\nConsequently, this study suggests that future NLP research in SRE cannot\nrely solely on organic dataset expansion. To overcome this inherent sparsity,\nwe propose as future work three targeted algorithmic advancements:\n1. Dynamic Few-Shot Selection with Retrieval-Augmented Generation (RAG):\nSystems might dynamically retrieve the explicit definitions and verifica-\ntion criteria from standards like OWASP ASVS to ground their classifi-\ncation of rare requirements. Future work should implement RAG for Dy-\nnamic Prompting in agentic approach. Instead of fixed examples, a retriever\nshould select thekmost semantically similar valid requirements from the\ntraining set to serve as in-context examples for the specific query. This\nensures the LLM is grounded with relevant architectural patterns (e.g.,\n“encryption” vs. “hashing”) rather than generic class examples.\n2. Human-AI Collaborative Multi-Agent Framework: A promising direction\nfor future work is a Human-in-the-Loop multi-agent framework that com-\nbines automated classification with targeted expert oversight for ambigu-\nous or high-risk cases. In this setup, a retrieval-augmented agent would\ndynamically select the most relevant annotated requirements using seman-\ntic similarity, replacing static few-shot examples. A second agent would\nassess classification confidence and consistency, flagging unclear cases and\nrequesting focused clarifications from a human expert when needed. A su-\npervisor agent would then consolidate these inputs and ensure that the\nfinal labels remain consistent with the OWASP ASVS taxonomy and the\nbroader project context. In parallel, an optimization agent would learn from\nhuman interventions to refine retrieval strategies and prompting over time,\nreducing manual effort while improving accuracy on challenging security\nrequirements.\n3. Synthetic Data Injection (for Supervised Models): To address the sparsity\nhindering BERT-based fine-tuning, we recommend a Teacher-Student Data\nGeneration loop. High-reasoning LLMs (e.g., GPT-4o, Gemini 2.5 Pro)\nshould be used to generate diverse synthetic requirements that implicitly\ntrigger V6/V12 classifications. These synthetic samples can populate the\ntraining set, allowing smaller supervised models (BERT) to learn robust\ndecision boundaries without requiring expensive manual annotation.\nAdditionally, future efforts should include a more stringent evaluation of\nmodel generalizability using a leave-one-project-out cross-validation approach.\nThis method involves training a model on requirements from five of the projects\nand testing it on the single, held-out project. Repeating this process for all six\nprojects would provide a much clearer and more realistic assessment of how\nwell these models perform on entirely new, unseen SRS documents, which is\na significant measure for real-world applicability.\nFurthermore, to bridge the gap between our current findings and the global\nresearch community, we plan to undertake a professional translation of the', 'CONTEXT: This chunk provides detailed descriptions of OWASP ASVS categories (V2–V13) and their associated security requirements, along with guidelines for classifying requirements based on specific operations (e.g., CRUD, file handling, sensitive data) and an output format for structured classification.\n\nCONTENT: 3.0). \nV4, Erişim Kontrolleri (Access Controls) - Covers requirements related to authorization, role management, \nand preventing unauthorized access to resources (e.g., ASVS 4.0). \nV5, ""Doğrulama, Temizleme ve Şifreleme"" (Verification, Cleaning, and Encryption) - Covers requirements \nrelated to input validation, output encoding, data sanitization, and cryptographic protection of sensitive \ndata (e.g., ASVS 5.0). \nV6, Depolanmış Kriptografi (Stored Cryptography) - Covers requirements for protecting data at rest using \ncryptography, including key management and secure storage of encryption keys (e.g., ASVS 6.0). \nV7, Hata Ayıklama ve Kayıt (Debugging and Logging) - Covers requirements related to secure logging \npractices, error handling, and preventing information leakage through debug information (e.g., ASVS 7.0). \nV8, Veri Koruma (Data Protection) - Covers requirements related to data at rest and in transit protection, \nincluding encryption, backups, and data retention policies (e.g., ASVS 8.0). \nV9, İletişim (Communication) - Covers requirements related to secure communication protocols, protecting \nagainst network attacks, and ensuring data integrity during transmission (e.g., ASVS 9.0). \nV10, Zararlı Kod (Malicious Code) - Covers requirements related to preventing and mitigating the risk of \nmalicious code injection, such as cross-site scripting (XSS) and SQL injection (e.g., ASVS 10.0). \nV12, Dosya ve Kaynakları (Files and Resources) - Covers requirements related to secure file handling, \naccess control to files and resources, and preventing directory traversal attacks (e.g., ASVS 12.0). \nV13, API ve Web Servisleri (API and Web Services) - Covers requirements specific to securing web \nservices, APIs, and microservices, including authentication, authorization, input validation, and error \nhandling (e.g., ASVS 13.0). \n \n********* \n \nExtra Information for Choosing Categories (Aligned with OWASP ASVS 4.0.3-tr): \n \nAccording to given descriptions for kind of requirements, we can decide categories as in PSV: \n \nIf Requirement describes a CRUD operation and if it is Create operation|1|1|1|1|0|1|1|1|1|0|1| \nIf Requirement describes a CRUD operation and if it is Read operation|1|1|1|1|0|1|1|1|1|0|1| \nIf Requirement describes a CRUD operation and if it is Update operation|1|1|1|1|0|1|1|1|1|0|1| \nIf Requirement describes a CRUD operation and if it is Delete operation|1|1|1|0|0|1|1|1|0|0|0| \nIf Requirement describes User Types: usage of various user types as role management|1|1|1|0|0|0|0|0|0|0|0| \nIf Requirement describes Store: actions related to storage and backup of the assets at rest, e.g., backing up \nlog files|0|0|0|1|0|1|1|0|0|1|0| \nIf Requirement describes Transfer: actions related to transfer or sharing of the assets, e.g., sending data \nfrom one service to another|0|0|0|0|0|1|1|1|0|0|1| \nIf Requirement describes Upload and Download Files etc. operation|1|1|1|1|0|1|0|1|1|1|1| \nIf Requirement describes operation on sensitive data like personal, financial, health etc|1|0|1|1|1|0|1|0|0|0|0| \n \n \n********* \nOutput (PSV- The requirements that are classified): \nWe need each requirement response in given format below within new line separator: \n<Requirement Number>|<Requirement \nDescription>|<V2>|<V3>|<V4>|<V5>|<V6>|<V7>|<V8>|<V9>|<V10>|<V12>|<V13>|<Explanation> format. \nand will contain: \n   - Requirement Number (integer) \n   - Requirement Description (string)', 'CONTEXT: **Evaluation of Model Challenges and Prompt Engineering Strategies**: This chunk discusses the challenges faced by models, particularly with underrepresented OWASP ASVS categories like V6 (Stored Cryptography) and V12 (File and Resources), and outlines the experimental setup for LLM prompt engineering, including zero-shot and few-shot configurations, persona-based prompting, and strategies to mitigate prompt sensitivity and contextual bias.\n\nCONTENT: 26 Yusuf G¨ ur et al.\nshowed both low Recall (0.658) and low Precision (0.781), indicating it was\nboth missing true cases and incorrectly labeling other requirements as V6.\nModels struggle significantly with underrepresented categories, particularly\nV12 (File and Resources) and V6 (Stored Cryptography). This is evident in\nthe high number of false negatives, where the models fail to identify true\ninstances of these classes. For example, the baseline BERT model incorrectly\nclassified over half of the V12 requirements as negative (14 false negatives vs.\n13 true positives). Conversely, the matrices show that models often misclassify\nrequirements by incorrectly assigning them to other categories. The BERT\nBase model, for example, incorrectly labeled 92 different requirements as V10\n(Malicious Code) and 78 requirements as V5 (Malicious Input Handling).\n5.2 Experimental Setup for LLM Prompt Engineering\nWe adapted prompt engineering strategies from Ronanki (Ronanki et al.,\n2024), originally used for multi-class requirement classification. Two configu-\nrations were evaluated: zero-shot prompting (Appendix A), which relied solely\non task instructions and the input requirement, and few-shot prompting (Ap-\npendix B), which additionally incorporated labeled examples. The few-shot\nsetup used 30 training instances that were chosen to (i) capture multi-label\ncases where a single requirement spans multiple security categories, (ii) cover\nall 11 OWASP ASVS categories, and (iii) reflect requirements from different\nbusiness domains to preserve linguistic variation.\nPersona-based prompting was applied in both configurations by instruct-\ning the model to act as an experienced cybersecurity specialist. This alignment\nhelped reduce output variance by grounding responses in professional domain\nexpertise rather than general conversational patterns. The final prompt was\nobtained after five iterations. These iterations focused on three primary ar-\neas: (1) instruction clarity to prevent the model from defaulting to binary\nsecurity labels; (2) contextual grounding, by iteratively refining the OWASP\nASVS category definitions to ensure the model adhered to provided standards;\n(3) constraint formatting, to stabilize the PSV output structure for machine-\nreadability and ensure the inclusion of mandatory explanation fields. All ex-\nperiments were conducted with fixed parameters (Temperature = 0, Top-p =\n0.95) and repeated three times. The reported results are averaged to reduce\nthe impact of any minor, non-deterministic fluctuations in the models’ token\nselection during individual inferences.\nPrompt-based methods can be sensitive to phrasing, structure, and exam-\nple selection. To manage prompt sensitivity and contextual bias, we adopted a\nunified Persona–Context–Constraint strategy. A single prompt structure was\nused across all models, without model-specific tuning or iterative optimization\naimed at maximizing scores. The few-shot examples were chosen for representa-\ntiveness with respect to the OWASP ASVS taxonomy rather than performance\ngains.', 'CONTEXT: This chunk focuses on the comparative performance analysis of models across OWASP ASVS classes, highlighting challenges with rare categories (e.g., Stored Cryptography, File Handling) through heatmaps and gap analysis, while addressing dataset limitations and threats to validity related to linguistic diversity and generalizability.\n\nCONTENT: 30 Yusuf G¨ ur et al.\nFig. 2: Critical Difference (CD) diagram visualizing the statistical comparison\nof classifiers. The horizontal axis represents the average rank of each model\nacross all 11 OWASP ASVS classes (lower ranks indicate better performance).\nThe Critical Difference (CD = 10.46) represents the minimum difference in\naverage rank required for two models to be considered statistically significantly\ndifferent atp <0.05.\n5.6 Comparative Visualization Analysis\nTo complement the statistical rankings, we visualized the performance land-\nscape across all 20 model configurations.\nHeatmap Analysis: Figure 3 shows F1-scores by class. Requirements related\nto V2 (Authentication), V3 (Session Management), and V4 (Access Control)\nare handled well by nearly all models, with scores typically above 0.95. These\ncategories are common and appear consistently across training sources. In\ncontrast, V6 (Stored Cryptography) and V12 (Files and Resources) show low\nscores across the board, including for GPT-4o and Gemini 2.0 Pro. This pat-\ntern points to a systematic difficulty with sparse categories rather than a\nweakness of a particular model.\nGap Analysis: Figure 4 contrasts average performance on frequent and in-\nfrequent classes. Fine-tuned models such as SecureBERT and BERT-Multilingual\nperform better on rare classes, with F1-scores around 0.70–0.80. Zero-shot\nLLMs perform poorly in these cases, often below 0.30. Few-shot prompting\nimproves results for some models, such as Gemini 2.0, but the gap remains\nsubstantial.\n6 Threats to Validity\nOur dataset consists of 2,652 requirements derived from six Software Require-\nments Specification (SRS) documents. While the number of source documents\n\nImplicit Security Requirements Classification 31\nFig. 3:Comprehensive Performance Heatmap.F1-scores for all 20 eval-\nuated models across 11 OWASP ASVS classes. The visualization highlights\nthe “zone of failure” (red) for classes V6 and V12 across models, contrasting\nwith the high performance (green) on ubiquitous classes like V2 and V4.\nis limited, potential bias was mitigated by strategically selecting projects from\nsix distinct business domains: Construction, Legal, Education, Software, Meet-\ning Management, and Accommodation. This semantic diversity ensures that\nthe models are evaluated on a broad vocabulary and varied functional contexts\nrather than a single industry vertical. A specific consideration regarding the\nexternal validity of this study is that the ASRD is a Turkish-language corpus\nwhich allows for the evaluation of automated tools in linguistically diverse\nindustrial contexts.\nFurthermore, because security requirements (e.g., Authentication, Log-\nging) represent cross-cutting concerns that apply universally across software\nsystems, we believe the fundamental patterns captured in the ASRD are trans-\nferable to other industries. Our current evaluation used a stratified split across\nthe entire dataset, meaning requirements from all six source projects are\npresent in the training, validation, and test sets. However, we acknowledge\nthat this approach does not fully measure the models’ ability to generalize to\nentirely new projects with different authors or linguistic styles, as the test set\nis not completely “unseen” in terms of project context.', 'CONTEXT: This chunk provides a detailed example of a zero-shot prompt designed for classifying and analyzing project requirements based on the OWASP ASVS 4.0.3-tr categories, showcasing how large language models (LLMs) can be guided to perform security-focused requirement classification with explanations, aligning with the broader study\'s focus on automating security requirements analysis.\n\nCONTENT: Implicit Security Requirements Classification 43\nAdvanced Intelligent Systems 6(7):2300798\nWen SF, Katt B (2023) A quantitative security evaluation and analysis model\nfor web applications based on owasp application security verification stan-\ndard. Computers & Security 135:103532, DOI 10.1016/j.cose.2023.103532\nWhite J, Hays S, Fu Q, Spencer-Smith J, Schmidt DC (2024) Chatgpt prompt\npatterns for improving code quality, refactoring, requirements elicitation,\nand software design. In: Generative AI for Effective Software Development,\nSpringer, pp 71–108\nYe J, Yao Z, Huang Z, Pan L, Liu J, Bai Y, Xin A, Weichuan L, Che X,\nHou L, et al. (2025) How do transformers learn implicit reasoning? In: The\nThirty-ninth Annual Conference on Neural Information Processing Systems\nZadenoori MA, Dabrowski J, Alhoshan W, Zhao L, Ferrari A (2025) Large\nlanguage models (llms) for requirements engineering (re): A systematic lit-\nerature review. arXiv preprint arXiv:250911446\nZhou X, Li Y, Chen H (2025) The transformer architecture: Foundations and\nadvances. IEEE Transactions on Neural Networks and Learning Systems\nForthcoming\nZhu Y, Wang J, Liu K (2025) Emergent abilities of large language models:\nIn-context learning, instruction following, and reasoning. Nature Machine\nIntelligence In press\n Lukasiewicz K, Cyga´ nska S (2019) Security-oriented agile approach with\nagilesafe and owasp asvs. In: 2019 Federated Conference on Computer\nScience and Information Systems (FedCSIS), IEEE, pp 653–662, DOI\n10.23919/FedCSIS.2019.00012\n\nZERO SHOT PROMPT \n \nPersona: You are an experienced cybersecurity specialist with extensive knowledge of the OWASP ASVS \n4.0.3-tr and secure software development practices. You are meticulous, detail-oriented, and focused on \nidentifying potential security vulnerabilities and ensuring that applications meet the highest security \nstandards. \n \n********* \n \nTask: Classify and analyze project requirements based on their security and functional aspects from a \ncybersecurity perspective, aligning with the OWASP Application Security Verification Standard (ASVS) \n4.0.3-tr. Provide detailed explanations for all classifications and potential security concerns. \n \n********* \n \nInstructions: \n \n1.  You will receive a list of project requirements in PSV (Pipe Separated Values) format. \n2.  Each line in the input will represent a requirement in “Requirement Number|Requirement Description” \nformat and will contain: \n    * Requirement Number (integer) \n    * Requirement Description (string) \n3.  For each requirement, classify it into the categories defined in the table below, which are derived from \nthe OWASP ASVS 4.0.3-tr. \n4.  Output the classification in PSV format, including all columns from the table, AND an ""Explanation"" \ncolumn within a new line. \n5.  Use ""1"" if the requirement aligns with the ASVS category, and ""0"" if it does not. \n6.  Provide a concise explanation for each classification, justifying why you assigned a ""1"" or ""0"" to each \ncategory. \n7.  Ensure the output has the same number of rows as the input. \n8.  Use the Extra Information for Choosing Categories to aid classification. \n \n********* \n \nContext: \n \nThe categories below are based on the OWASP Application Security Verification Standard (ASVS) 4.0.3-tr. \nRefer to the ASVS documentation for detailed information on each category. The specific ASVS levels (L1, \nL2, L3) are not explicitly represented in the columns, but the categories themselves align with the ASVS \nrequirements. \n \n********* \n \nCategories (Aligned with OWASP ASVS 4.0.3-tr): \n \nV2, Kimlik Doğrulama (Identity Verification) - Covers requirements related to user authentication, \npassword management, and session handling (e.g., ASVS 2.0). \nV3, Oturum Yönetimi (Session Management) - Covers requirements related to secure session handling, \nincluding session timeouts, protection against session fixation, and secure cookie management (e.g., ASVS \n44 Yusuf G¨ ur et al.\nA Zero-shot Prompt', ""CONTEXT: **Dataset Filtering and Composition for ASRD**: This chunk details the rigorous filtering, randomization, and composition process used to create the Agency Security Requirements Dataset (ASRD), a Turkish-language corpus of 2,652 functional requirements across six business domains, annotated for security classification using the OWASP ASVS taxonomy. It also provides sample requirements to illustrate the dataset's diversity and focus on implicit security dependencies.\n\nCONTENT: 10 Yusuf G¨ ur et al.\n3. Filtering: The initial extraction yielded 3,264 items. To ensure the qual-\nity of the dataset, domain experts conducted a rigorous manual filtering\npass to remove 612 items. This exclusion process went beyond simple for-\nmatting artifacts (such as headers or tables of contents). We specifically\nremoved non-functional administrative clauses that addressed the vendor\nrather than the system (e.g., “The contractor must provide training man-\nuals”), project management constraints (e.g., “The project must be deliv-\nered by Q3”), and requirements with ambiguous scope that lacked clear\nverification criteria.\n4. Randomization: The remaining requirements were randomized and masked\nto mitigate potential annotation bias related to the document source or the\nsequence of requirements.\nThe resulting ASRD comprises 2,652 well-formed functional requirements\nacross six distinct business areas: Construction (1,202), Legal (177), Education\n(312), Software (199), Meeting (126), and Accommodation (636). A sample of\nthese requirements is provided in Table 2. The requirements included in this\nsample were selected based on three specific criteria to ensure a representative\noverview:\n–Samples were chosen to reflect the varied terminology and phrasing styles\nused across the six different business domains.\n–We prioritized requirements that demonstrate how standard functional de-\nscriptions conceal underlying security dependencies.\n–We ensured that the sample includes representative examples for all 11\nOWASP ASVS security classes used in the study to demonstrate the full\nscope of the classification task.\nIt is important to note that the ASRD is a Turkish-language corpus; all experi-\nments detailed in this study, including BERT fine-tuning and LLM prompting,\nwere conducted exclusively on the original text. English translations are pro-\nvided only for readability.\nTable 2: Sample Requirements from ASRD\nNo Original Requirements Translated Requirements\n1 Sistem, yetkili kullanıcının hukuk\ndosyası t¨ ur¨ u kayıt etmesini\nsa˘ glayacaktır.\nThe system shall allow authorized\nusers to record the legal file type.\n2 Sistem, kullanıcının kullanılmamı¸ s\nhukuk dosyası t¨ ur¨ un¨ u g¨ uncellemesini\nsa˘ glayacaktır.\nThe system shall allow the user to up-\ndate an unused legal file type.\n3 Sistem, kullanıcının kullanılmamı¸ s\nolan hukuk dosyası t¨ ur¨ un¨ u silmesini\nsa˘ glayacaktır.\nThe system shall allow the user to\ndelete an unused legal file type.\n4 Sistem, kullanıcının hukuk dosyası\nt¨ urlerini sorgulamasını ve listelemesini\nsa˘ glayacaktır.\nThe system shall allow the user to\nquery and list legal file types."", 'CONTEXT: This chunk delves into the challenges and findings related to classifying implicit security requirements in Software Requirements Specifications (SRS), highlighting the limitations of static few-shot prompting for minority classes, the comparative performance of fine-tuned BERT models and LLMs, and the practical implications for automating security compliance in industrial contexts.\n\nCONTENT: Implicit Security Requirements Classification 35\nexplicitly stated as a functional transaction, our domain experts classified this\nrequirement across nearly all OWASP ASVS categories. In an industrial con-\ntext, an “asset handover” implies a legally binding transfer of custody, ne-\ncessitating Authentication (V2) and Access Control (V4) to verify identity\nand authority, Data Protection (V8) for non-repudiation, and Secure Logging\n(V7) for auditability. A purely keyword-based or explicit extraction approach\nwould likely miss these dependencies, leaving the module vulnerable. Our find-\nings demonstrate that LLMs, when prompted with few-shot examples, can\neffectively mimic this expert reasoning, bridging the gap between functional\nspecifications and security compliance.\nWhile the ASRD dataset accurately reflects the natural distribution of se-\ncurity requirements in industrial projects, the performance drop on minority\nclasses reveals distinct limitations across the two model architectures eval-\nuated. For the LLMs, the limitation lies in the rigidity of Static Few-Shot\nPrompting. Our results indicate that providing a fixed set of 1–3 examples\n(Few-Shot) yielded inconsistent gains and, in cases like Llama-3 on V12, ac-\ntually degraded performance compared to Zero-Shot. This suggests that “im-\nplicit” requirements for minority classes are highly context-dependent. A fixed\nexample of V12 related to “file upload” does not help the LLM identify a V12\nrequirement related to “directory traversal.” Consequently, the models fail to\ngeneralize the concept of the minority class from static prompts, leading to\nthe low F1-scores (0.077–0.172) observed. This confirms that for rare, high-\nvariance security classes, static prompting is insufficient.\nOur study found that while fine-tuned BERT models provide strong results\n(up to 0.942 Macro-F1), few-shot prompting with a top-tier LLM like Gemini\n2.0 can achieve nearly identical performance (0.941 Macro-F1). This supports\nthe growing body of literature suggesting that in-context learning with large\nmodels can close the performance gap with task-specific fine-tuning, often\nwith significantly less labeled data. This shift from a traditional “pre-train,\nfine-tune” to a “pre-train, prompt, and predict” paradigm, as surveyed by\nLiu et al. (Liu et al., 2023a), offers a more agile and resource-efficient path\nfor deploying NLP solutions in specialized industrial contexts. Our results\nprovide a concrete example of this trend within the SRE domain and align\nwith findings given by Karlsson et.al (Karlsson et al., 2025).\nThese findings have immediate implications for the software industry, par-\nticularly in implementing “Security by Design” and “Shift-Left” strategies. In\ndiverse development environments—ranging from Agile teams to regulated sec-\ntors like healthcare or defense—developers often lack deep security expertise.\nThey focus on writing functional requirements (SRS) to meet business needs,\noften assuming security will be “added on” later. By automating the classi-\nfication of implicit security requirements using the method proposed in this\nstudy, organizations can instantly map functional specs to actionable OWASP\nASVS controls before a single line of code is written. Rather than replacing\nhuman oversight, this approach functions as a human-in-the-loop productiv-\nity multiplier. It serves as a sophisticated pre-screening tool that drastically\nspeeds up the initial analysis phase, allowing scarce cybersecurity experts to']","The OWASP ASVS framework is preferred over STRIDE for this specific task because it provides a structured and detailed taxonomy (V2–V13) for classifying security requirements, which aligns well with the goal of ensuring consistency and accuracy in annotating implicit security requirements. The OWASP ASVS categories are explicitly designed to cover a wide range of security concerns, such as authentication, access control, data protection, and secure file handling, making it highly suitable for tasks that require detailed classification and mapping of functional requirements to actionable security controls.

In contrast, STRIDE is a threat modeling framework that focuses on identifying and categorizing threats (e.g., Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege) rather than providing a granular taxonomy for security requirements. For tasks like annotating and classifying implicit security requirements in Software Requirements Specifications (SRS), OWASP ASVS offers a more domain-specific and actionable framework.

Additionally, the iterative development of annotation guidelines based on OWASP ASVS categories ensures clarity and consistency, which is critical for creating a high-quality, annotated dataset like the ASRD. This structured approach allows for better alignment with security standards and facilitates automation in security compliance tasks, as demonstrated in the study.

(Source: temp/b31390a6-3d40-4aa2-b634-17feb052e723_template.pdf)","STRIDE operates at a high level of abstraction useful for threat modeling but lacks the granularity needed for requirement classification, whereas ASVS provides a detailed, developer-centric taxonomy[cite: 129, 132].",0.8849999999823,0.7142857142857143,0.88766852048379
What is the 'Human-AI Collaborative Multi-Agent Framework' proposed for future work?,"['CONTEXT: **Proposed Future Directions for Addressing Dataset Sparsity and Improving Classification Accuracy:** This chunk outlines three key algorithmic advancements—Dynamic Few-Shot Selection with RAG, a Human-AI Collaborative Framework, and Synthetic Data Injection—to tackle challenges like class imbalance and rare security requirements, while also suggesting improved evaluation methods for model generalizability.\n\nCONTENT: Implicit Security Requirements Classification 37\ntime. This approach effectively substitutes the need for dense in-context ex-\namples with explicit external standards, ensuring accurate classification for\nrare security events where traditional fine-tuning or few-shot prompting fails.\nConsequently, this study suggests that future NLP research in SRE cannot\nrely solely on organic dataset expansion. To overcome this inherent sparsity,\nwe propose as future work three targeted algorithmic advancements:\n1. Dynamic Few-Shot Selection with Retrieval-Augmented Generation (RAG):\nSystems might dynamically retrieve the explicit definitions and verifica-\ntion criteria from standards like OWASP ASVS to ground their classifi-\ncation of rare requirements. Future work should implement RAG for Dy-\nnamic Prompting in agentic approach. Instead of fixed examples, a retriever\nshould select thekmost semantically similar valid requirements from the\ntraining set to serve as in-context examples for the specific query. This\nensures the LLM is grounded with relevant architectural patterns (e.g.,\n“encryption” vs. “hashing”) rather than generic class examples.\n2. Human-AI Collaborative Multi-Agent Framework: A promising direction\nfor future work is a Human-in-the-Loop multi-agent framework that com-\nbines automated classification with targeted expert oversight for ambigu-\nous or high-risk cases. In this setup, a retrieval-augmented agent would\ndynamically select the most relevant annotated requirements using seman-\ntic similarity, replacing static few-shot examples. A second agent would\nassess classification confidence and consistency, flagging unclear cases and\nrequesting focused clarifications from a human expert when needed. A su-\npervisor agent would then consolidate these inputs and ensure that the\nfinal labels remain consistent with the OWASP ASVS taxonomy and the\nbroader project context. In parallel, an optimization agent would learn from\nhuman interventions to refine retrieval strategies and prompting over time,\nreducing manual effort while improving accuracy on challenging security\nrequirements.\n3. Synthetic Data Injection (for Supervised Models): To address the sparsity\nhindering BERT-based fine-tuning, we recommend a Teacher-Student Data\nGeneration loop. High-reasoning LLMs (e.g., GPT-4o, Gemini 2.5 Pro)\nshould be used to generate diverse synthetic requirements that implicitly\ntrigger V6/V12 classifications. These synthetic samples can populate the\ntraining set, allowing smaller supervised models (BERT) to learn robust\ndecision boundaries without requiring expensive manual annotation.\nAdditionally, future efforts should include a more stringent evaluation of\nmodel generalizability using a leave-one-project-out cross-validation approach.\nThis method involves training a model on requirements from five of the projects\nand testing it on the single, held-out project. Repeating this process for all six\nprojects would provide a much clearer and more realistic assessment of how\nwell these models perform on entirely new, unseen SRS documents, which is\na significant measure for real-world applicability.\nFurthermore, to bridge the gap between our current findings and the global\nresearch community, we plan to undertake a professional translation of the']","The 'Human-AI Collaborative Multi-Agent Framework' proposed for future work is a system designed to combine automated classification with targeted expert oversight, particularly for ambiguous or high-risk cases. This framework involves multiple agents working collaboratively:

1. **Retrieval-Augmented Agent**: Dynamically selects the most relevant annotated requirements using semantic similarity, replacing static few-shot examples.
2. **Assessment Agent**: Evaluates classification confidence and consistency, flags unclear cases, and requests clarifications from human experts when necessary.
3. **Supervisor Agent**: Consolidates inputs from the other agents and ensures that the final labels align with the OWASP ASVS taxonomy and the broader project context.
4. **Optimization Agent**: Learns from human interventions to refine retrieval strategies and prompting over time, reducing manual effort while improving accuracy on challenging security requirements.

This framework aims to enhance classification accuracy by integrating human expertise into the process, ensuring consistency and reliability, especially for rare or complex security requirements. It also emphasizes iterative learning to optimize the system's performance over time.

**Source**: temp/b31390a6-3d40-4aa2-b634-17feb052e723_template.pdf","It is a framework combining automated classification with expert oversight, where agents retrieve relevant requirements, assess confidence, and flag unclear cases for human clarification[cite: 729, 730].",0.9999999999,1.0,0.9919826771587418
