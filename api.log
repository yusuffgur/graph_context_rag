2026-01-03 12:35:17,480 - src.modules.llm - INFO - Configuring ResilientLLM with Provider: AZURE
INFO:     Started server process [35006]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     127.0.0.1:62174 - "GET /settings HTTP/1.1" 200 OK
INFO:     127.0.0.1:62175 - "GET /documents HTTP/1.1" 200 OK
INFO:httpx:HTTP Request: POST http://localhost:6333/collections/federated_docs/points/delete?wait=true "HTTP/1.1 200 OK"
2026-01-03 12:35:32,541 - API - INFO - ‚ôªÔ∏è System Reset Triggered
INFO:API:‚ôªÔ∏è System Reset Triggered
INFO:     127.0.0.1:62181 - "POST /reset HTTP/1.1" 200 OK
INFO:     127.0.0.1:62185 - "GET /documents HTTP/1.1" 200 OK
INFO:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
INFO:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
INFO:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
INFO:     127.0.0.1:62191 - "POST /upload HTTP/1.1" 200 OK
INFO:     127.0.0.1:62194 - "GET /stream/0b21c8e8-6086-46c8-a197-e548c2ca8d64 HTTP/1.1" 200 OK
INFO:     127.0.0.1:62211 - "GET /stream/0b21c8e8-6086-46c8-a197-e548c2ca8d64 HTTP/1.1" 200 OK
INFO:httpx:HTTP Request: POST https://genai-core-dev.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-05-15 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:6333/collections/federated_docs/points/query "HTTP/1.1 200 OK"
2026-01-03 12:46:25,041 - src.modules.llm - INFO - Using Cloud Fallback for Local Task (Provider: azure)
INFO:src.modules.llm:Using Cloud Fallback for Local Task (Provider: azure)
INFO:httpx:HTTP Request: POST https://genai-core-dev.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://genai-core-dev.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 "HTTP/1.1 200 OK"
2026-01-03 12:46:29,650 - API - INFO - üêõ Debug [Query='Which statistical tests used to evaluate LLMs performance?']: {
  "original_query": "Which statistical tests used to evaluate LLMs performance?",
  "refined_query": "Which statistical tests used to evaluate LLMs performance?",
  "final_prompt": "\n            You are a helpful assistant. \n            Answer the user's query mostly based on the provided Context.\n            \n            - If the Context mentions the term, summarize its usage, examples, or categories found.\n            - If the answer is NOT in the Context, say \"I cannot find the answer in the provided documents.\"\n            - Cite the source filename if possible.\n\n            Original Query: Which statistical tests used to evaluate LLMs performance?\n            Refined Intent: Which statistical tests used to evaluate LLMs performance?\n\n            [Vector Knowledge]\n            - CONTEXT: **Statistical Evaluation of Model Performance**: This chunk discusses the use of the Wilcoxon Signed-Rank Test as a robust, non-parametric method for comparing the performance of classifiers, particularly addressing challenges like outliers and extreme performance drops in minority security requirement classes (e.g., V6 and V12).\n\nCONTENT: isons, we employed the Wilcoxon Signed-Rank Test. Dem\u02c7 sar recommends the\nWilcoxon test as a \u201csafe and robust\u201d non-parametric alternative to the paired\nt-test for comparing two classifiers. Unlike the t-test, which assumes commen-\nsurability of differences and normality, the Wilcoxon test compares the ranks\nof the differences. This ensures that outliers (such as the extreme performance\ndrops observed in minority classes like V6 and V12) do not disproportionately\nskew the results. (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Comparison of Model Performance and Statistical Analysis of Results**: This chunk focuses on the statistical evaluation of model performance, comparing fine-tuned models like SecureBERT with few-shot and zero-shot LLM approaches (e.g., Gemini 2.0, GPT-4o), using metrics such as the Wilcoxon Signed-Rank Test to assess significant differences in their effectiveness for security requirements classification.\n\nCONTENT: a measurable statistical advantage.\n\u2013Fine-Tuning vs. Few-Shot State-of-the-Art: The pairwise comparison be-\ntween the best fine-tuned model (SecureBERT) and the best LLM ap-\nproach (Gemini 2.0 Few-Shot) yielded ap-value of 0.102. This value is not\nstatistically significant at the\u03b1= 0.05 level, supporting our finding that\nfew-shot prompting with state-of-the-art LLMs can effectively match the\nperformance of specialized, fine-tuned BERT models in this domain.\nTable 8: Significant Pairwise Differences (Wilcoxon Signed-Rank Test)\nModel A Model B Statistic p-value\nNaive Baseline SecureBERT 0.0 0.00098\nNaive Baseline BERT Turkish 0.0 0.00098\nNaive Baseline Gemini 2.0 (Few-Shot) 0.0 0.00098\nNaive Baseline Gemini 2.0 Pro (Few-Shot) 0.0 0.00098\nSecureBERT GPT-4o Mini (Few-Shot) 0.0 0.00098\nSecureBERT GPT-4o Mini (Zero-Shot) 0.0 0.00098\nSecureBERT GPT-4o (Zero-Shot) 0.0 0.00098\nSecureBERT GPT-4o (Few-Shot) 0.0 0.00098\nSecureBERT Gemini 2.0 (Zero-Shot) 25.0 0.501 (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Performance Comparison of LLMs and Fine-Tuned Models**: This chunk discusses the performance of LLMs, particularly Gemini 2.0, in few-shot and zero-shot configurations compared to fine-tuned BERT models, highlighting statistical evaluations of classifier performance differences in the context of security requirements classification.\n\nCONTENT: The top-performing LLM was Gemini 2.0, which in its few-shot configu-\nration reached a Macro-F1 score of 0.941. This score is on par with the best\nfine-tuned BERT model, indicating that few-shot prompting with a powerful\nLLM can match the performance of a specialized, fine-tuned model. However,\nmost other LLMs did not reach this level. For example, GPT-4o\u2019s few-shot\nMacro-F1 score was only 0.814. In zero-shot tests, where no examples were\nprovided, the best LLM (Gemini 2.0) scored a Macro-F1 of 0.890. This score\nremains below the weakest fine-tuned BERT model, with fine-tuned models\nperforming best, followed by few-shot LLMs and then zero-shot LLMs.\n5.5 Statistical Evaluation of Classifiers Performance\nWe also examined whether the observed models\u2019 performance differences were\nstatistically significant. Following the comparison framework of (Dem\u02c7 sar, 2006),\nwe compared the distributions of classifier performance scores. The 11 OWASP (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Evaluation Metrics and Classification Performance Comparison**: This chunk discusses the evaluation metrics used to assess the performance of fine-tuned BERT models and prompt-based LLM approaches, ensuring a fair comparison across security classification tasks by utilizing shared test sets and comprehensive F1-score variants.\n\nCONTENT: effective fine-tuning.\nAlthough zero-shot LLMs do not require data partitioning and few-shot\nsettings rely on only a small number of examples, the shared test set was\nretained to ensure a fair comparison between fine-tuned model variants and\nprompt-based LLM approaches.\nWe used a comprehensive suite of metrics for classification performance\nevaluation. We report Precision, which measures the accuracy of positive pre-\ndictions, and Recall, which measures the model\u2019s ability to identify all rele-\nvant instances. The F1-score, the harmonic mean of precision and recall, is\nused to provide a single measure balancing this trade-off. To assess overall\nperformance across all 11 security classes, we calculated the micro-averaged\nF1, which reflects aggregate accuracy, the macro-averaged F1, which assesses\nperformance by treating all classes equally regardless of their frequency, and\nthe weighted-averaged F1, which accounts for class support.\n4.2 Classification using BERT based Models (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Statistical Analysis of Classifier Performance**: This chunk details the statistical evaluation of classifier performance across OWASP ASVS security classes, using the Friedman test to confirm significant differences among models, followed by pairwise Wilcoxon Signed-Rank tests to identify specific model performance disparities.\n\nCONTENT: we compared the distributions of classifier performance scores. The 11 OWASP\nASVS security classes were treated as independent domains (stratified subsam-\nples from the main data set). The performance of the 20 evaluated classifiers\nwas assessed using the Friedman test, followed by pairwise Wilcoxon signed-\nrank tests.\nThe null hypothesis states that all classifiers perform equivalently and their\nrank distributions are identical. In our analysis, the Friedman test yielded a\nstatistic of 188.103 with ap-value of 2.38\u00d710 \u221228. This result rejects the null\nhypothesis (p <0.05), confirming that the observed performance differences\nacross the models are non-random and statistically significant.\nFollowing the rejection of the null hypothesis, we proceeded to identify the\nspecific pairs of models that differ significantly. For these pairwise compar-\nisons, we employed the Wilcoxon Signed-Rank Test. Dem\u02c7 sar recommends the (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n            \n            [Graph Knowledge]\n            Focused Entity: \n            Relationships: No specific graph context.\n            \n            Your Answer:\n            ",
  "vector_candidates": 20,
  "reranked_candidates": 20,
  "llm_provider": "azure"
}
INFO:API:üêõ Debug [Query='Which statistical tests used to evaluate LLMs performance?']: {
  "original_query": "Which statistical tests used to evaluate LLMs performance?",
  "refined_query": "Which statistical tests used to evaluate LLMs performance?",
  "final_prompt": "\n            You are a helpful assistant. \n            Answer the user's query mostly based on the provided Context.\n            \n            - If the Context mentions the term, summarize its usage, examples, or categories found.\n            - If the answer is NOT in the Context, say \"I cannot find the answer in the provided documents.\"\n            - Cite the source filename if possible.\n\n            Original Query: Which statistical tests used to evaluate LLMs performance?\n            Refined Intent: Which statistical tests used to evaluate LLMs performance?\n\n            [Vector Knowledge]\n            - CONTEXT: **Statistical Evaluation of Model Performance**: This chunk discusses the use of the Wilcoxon Signed-Rank Test as a robust, non-parametric method for comparing the performance of classifiers, particularly addressing challenges like outliers and extreme performance drops in minority security requirement classes (e.g., V6 and V12).\n\nCONTENT: isons, we employed the Wilcoxon Signed-Rank Test. Dem\u02c7 sar recommends the\nWilcoxon test as a \u201csafe and robust\u201d non-parametric alternative to the paired\nt-test for comparing two classifiers. Unlike the t-test, which assumes commen-\nsurability of differences and normality, the Wilcoxon test compares the ranks\nof the differences. This ensures that outliers (such as the extreme performance\ndrops observed in minority classes like V6 and V12) do not disproportionately\nskew the results. (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Comparison of Model Performance and Statistical Analysis of Results**: This chunk focuses on the statistical evaluation of model performance, comparing fine-tuned models like SecureBERT with few-shot and zero-shot LLM approaches (e.g., Gemini 2.0, GPT-4o), using metrics such as the Wilcoxon Signed-Rank Test to assess significant differences in their effectiveness for security requirements classification.\n\nCONTENT: a measurable statistical advantage.\n\u2013Fine-Tuning vs. Few-Shot State-of-the-Art: The pairwise comparison be-\ntween the best fine-tuned model (SecureBERT) and the best LLM ap-\nproach (Gemini 2.0 Few-Shot) yielded ap-value of 0.102. This value is not\nstatistically significant at the\u03b1= 0.05 level, supporting our finding that\nfew-shot prompting with state-of-the-art LLMs can effectively match the\nperformance of specialized, fine-tuned BERT models in this domain.\nTable 8: Significant Pairwise Differences (Wilcoxon Signed-Rank Test)\nModel A Model B Statistic p-value\nNaive Baseline SecureBERT 0.0 0.00098\nNaive Baseline BERT Turkish 0.0 0.00098\nNaive Baseline Gemini 2.0 (Few-Shot) 0.0 0.00098\nNaive Baseline Gemini 2.0 Pro (Few-Shot) 0.0 0.00098\nSecureBERT GPT-4o Mini (Few-Shot) 0.0 0.00098\nSecureBERT GPT-4o Mini (Zero-Shot) 0.0 0.00098\nSecureBERT GPT-4o (Zero-Shot) 0.0 0.00098\nSecureBERT GPT-4o (Few-Shot) 0.0 0.00098\nSecureBERT Gemini 2.0 (Zero-Shot) 25.0 0.501 (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Performance Comparison of LLMs and Fine-Tuned Models**: This chunk discusses the performance of LLMs, particularly Gemini 2.0, in few-shot and zero-shot configurations compared to fine-tuned BERT models, highlighting statistical evaluations of classifier performance differences in the context of security requirements classification.\n\nCONTENT: The top-performing LLM was Gemini 2.0, which in its few-shot configu-\nration reached a Macro-F1 score of 0.941. This score is on par with the best\nfine-tuned BERT model, indicating that few-shot prompting with a powerful\nLLM can match the performance of a specialized, fine-tuned model. However,\nmost other LLMs did not reach this level. For example, GPT-4o\u2019s few-shot\nMacro-F1 score was only 0.814. In zero-shot tests, where no examples were\nprovided, the best LLM (Gemini 2.0) scored a Macro-F1 of 0.890. This score\nremains below the weakest fine-tuned BERT model, with fine-tuned models\nperforming best, followed by few-shot LLMs and then zero-shot LLMs.\n5.5 Statistical Evaluation of Classifiers Performance\nWe also examined whether the observed models\u2019 performance differences were\nstatistically significant. Following the comparison framework of (Dem\u02c7 sar, 2006),\nwe compared the distributions of classifier performance scores. The 11 OWASP (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Evaluation Metrics and Classification Performance Comparison**: This chunk discusses the evaluation metrics used to assess the performance of fine-tuned BERT models and prompt-based LLM approaches, ensuring a fair comparison across security classification tasks by utilizing shared test sets and comprehensive F1-score variants.\n\nCONTENT: effective fine-tuning.\nAlthough zero-shot LLMs do not require data partitioning and few-shot\nsettings rely on only a small number of examples, the shared test set was\nretained to ensure a fair comparison between fine-tuned model variants and\nprompt-based LLM approaches.\nWe used a comprehensive suite of metrics for classification performance\nevaluation. We report Precision, which measures the accuracy of positive pre-\ndictions, and Recall, which measures the model\u2019s ability to identify all rele-\nvant instances. The F1-score, the harmonic mean of precision and recall, is\nused to provide a single measure balancing this trade-off. To assess overall\nperformance across all 11 security classes, we calculated the micro-averaged\nF1, which reflects aggregate accuracy, the macro-averaged F1, which assesses\nperformance by treating all classes equally regardless of their frequency, and\nthe weighted-averaged F1, which accounts for class support.\n4.2 Classification using BERT based Models (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Statistical Analysis of Classifier Performance**: This chunk details the statistical evaluation of classifier performance across OWASP ASVS security classes, using the Friedman test to confirm significant differences among models, followed by pairwise Wilcoxon Signed-Rank tests to identify specific model performance disparities.\n\nCONTENT: we compared the distributions of classifier performance scores. The 11 OWASP\nASVS security classes were treated as independent domains (stratified subsam-\nples from the main data set). The performance of the 20 evaluated classifiers\nwas assessed using the Friedman test, followed by pairwise Wilcoxon signed-\nrank tests.\nThe null hypothesis states that all classifiers perform equivalently and their\nrank distributions are identical. In our analysis, the Friedman test yielded a\nstatistic of 188.103 with ap-value of 2.38\u00d710 \u221228. This result rejects the null\nhypothesis (p <0.05), confirming that the observed performance differences\nacross the models are non-random and statistically significant.\nFollowing the rejection of the null hypothesis, we proceeded to identify the\nspecific pairs of models that differ significantly. For these pairwise compar-\nisons, we employed the Wilcoxon Signed-Rank Test. Dem\u02c7 sar recommends the (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n            \n            [Graph Knowledge]\n            Focused Entity: \n            Relationships: No specific graph context.\n            \n            Your Answer:\n            ",
  "vector_candidates": 20,
  "reranked_candidates": 20,
  "llm_provider": "azure"
}
INFO:     127.0.0.1:62539 - "GET /query?q=Which+statistical+tests+used+to+evaluate+LLMs+performance%3F HTTP/1.1" 200 OK
INFO:     127.0.0.1:62543 - "GET /files/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf HTTP/1.1" 200 OK
INFO:httpx:HTTP Request: POST https://genai-core-dev.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-05-15 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:6333/collections/federated_docs/points/query "HTTP/1.1 200 OK"
2026-01-03 12:47:44,321 - src.modules.llm - INFO - Using Cloud Fallback for Local Task (Provider: azure)
INFO:src.modules.llm:Using Cloud Fallback for Local Task (Provider: azure)
INFO:httpx:HTTP Request: POST https://genai-core-dev.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://genai-core-dev.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 "HTTP/1.1 200 OK"
2026-01-03 12:47:49,418 - API - INFO - üêõ Debug [Query='Which OWASP classed are problematic and can you give performance summary of models on these classes?']: {
  "original_query": "Which OWASP classed are problematic and can you give performance summary of models on these classes?",
  "refined_query": "Which OWASP classed are problematic and can you give performance summary of models on these classes?",
  "final_prompt": "\n            You are a helpful assistant. \n            Answer the user's query mostly based on the provided Context.\n            \n            - If the Context mentions the term, summarize its usage, examples, or categories found.\n            - If the answer is NOT in the Context, say \"I cannot find the answer in the provided documents.\"\n            - Cite the source filename if possible.\n\n            Original Query: Which OWASP classed are problematic and can you give performance summary of models on these classes?\n            Refined Intent: Which OWASP classed are problematic and can you give performance summary of models on these classes?\n\n            [Vector Knowledge]\n            - CONTEXT: **Performance Analysis and Dataset Diversity in Security Classification**: This chunk discusses the performance heatmap of models across OWASP ASVS classes, highlighting challenges with rare categories (V6 and V12) and the strategic selection of diverse business domains to ensure semantic and contextual variety in evaluating the Turkish-language ASRD dataset.\n\nCONTENT: Implicit Security Requirements Classification 31\nFig. 3:Comprehensive Performance Heatmap.F1-scores for all 20 eval-\nuated models across 11 OWASP ASVS classes. The visualization highlights\nthe \u201czone of failure\u201d (red) for classes V6 and V12 across models, contrasting\nwith the high performance (green) on ubiquitous classes like V2 and V4.\nis limited, potential bias was mitigated by strategically selecting projects from\nsix distinct business domains: Construction, Legal, Education, Software, Meet-\ning Management, and Accommodation. This semantic diversity ensures that\nthe models are evaluated on a broad vocabulary and varied functional contexts\nrather than a single industry vertical. A specific consideration regarding the\nexternal validity of this study is that the ASRD is a Turkish-language corpus\nwhich allows for the evaluation of automated tools in linguistically diverse\nindustrial contexts.\nFurthermore, because security requirements (e.g., Authentication, Log- (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Comparative Model Performance and Visualization Analysis**: This section presents a statistical comparison of classifiers using a Critical Difference (CD) diagram and heatmap analysis, highlighting model performance across OWASP ASVS classes, with a focus on strengths in common categories (e.g., Authentication, Access Control) and challenges in rare categories (e.g., Stored Cryptography, Files and Resources).\n\nCONTENT: 30 Yusuf G\u00a8 ur et al.\nFig. 2: Critical Difference (CD) diagram visualizing the statistical comparison\nof classifiers. The horizontal axis represents the average rank of each model\nacross all 11 OWASP ASVS classes (lower ranks indicate better performance).\nThe Critical Difference (CD = 10.46) represents the minimum difference in\naverage rank required for two models to be considered statistically significantly\ndifferent atp <0.05.\n5.6 Comparative Visualization Analysis\nTo complement the statistical rankings, we visualized the performance land-\nscape across all 20 model configurations.\nHeatmap Analysis: Figure 3 shows F1-scores by class. Requirements related\nto V2 (Authentication), V3 (Session Management), and V4 (Access Control)\nare handled well by nearly all models, with scores typically above 0.95. These\ncategories are common and appear consistently across training sources. In\ncontrast, V6 (Stored Cryptography) and V12 (Files and Resources) show low (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: This chunk provides a detailed breakdown of the OWASP ASVS verification categories (V2\u2013V13) and presents performance metrics (Precision, Recall, F1-scores, and Support) for the classification of security requirements across these categories, highlighting challenges with rare classes like Stored Cryptography (V6) and File and Resources (V12).\n\nCONTENT: V2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements\nV10, Malicious Code Verification Requirements\nV12, File and Resources Verification Requirements\nV13, API and Web Service Verification Requirements\nPRECISION 0.943 0.922 0.967 0.905 0.214 0.903 0.918 0.906 0.876 0.250 0.880RECALL 0.950 0.932 0.974 0.943 0.231 0.907 0.927 0.914 0.920 0.250 0.899F1 0.947 0.927 0.970 0.924 0.222 0.905 0.922 0.910 0.898 0.250 0.890SUPPORT 382 381 385 314 38 389 384 380 299 27 317TOTAL 398 398 398 398 398 398 398 398 398 398 398 (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Performance Metrics of BERT Models for Security Requirement Classification**  \nThis chunk provides detailed performance metrics (Precision, Recall, F1-scores, and Support) for BERT-based models, specifically evaluating their effectiveness in classifying security requirements across OWASP ASVS categories, highlighting strengths and challenges in handling rare security classes.\n\nCONTENT: bert-base-turkish-cased\nV2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements\nV10, Malicious Code Verification Requirements\nV12, File and Resources Verification Requirements\nV13, API and Web Service Verification Requirements\nPRECISION 0.974 0.974 0.980 0.975 0.756 0.985 0.982 0.974 0.980 0.800 0.972RECALL 0.997 0.997 1.000 0.994 0.816 0.997 1.000 1.000 0.977 0.593 0.994F1 0.985 0.985 0.990 0.984 0.785 0.991 0.991 0.987 0.978 0.681 0.983SUPPORT 382 381 385 314 38 389 384 380 299 27 317TOTAL 398 398 398 398 398 398 398 398 398 398 398\nbert-base-uncased\nV2, Authentication Verification Requirements (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: This chunk provides performance metrics (Precision, Recall, F1-scores, and Support) for the Gpt_4o_mini model using few-shot learning across various OWASP ASVS security verification categories, highlighting its effectiveness and challenges in classifying specific security requirements, particularly in underrepresented classes like Stored Cryptography (V6) and File and Resources (V12).\n\nCONTENT: Gpt_4o_mini_Few_Shot\nV2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements\nV10, Malicious Code Verification Requirements\nV12, File and Resources Verification Requirements\nV13, API and Web Service Verification Requirements\nPRECISION 0.969 0.956 0.964 0.821 0.128 0.959 0.871 0.862 0.768 0.179 0.813RECALL 0.976 0.966 0.969 0.829 0.128 0.964 0.878 0.866 0.773 0.179 0.821F1 0.973 0.961 0.966 0.825 0.128 0.962 0.874 0.864 0.771 0.179 0.817SUPPORT 382 381 385 314 38 389 384 380 299 27 317TOTAL 398 398 398 398 398 398 398 398 398 398 398 (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n            \n            [Graph Knowledge]\n            Focused Entity: OWASP\n            Relationships: [[b'type(r)', b'm.name'], [], [b'Cached execution: 0', b'Query internal execution time: 0.473167 milliseconds']]\n            \n            Your Answer:\n            ",
  "vector_candidates": 20,
  "reranked_candidates": 20,
  "llm_provider": "azure"
}
INFO:API:üêõ Debug [Query='Which OWASP classed are problematic and can you give performance summary of models on these classes?']: {
  "original_query": "Which OWASP classed are problematic and can you give performance summary of models on these classes?",
  "refined_query": "Which OWASP classed are problematic and can you give performance summary of models on these classes?",
  "final_prompt": "\n            You are a helpful assistant. \n            Answer the user's query mostly based on the provided Context.\n            \n            - If the Context mentions the term, summarize its usage, examples, or categories found.\n            - If the answer is NOT in the Context, say \"I cannot find the answer in the provided documents.\"\n            - Cite the source filename if possible.\n\n            Original Query: Which OWASP classed are problematic and can you give performance summary of models on these classes?\n            Refined Intent: Which OWASP classed are problematic and can you give performance summary of models on these classes?\n\n            [Vector Knowledge]\n            - CONTEXT: **Performance Analysis and Dataset Diversity in Security Classification**: This chunk discusses the performance heatmap of models across OWASP ASVS classes, highlighting challenges with rare categories (V6 and V12) and the strategic selection of diverse business domains to ensure semantic and contextual variety in evaluating the Turkish-language ASRD dataset.\n\nCONTENT: Implicit Security Requirements Classification 31\nFig. 3:Comprehensive Performance Heatmap.F1-scores for all 20 eval-\nuated models across 11 OWASP ASVS classes. The visualization highlights\nthe \u201czone of failure\u201d (red) for classes V6 and V12 across models, contrasting\nwith the high performance (green) on ubiquitous classes like V2 and V4.\nis limited, potential bias was mitigated by strategically selecting projects from\nsix distinct business domains: Construction, Legal, Education, Software, Meet-\ning Management, and Accommodation. This semantic diversity ensures that\nthe models are evaluated on a broad vocabulary and varied functional contexts\nrather than a single industry vertical. A specific consideration regarding the\nexternal validity of this study is that the ASRD is a Turkish-language corpus\nwhich allows for the evaluation of automated tools in linguistically diverse\nindustrial contexts.\nFurthermore, because security requirements (e.g., Authentication, Log- (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Comparative Model Performance and Visualization Analysis**: This section presents a statistical comparison of classifiers using a Critical Difference (CD) diagram and heatmap analysis, highlighting model performance across OWASP ASVS classes, with a focus on strengths in common categories (e.g., Authentication, Access Control) and challenges in rare categories (e.g., Stored Cryptography, Files and Resources).\n\nCONTENT: 30 Yusuf G\u00a8 ur et al.\nFig. 2: Critical Difference (CD) diagram visualizing the statistical comparison\nof classifiers. The horizontal axis represents the average rank of each model\nacross all 11 OWASP ASVS classes (lower ranks indicate better performance).\nThe Critical Difference (CD = 10.46) represents the minimum difference in\naverage rank required for two models to be considered statistically significantly\ndifferent atp <0.05.\n5.6 Comparative Visualization Analysis\nTo complement the statistical rankings, we visualized the performance land-\nscape across all 20 model configurations.\nHeatmap Analysis: Figure 3 shows F1-scores by class. Requirements related\nto V2 (Authentication), V3 (Session Management), and V4 (Access Control)\nare handled well by nearly all models, with scores typically above 0.95. These\ncategories are common and appear consistently across training sources. In\ncontrast, V6 (Stored Cryptography) and V12 (Files and Resources) show low (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: This chunk provides a detailed breakdown of the OWASP ASVS verification categories (V2\u2013V13) and presents performance metrics (Precision, Recall, F1-scores, and Support) for the classification of security requirements across these categories, highlighting challenges with rare classes like Stored Cryptography (V6) and File and Resources (V12).\n\nCONTENT: V2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements\nV10, Malicious Code Verification Requirements\nV12, File and Resources Verification Requirements\nV13, API and Web Service Verification Requirements\nPRECISION 0.943 0.922 0.967 0.905 0.214 0.903 0.918 0.906 0.876 0.250 0.880RECALL 0.950 0.932 0.974 0.943 0.231 0.907 0.927 0.914 0.920 0.250 0.899F1 0.947 0.927 0.970 0.924 0.222 0.905 0.922 0.910 0.898 0.250 0.890SUPPORT 382 381 385 314 38 389 384 380 299 27 317TOTAL 398 398 398 398 398 398 398 398 398 398 398 (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Performance Metrics of BERT Models for Security Requirement Classification**  \nThis chunk provides detailed performance metrics (Precision, Recall, F1-scores, and Support) for BERT-based models, specifically evaluating their effectiveness in classifying security requirements across OWASP ASVS categories, highlighting strengths and challenges in handling rare security classes.\n\nCONTENT: bert-base-turkish-cased\nV2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements\nV10, Malicious Code Verification Requirements\nV12, File and Resources Verification Requirements\nV13, API and Web Service Verification Requirements\nPRECISION 0.974 0.974 0.980 0.975 0.756 0.985 0.982 0.974 0.980 0.800 0.972RECALL 0.997 0.997 1.000 0.994 0.816 0.997 1.000 1.000 0.977 0.593 0.994F1 0.985 0.985 0.990 0.984 0.785 0.991 0.991 0.987 0.978 0.681 0.983SUPPORT 382 381 385 314 38 389 384 380 299 27 317TOTAL 398 398 398 398 398 398 398 398 398 398 398\nbert-base-uncased\nV2, Authentication Verification Requirements (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: This chunk provides performance metrics (Precision, Recall, F1-scores, and Support) for the Gpt_4o_mini model using few-shot learning across various OWASP ASVS security verification categories, highlighting its effectiveness and challenges in classifying specific security requirements, particularly in underrepresented classes like Stored Cryptography (V6) and File and Resources (V12).\n\nCONTENT: Gpt_4o_mini_Few_Shot\nV2, Authentication Verification Requirements\nV3, Session Management Verification Requirements\nV4, Access Control Verification Requirements\nV5, Validation, Sanitization and Encoding Verification Requirements\nV6, Stored Cryptography Verification Requirements\nV7, Error Handling and Logging Verification Requirements\nV8, Data Protection Verification Requirements\nV9, Communications Verification Requirements\nV10, Malicious Code Verification Requirements\nV12, File and Resources Verification Requirements\nV13, API and Web Service Verification Requirements\nPRECISION 0.969 0.956 0.964 0.821 0.128 0.959 0.871 0.862 0.768 0.179 0.813RECALL 0.976 0.966 0.969 0.829 0.128 0.964 0.878 0.866 0.773 0.179 0.821F1 0.973 0.961 0.966 0.825 0.128 0.962 0.874 0.864 0.771 0.179 0.817SUPPORT 382 381 385 314 38 389 384 380 299 27 317TOTAL 398 398 398 398 398 398 398 398 398 398 398 (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n            \n            [Graph Knowledge]\n            Focused Entity: OWASP\n            Relationships: [[b'type(r)', b'm.name'], [], [b'Cached execution: 0', b'Query internal execution time: 0.473167 milliseconds']]\n            \n            Your Answer:\n            ",
  "vector_candidates": 20,
  "reranked_candidates": 20,
  "llm_provider": "azure"
}
INFO:     127.0.0.1:62571 - "GET /query?q=Which+OWASP+classed+are+problematic+and+can+you+give+performance+summary+of+models+on+these+classes%3F HTTP/1.1" 200 OK
INFO:     127.0.0.1:62575 - "GET /files/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf HTTP/1.1" 304 Not Modified
INFO:kafka.client:Closing idle connection 0, last active 1173087 ms ago
INFO:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
INFO:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
INFO:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
INFO:kafka.client:Closing idle connection 0, last active 1901625 ms ago
INFO:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
INFO:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
INFO:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
INFO:httpx:HTTP Request: POST https://genai-core-dev.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-05-15 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:6333/collections/federated_docs/points/query "HTTP/1.1 200 OK"
2026-01-03 15:30:06,336 - src.modules.llm - INFO - Using Cloud Fallback for Local Task (Provider: azure)
INFO:src.modules.llm:Using Cloud Fallback for Local Task (Provider: azure)
INFO:httpx:HTTP Request: POST https://genai-core-dev.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://genai-core-dev.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 "HTTP/1.1 200 OK"
2026-01-03 15:30:11,346 - API - INFO - üêõ Debug [Query='so what can we do to get better results in future?']: {
  "original_query": "so what can we do to get better results in future?",
  "refined_query": "so what can we do to get better results in future?",
  "final_prompt": "\n            You are a helpful assistant. \n            Answer the user's query mostly based on the provided Context.\n            \n            - If the Context mentions the term, summarize its usage, examples, or categories found.\n            - If the answer is NOT in the Context, say \"I cannot find the answer in the provided documents.\"\n            - Cite the source filename if possible.\n\n            Original Query: so what can we do to get better results in future?\n            Refined Intent: so what can we do to get better results in future?\n\n            [Vector Knowledge]\n            - CONTEXT: \"Discussion on Model Transparency and Trade-offs in Security Classification\" \u2013 This chunk highlights the importance of documenting model identifiers for transparency, discusses the trade-offs between dataset curation effort and classification performance, and evaluates the impact of emerging LLM capabilities on Security Requirements Engineering.\n\nCONTENT: searchers may not be able to query the exact same model weights in the future.\nTo mitigate this, we have documented the exact model identifier strings used\nduring our data collection window. While this does not guarantee future ac-\ncess to these specific snapshots, it ensures transparency regarding the specific\ntechnological generation evaluated. We argue that this trade-off is necessary\nto provide a forward-looking analysis of how emerging LLM capabilities (such\nas intrinsic reasoning) impact the field of Security Requirements Engineering.\n7 Discussion\nThe combined experimental results obtained from fine-tuning BERT models\nand employing various LLM prompting strategies, further refined by the com-\nparison between zero-shot and few-shot methods, yield several key insights.\nAn important dimension of our comparison is the trade-off between the ef-\nfort required to curate the dataset and the resulting classification performance. (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Exploring Feasibility and Future Directions in Automated Security Requirements Classification**: This chunk discusses the evaluation of automated security requirements classification methods under real-world conditions, emphasizing the role of few-shot prompting and fine-tuned models, and outlines future work on hybrid approaches to address challenges like underrepresented security classes.\n\nCONTENT: prior work across domains, where prompt design and example selection were\nshown to play a decisive role in task performance (Mann et al., 2020; Liu et al.,\n2023a).\n8 Conclusion and Future Work\nThis work examined the feasibility of automated security requirements classi-\nfication under realistic conditions, where requirements are implicit, unevenly\ndistributed across categories, and drawn from industrial settings rather than\ncurated benchmarks. By introducing ASRD and evaluating supervised and\nprompt-based methods on a shared benchmark, this work enables direct com-\nparison across approaches. The results show that few-shot prompting is suit-\nable when labeled data is limited, while fine-tuned models are better at han-\ndling rare security requirements.\nAs a future work, we plan to explore hybrid SRE approaches that combine\nLLMs with retrieval and agent-based components. In particular, improving\nperformance on underrepresented classes remains an open problem and may (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Evaluating Model Generalizability and Future Translation Efforts**: This section discusses the importance of assessing model performance on unseen projects using a leave-one-project-out cross-validation approach and outlines plans for translating the ASRD dataset to enhance its accessibility to the global research community.\n\nCONTENT: Additionally, future efforts should include a more stringent evaluation of\nmodel generalizability using a leave-one-project-out cross-validation approach.\nThis method involves training a model on requirements from five of the projects\nand testing it on the single, held-out project. Repeating this process for all six\nprojects would provide a much clearer and more realistic assessment of how\nwell these models perform on entirely new, unseen SRS documents, which is\na significant measure for real-world applicability.\nFurthermore, to bridge the gap between our current findings and the global\nresearch community, we plan to undertake a professional translation of the (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Advancing Rare Security Requirement Classification with Dynamic Retrieval**: This chunk discusses the limitations of relying solely on organic dataset expansion for classifying rare security requirements and proposes future advancements, including Dynamic Few-Shot Selection with Retrieval-Augmented Generation (RAG), to enhance classification accuracy by leveraging explicit standards like OWASP ASVS.\n\nCONTENT: Implicit Security Requirements Classification 37\ntime. This approach effectively substitutes the need for dense in-context ex-\namples with explicit external standards, ensuring accurate classification for\nrare security events where traditional fine-tuning or few-shot prompting fails.\nConsequently, this study suggests that future NLP research in SRE cannot\nrely solely on organic dataset expansion. To overcome this inherent sparsity,\nwe propose as future work three targeted algorithmic advancements:\n1. Dynamic Few-Shot Selection with Retrieval-Augmented Generation (RAG):\nSystems might dynamically retrieve the explicit definitions and verifica-\ntion criteria from standards like OWASP ASVS to ground their classifi-\ncation of rare requirements. Future work should implement RAG for Dy-\nnamic Prompting in agentic approach. Instead of fixed examples, a retriever\nshould select thekmost semantically similar valid requirements from the (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Exploring Advanced Human-AI Collaboration for Security Classification**: This chunk discusses future directions for enhancing security requirement classification through a Human-AI collaborative multi-agent framework, leveraging semantic similarity for dynamic in-context example selection and integrating expert oversight for ambiguous or high-risk cases.\n\nCONTENT: should select thekmost semantically similar valid requirements from the\ntraining set to serve as in-context examples for the specific query. This\nensures the LLM is grounded with relevant architectural patterns (e.g.,\n\u201cencryption\u201d vs. \u201chashing\u201d) rather than generic class examples.\n2. Human-AI Collaborative Multi-Agent Framework: A promising direction\nfor future work is a Human-in-the-Loop multi-agent framework that com-\nbines automated classification with targeted expert oversight for ambigu-\nous or high-risk cases. In this setup, a retrieval-augmented agent would\ndynamically select the most relevant annotated requirements using seman-\ntic similarity, replacing static few-shot examples. A second agent would\nassess classification confidence and consistency, flagging unclear cases and\nrequesting focused clarifications from a human expert when needed. A su-\npervisor agent would then consolidate these inputs and ensure that the (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n            \n            [Graph Knowledge]\n            Focused Entity: \n            Relationships: No specific graph context.\n            \n            Your Answer:\n            ",
  "vector_candidates": 20,
  "reranked_candidates": 20,
  "llm_provider": "azure"
}
INFO:API:üêõ Debug [Query='so what can we do to get better results in future?']: {
  "original_query": "so what can we do to get better results in future?",
  "refined_query": "so what can we do to get better results in future?",
  "final_prompt": "\n            You are a helpful assistant. \n            Answer the user's query mostly based on the provided Context.\n            \n            - If the Context mentions the term, summarize its usage, examples, or categories found.\n            - If the answer is NOT in the Context, say \"I cannot find the answer in the provided documents.\"\n            - Cite the source filename if possible.\n\n            Original Query: so what can we do to get better results in future?\n            Refined Intent: so what can we do to get better results in future?\n\n            [Vector Knowledge]\n            - CONTEXT: \"Discussion on Model Transparency and Trade-offs in Security Classification\" \u2013 This chunk highlights the importance of documenting model identifiers for transparency, discusses the trade-offs between dataset curation effort and classification performance, and evaluates the impact of emerging LLM capabilities on Security Requirements Engineering.\n\nCONTENT: searchers may not be able to query the exact same model weights in the future.\nTo mitigate this, we have documented the exact model identifier strings used\nduring our data collection window. While this does not guarantee future ac-\ncess to these specific snapshots, it ensures transparency regarding the specific\ntechnological generation evaluated. We argue that this trade-off is necessary\nto provide a forward-looking analysis of how emerging LLM capabilities (such\nas intrinsic reasoning) impact the field of Security Requirements Engineering.\n7 Discussion\nThe combined experimental results obtained from fine-tuning BERT models\nand employing various LLM prompting strategies, further refined by the com-\nparison between zero-shot and few-shot methods, yield several key insights.\nAn important dimension of our comparison is the trade-off between the ef-\nfort required to curate the dataset and the resulting classification performance. (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Exploring Feasibility and Future Directions in Automated Security Requirements Classification**: This chunk discusses the evaluation of automated security requirements classification methods under real-world conditions, emphasizing the role of few-shot prompting and fine-tuned models, and outlines future work on hybrid approaches to address challenges like underrepresented security classes.\n\nCONTENT: prior work across domains, where prompt design and example selection were\nshown to play a decisive role in task performance (Mann et al., 2020; Liu et al.,\n2023a).\n8 Conclusion and Future Work\nThis work examined the feasibility of automated security requirements classi-\nfication under realistic conditions, where requirements are implicit, unevenly\ndistributed across categories, and drawn from industrial settings rather than\ncurated benchmarks. By introducing ASRD and evaluating supervised and\nprompt-based methods on a shared benchmark, this work enables direct com-\nparison across approaches. The results show that few-shot prompting is suit-\nable when labeled data is limited, while fine-tuned models are better at han-\ndling rare security requirements.\nAs a future work, we plan to explore hybrid SRE approaches that combine\nLLMs with retrieval and agent-based components. In particular, improving\nperformance on underrepresented classes remains an open problem and may (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Evaluating Model Generalizability and Future Translation Efforts**: This section discusses the importance of assessing model performance on unseen projects using a leave-one-project-out cross-validation approach and outlines plans for translating the ASRD dataset to enhance its accessibility to the global research community.\n\nCONTENT: Additionally, future efforts should include a more stringent evaluation of\nmodel generalizability using a leave-one-project-out cross-validation approach.\nThis method involves training a model on requirements from five of the projects\nand testing it on the single, held-out project. Repeating this process for all six\nprojects would provide a much clearer and more realistic assessment of how\nwell these models perform on entirely new, unseen SRS documents, which is\na significant measure for real-world applicability.\nFurthermore, to bridge the gap between our current findings and the global\nresearch community, we plan to undertake a professional translation of the (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Advancing Rare Security Requirement Classification with Dynamic Retrieval**: This chunk discusses the limitations of relying solely on organic dataset expansion for classifying rare security requirements and proposes future advancements, including Dynamic Few-Shot Selection with Retrieval-Augmented Generation (RAG), to enhance classification accuracy by leveraging explicit standards like OWASP ASVS.\n\nCONTENT: Implicit Security Requirements Classification 37\ntime. This approach effectively substitutes the need for dense in-context ex-\namples with explicit external standards, ensuring accurate classification for\nrare security events where traditional fine-tuning or few-shot prompting fails.\nConsequently, this study suggests that future NLP research in SRE cannot\nrely solely on organic dataset expansion. To overcome this inherent sparsity,\nwe propose as future work three targeted algorithmic advancements:\n1. Dynamic Few-Shot Selection with Retrieval-Augmented Generation (RAG):\nSystems might dynamically retrieve the explicit definitions and verifica-\ntion criteria from standards like OWASP ASVS to ground their classifi-\ncation of rare requirements. Future work should implement RAG for Dy-\nnamic Prompting in agentic approach. Instead of fixed examples, a retriever\nshould select thekmost semantically similar valid requirements from the (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Exploring Advanced Human-AI Collaboration for Security Classification**: This chunk discusses future directions for enhancing security requirement classification through a Human-AI collaborative multi-agent framework, leveraging semantic similarity for dynamic in-context example selection and integrating expert oversight for ambiguous or high-risk cases.\n\nCONTENT: should select thekmost semantically similar valid requirements from the\ntraining set to serve as in-context examples for the specific query. This\nensures the LLM is grounded with relevant architectural patterns (e.g.,\n\u201cencryption\u201d vs. \u201chashing\u201d) rather than generic class examples.\n2. Human-AI Collaborative Multi-Agent Framework: A promising direction\nfor future work is a Human-in-the-Loop multi-agent framework that com-\nbines automated classification with targeted expert oversight for ambigu-\nous or high-risk cases. In this setup, a retrieval-augmented agent would\ndynamically select the most relevant annotated requirements using seman-\ntic similarity, replacing static few-shot examples. A second agent would\nassess classification confidence and consistency, flagging unclear cases and\nrequesting focused clarifications from a human expert when needed. A su-\npervisor agent would then consolidate these inputs and ensure that the (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n            \n            [Graph Knowledge]\n            Focused Entity: \n            Relationships: No specific graph context.\n            \n            Your Answer:\n            ",
  "vector_candidates": 20,
  "reranked_candidates": 20,
  "llm_provider": "azure"
}
INFO:     127.0.0.1:49310 - "GET /query?q=so+what+can+we+do+to+get+better+results+in+future%3F HTTP/1.1" 200 OK
INFO:     127.0.0.1:49442 - "GET /documents HTTP/1.1" 200 OK
INFO:     127.0.0.1:49441 - "GET /settings HTTP/1.1" 200 OK
INFO:httpx:HTTP Request: POST https://genai-core-dev.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-05-15 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:6333/collections/federated_docs/points/query "HTTP/1.1 200 OK"
2026-01-03 15:35:51,224 - src.modules.llm - INFO - Using Cloud Fallback for Local Task (Provider: azure)
INFO:src.modules.llm:Using Cloud Fallback for Local Task (Provider: azure)
INFO:httpx:HTTP Request: POST https://genai-core-dev.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://genai-core-dev.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 "HTTP/1.1 200 OK"
2026-01-03 15:35:55,930 - API - INFO - üêõ Debug [Query='so what can we do to get better results in future?']: {
  "original_query": "so what can we do to get better results in future?",
  "refined_query": "so what can we do to get better results in future?",
  "final_prompt": "\n            You are a helpful assistant. \n            Answer the user's query mostly based on the provided Context.\n            \n            - If the Context mentions the term, summarize its usage, examples, or categories found.\n            - If the answer is NOT in the Context, say \"I cannot find the answer in the provided documents.\"\n            - Cite the source filename if possible.\n\n            Original Query: so what can we do to get better results in future?\n            Refined Intent: so what can we do to get better results in future?\n\n            [Vector Knowledge]\n            - CONTEXT: \"Discussion on Model Transparency and Trade-offs in Security Classification\" \u2013 This chunk highlights the importance of documenting model identifiers for transparency, discusses the trade-offs between dataset curation effort and classification performance, and evaluates the impact of emerging LLM capabilities on Security Requirements Engineering.\n\nCONTENT: searchers may not be able to query the exact same model weights in the future.\nTo mitigate this, we have documented the exact model identifier strings used\nduring our data collection window. While this does not guarantee future ac-\ncess to these specific snapshots, it ensures transparency regarding the specific\ntechnological generation evaluated. We argue that this trade-off is necessary\nto provide a forward-looking analysis of how emerging LLM capabilities (such\nas intrinsic reasoning) impact the field of Security Requirements Engineering.\n7 Discussion\nThe combined experimental results obtained from fine-tuning BERT models\nand employing various LLM prompting strategies, further refined by the com-\nparison between zero-shot and few-shot methods, yield several key insights.\nAn important dimension of our comparison is the trade-off between the ef-\nfort required to curate the dataset and the resulting classification performance. (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Exploring Feasibility and Future Directions in Automated Security Requirements Classification**: This chunk discusses the evaluation of automated security requirements classification methods under real-world conditions, emphasizing the role of few-shot prompting and fine-tuned models, and outlines future work on hybrid approaches to address challenges like underrepresented security classes.\n\nCONTENT: prior work across domains, where prompt design and example selection were\nshown to play a decisive role in task performance (Mann et al., 2020; Liu et al.,\n2023a).\n8 Conclusion and Future Work\nThis work examined the feasibility of automated security requirements classi-\nfication under realistic conditions, where requirements are implicit, unevenly\ndistributed across categories, and drawn from industrial settings rather than\ncurated benchmarks. By introducing ASRD and evaluating supervised and\nprompt-based methods on a shared benchmark, this work enables direct com-\nparison across approaches. The results show that few-shot prompting is suit-\nable when labeled data is limited, while fine-tuned models are better at han-\ndling rare security requirements.\nAs a future work, we plan to explore hybrid SRE approaches that combine\nLLMs with retrieval and agent-based components. In particular, improving\nperformance on underrepresented classes remains an open problem and may (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Evaluating Model Generalizability and Future Translation Efforts**: This section discusses the importance of assessing model performance on unseen projects using a leave-one-project-out cross-validation approach and outlines plans for translating the ASRD dataset to enhance its accessibility to the global research community.\n\nCONTENT: Additionally, future efforts should include a more stringent evaluation of\nmodel generalizability using a leave-one-project-out cross-validation approach.\nThis method involves training a model on requirements from five of the projects\nand testing it on the single, held-out project. Repeating this process for all six\nprojects would provide a much clearer and more realistic assessment of how\nwell these models perform on entirely new, unseen SRS documents, which is\na significant measure for real-world applicability.\nFurthermore, to bridge the gap between our current findings and the global\nresearch community, we plan to undertake a professional translation of the (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Advancing Rare Security Requirement Classification with Dynamic Retrieval**: This chunk discusses the limitations of relying solely on organic dataset expansion for classifying rare security requirements and proposes future advancements, including Dynamic Few-Shot Selection with Retrieval-Augmented Generation (RAG), to enhance classification accuracy by leveraging explicit standards like OWASP ASVS.\n\nCONTENT: Implicit Security Requirements Classification 37\ntime. This approach effectively substitutes the need for dense in-context ex-\namples with explicit external standards, ensuring accurate classification for\nrare security events where traditional fine-tuning or few-shot prompting fails.\nConsequently, this study suggests that future NLP research in SRE cannot\nrely solely on organic dataset expansion. To overcome this inherent sparsity,\nwe propose as future work three targeted algorithmic advancements:\n1. Dynamic Few-Shot Selection with Retrieval-Augmented Generation (RAG):\nSystems might dynamically retrieve the explicit definitions and verifica-\ntion criteria from standards like OWASP ASVS to ground their classifi-\ncation of rare requirements. Future work should implement RAG for Dy-\nnamic Prompting in agentic approach. Instead of fixed examples, a retriever\nshould select thekmost semantically similar valid requirements from the (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Exploring Advanced Human-AI Collaboration for Security Classification**: This chunk discusses future directions for enhancing security requirement classification through a Human-AI collaborative multi-agent framework, leveraging semantic similarity for dynamic in-context example selection and integrating expert oversight for ambiguous or high-risk cases.\n\nCONTENT: should select thekmost semantically similar valid requirements from the\ntraining set to serve as in-context examples for the specific query. This\nensures the LLM is grounded with relevant architectural patterns (e.g.,\n\u201cencryption\u201d vs. \u201chashing\u201d) rather than generic class examples.\n2. Human-AI Collaborative Multi-Agent Framework: A promising direction\nfor future work is a Human-in-the-Loop multi-agent framework that com-\nbines automated classification with targeted expert oversight for ambigu-\nous or high-risk cases. In this setup, a retrieval-augmented agent would\ndynamically select the most relevant annotated requirements using seman-\ntic similarity, replacing static few-shot examples. A second agent would\nassess classification confidence and consistency, flagging unclear cases and\nrequesting focused clarifications from a human expert when needed. A su-\npervisor agent would then consolidate these inputs and ensure that the (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n            \n            [Graph Knowledge]\n            Focused Entity: \n            Relationships: No specific graph context.\n            \n            Your Answer:\n            ",
  "vector_candidates": 20,
  "reranked_candidates": 20,
  "llm_provider": "azure"
}
INFO:API:üêõ Debug [Query='so what can we do to get better results in future?']: {
  "original_query": "so what can we do to get better results in future?",
  "refined_query": "so what can we do to get better results in future?",
  "final_prompt": "\n            You are a helpful assistant. \n            Answer the user's query mostly based on the provided Context.\n            \n            - If the Context mentions the term, summarize its usage, examples, or categories found.\n            - If the answer is NOT in the Context, say \"I cannot find the answer in the provided documents.\"\n            - Cite the source filename if possible.\n\n            Original Query: so what can we do to get better results in future?\n            Refined Intent: so what can we do to get better results in future?\n\n            [Vector Knowledge]\n            - CONTEXT: \"Discussion on Model Transparency and Trade-offs in Security Classification\" \u2013 This chunk highlights the importance of documenting model identifiers for transparency, discusses the trade-offs between dataset curation effort and classification performance, and evaluates the impact of emerging LLM capabilities on Security Requirements Engineering.\n\nCONTENT: searchers may not be able to query the exact same model weights in the future.\nTo mitigate this, we have documented the exact model identifier strings used\nduring our data collection window. While this does not guarantee future ac-\ncess to these specific snapshots, it ensures transparency regarding the specific\ntechnological generation evaluated. We argue that this trade-off is necessary\nto provide a forward-looking analysis of how emerging LLM capabilities (such\nas intrinsic reasoning) impact the field of Security Requirements Engineering.\n7 Discussion\nThe combined experimental results obtained from fine-tuning BERT models\nand employing various LLM prompting strategies, further refined by the com-\nparison between zero-shot and few-shot methods, yield several key insights.\nAn important dimension of our comparison is the trade-off between the ef-\nfort required to curate the dataset and the resulting classification performance. (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Exploring Feasibility and Future Directions in Automated Security Requirements Classification**: This chunk discusses the evaluation of automated security requirements classification methods under real-world conditions, emphasizing the role of few-shot prompting and fine-tuned models, and outlines future work on hybrid approaches to address challenges like underrepresented security classes.\n\nCONTENT: prior work across domains, where prompt design and example selection were\nshown to play a decisive role in task performance (Mann et al., 2020; Liu et al.,\n2023a).\n8 Conclusion and Future Work\nThis work examined the feasibility of automated security requirements classi-\nfication under realistic conditions, where requirements are implicit, unevenly\ndistributed across categories, and drawn from industrial settings rather than\ncurated benchmarks. By introducing ASRD and evaluating supervised and\nprompt-based methods on a shared benchmark, this work enables direct com-\nparison across approaches. The results show that few-shot prompting is suit-\nable when labeled data is limited, while fine-tuned models are better at han-\ndling rare security requirements.\nAs a future work, we plan to explore hybrid SRE approaches that combine\nLLMs with retrieval and agent-based components. In particular, improving\nperformance on underrepresented classes remains an open problem and may (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Evaluating Model Generalizability and Future Translation Efforts**: This section discusses the importance of assessing model performance on unseen projects using a leave-one-project-out cross-validation approach and outlines plans for translating the ASRD dataset to enhance its accessibility to the global research community.\n\nCONTENT: Additionally, future efforts should include a more stringent evaluation of\nmodel generalizability using a leave-one-project-out cross-validation approach.\nThis method involves training a model on requirements from five of the projects\nand testing it on the single, held-out project. Repeating this process for all six\nprojects would provide a much clearer and more realistic assessment of how\nwell these models perform on entirely new, unseen SRS documents, which is\na significant measure for real-world applicability.\nFurthermore, to bridge the gap between our current findings and the global\nresearch community, we plan to undertake a professional translation of the (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Advancing Rare Security Requirement Classification with Dynamic Retrieval**: This chunk discusses the limitations of relying solely on organic dataset expansion for classifying rare security requirements and proposes future advancements, including Dynamic Few-Shot Selection with Retrieval-Augmented Generation (RAG), to enhance classification accuracy by leveraging explicit standards like OWASP ASVS.\n\nCONTENT: Implicit Security Requirements Classification 37\ntime. This approach effectively substitutes the need for dense in-context ex-\namples with explicit external standards, ensuring accurate classification for\nrare security events where traditional fine-tuning or few-shot prompting fails.\nConsequently, this study suggests that future NLP research in SRE cannot\nrely solely on organic dataset expansion. To overcome this inherent sparsity,\nwe propose as future work three targeted algorithmic advancements:\n1. Dynamic Few-Shot Selection with Retrieval-Augmented Generation (RAG):\nSystems might dynamically retrieve the explicit definitions and verifica-\ntion criteria from standards like OWASP ASVS to ground their classifi-\ncation of rare requirements. Future work should implement RAG for Dy-\nnamic Prompting in agentic approach. Instead of fixed examples, a retriever\nshould select thekmost semantically similar valid requirements from the (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n- CONTEXT: **Exploring Advanced Human-AI Collaboration for Security Classification**: This chunk discusses future directions for enhancing security requirement classification through a Human-AI collaborative multi-agent framework, leveraging semantic similarity for dynamic in-context example selection and integrating expert oversight for ambiguous or high-risk cases.\n\nCONTENT: should select thekmost semantically similar valid requirements from the\ntraining set to serve as in-context examples for the specific query. This\nensures the LLM is grounded with relevant architectural patterns (e.g.,\n\u201cencryption\u201d vs. \u201chashing\u201d) rather than generic class examples.\n2. Human-AI Collaborative Multi-Agent Framework: A promising direction\nfor future work is a Human-in-the-Loop multi-agent framework that com-\nbines automated classification with targeted expert oversight for ambigu-\nous or high-risk cases. In this setup, a retrieval-augmented agent would\ndynamically select the most relevant annotated requirements using seman-\ntic similarity, replacing static few-shot examples. A second agent would\nassess classification confidence and consistency, flagging unclear cases and\nrequesting focused clarifications from a human expert when needed. A su-\npervisor agent would then consolidate these inputs and ensure that the (Src: temp/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf)\n            \n            [Graph Knowledge]\n            Focused Entity: \n            Relationships: No specific graph context.\n            \n            Your Answer:\n            ",
  "vector_candidates": 20,
  "reranked_candidates": 20,
  "llm_provider": "azure"
}
INFO:     127.0.0.1:49448 - "GET /query?q=so+what+can+we+do+to+get+better+results+in+future%3F&filter=temp%2F0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf HTTP/1.1" 200 OK
INFO:     127.0.0.1:49452 - "GET /files/0b21c8e8-6086-46c8-a197-e548c2ca8d64_template.pdf HTTP/1.1" 200 OK
